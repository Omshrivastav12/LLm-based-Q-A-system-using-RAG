{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloud platform for storing embeddings(Chunks in form of vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_URL='https://rag-project-rd3frgtw.weaviate.network'\n",
    "API_KEY='THlGuygLCgNr5jVjC7XkFJ5Wy3c0Id0jZtdR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omshr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\weaviate\\__init__.py:128: DeprecationWarning: Dep010: Importing AuthApiKey from weaviate is deprecated. Please import it from its specific module: weaviate.auth\n",
      "  _Warnings.root_module_import(name, map_[name])\n",
      "c:\\Users\\omshr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\weaviate\\warnings.py:158: DeprecationWarning: Dep016: You are using the Weaviate v3 client, which is deprecated.\n",
      "            Consider upgrading to the new and improved v4 client instead!\n",
      "            See here for usage: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "\n",
    "WEAVIATE_URL = CLUSTER_URL\n",
    "WEAVIATE_API_KEY = API_KEY\n",
    "\n",
    "client = weaviate.Client(\n",
    "    url=WEAVIATE_URL, auth_client_secret=weaviate.AuthApiKey(WEAVIATE_API_KEY)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model for performing embedding to our chunked documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings #This line imports the HuggingFaceEmbeddings class from the langchain.embeddings module. This class helps you create embedding models using pre-trained models from the Hugging Face ecosystem.\n",
    "\n",
    "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\" # This line assigns the name of the pre-trained model you want to use for generating embeddings. In this case, it's \"sentence-transformers/all-mpnet-base-v2\" which is a pre-trained sentence transformer model based on the all-mpnet-base-v2 architecture.\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings( #embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name): This line creates an instance of the HuggingFaceEmbeddings class. It takes the following arguments:model_name: The name of the pre-trained model specified earlier.\n",
    "  model_name=embedding_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader #Aquisitizing data into working environment.\n",
    "loader = PyPDFLoader(\"Merged_Papers_RAG.pdf\", extract_images=True) #creating object of pdf loader.\n",
    "pages = loader.load() #loading actual raw text (pages to page) from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Future Frame Prediction Using Convolutional VRNN for Anomaly Detection\\nYiwei Lu, Mahesh Kumar Krishna Reddy, Seyed shahabeddin Nabavi and Yang Wang\\nUniversity of Manitoba, Winnipeg, MB, Canada\\n{luy2,kumarkm,nabaviss,ywang }@cs.umanitoba.ca\\nAbstract\\nAnomaly detection in videos aims at reporting anything\\nthat does not conform the normal behaviour or distribution.\\nHowever, due to the sparsity of abnormal video clips in real\\nlife, collecting annotated data for supervised learning is ex-\\nceptionally cumbersome. Inspired by the practicability of\\ngenerative models for semi-supervised learning, we propose\\na novel sequential generative model based on variational\\nautoencoder (VAE) for future frame prediction with convo-\\nlutional LSTM (ConvLSTM). To the best of our knowledge,\\nthis is the ﬁrst work that considers temporal information in\\nfuture frame prediction based anomaly detection framework\\nfrom the model perspective. Our experiments demonstrate\\nthat our approach is superior to the state-of-the-art meth-\\nods on three benchmark datasets.\\n1. Introduction\\nAnomaly detection is an essential problem in video\\nsurveillance. Due to the massive amount of available video\\ndata from surveillance cameras, it is time-consuming and\\ninefﬁcient to have human observers watching surveillance\\nvideos and report any anomalies. Ideally, we want an au-\\ntomatic system that can report abnormal events. Anomaly\\ndetection is challenging since the deﬁnition of “anomaly”\\nis broad and ambiguous – anything that deviates expected\\nbehaviours can be considered as “anomaly”. It is infeasi-\\nble to collect labeled training data that cover all possible\\nanomalies. As a result, recent work in anomaly detection\\nhas focused on unsupervised approaches that do not require\\nhuman labels.\\nSome recent work (e.g. [10, 16, 17, 26]) in anomaly de-\\ntection uses the idea of frame reconstruction. They build\\nmodels that learn to reconstruct the normal (or regular)\\nframes observed during training. During testing, any irreg-\\nular (abnormal) event will lead to a large reconstruction er-\\nror. The higher reconstruction error indicates the possible\\nabnormal event in the frame. Previous work [1, 26, 15]\\nhas applied variants of generative models such as varia-\\ntional autoencoder (V AE) [14] or generative adversarial net-\\nFigure 1. An example of our proposed video anomaly detection\\nmethod. Our method uses a future frame prediction framework.\\nGiven several observed frames in a video, our model predicts the\\nfuture frame. If the future frame is an anomaly, the predicted\\nfuture frame is likely to be very different from the actual future\\nframe. This reconstruction error allows us to detect the anomaly\\nin a video.\\nwork (GAN) [9] to model the distribution of the natural be-\\nhaviours. To build a real-time anomaly detection system,\\nLiu et al. [15] propose a future frame prediction framework\\nfor anomaly detection. Given several observed frames, their\\nmethod learns a GAN-based model to predict the future\\nframe. An anomaly then corresponds to a large difference\\nbetween the predicted future frame and the actual future\\nframe. One limitation of [15] is that it directly concate-\\nnates the several observed frames as the input to the GAN\\nmodel. As a result, the model does not directly represent the\\ntemporal information in a video. Although [15] uses optical\\nﬂow features which capture some temporal information at\\nthe feature level, the optical ﬂow information is only used\\nas a constraint during training and is not used during testing.\\nIn this paper, we follow the future frame prediction\\nframework in [15] and propose a new approach that bet-\\nter capture the temporal information in a video for anomaly\\ndetection. We propose to combine sequential models (in\\nparticular, ConvLSTM) with generative models (in particu-\\n978-1-5386-9294-3/18/$31.00 2019 IEEEarXiv:1909.02168v2  [cs.CV]  18 Oct 2019\\nSuspicious Behaviour\\nNormal Events\\nDetected\\nNormal Events\\n1.0\\nProbability of Anomaly\\n0.8\\n0.6\\n0.4\\n0.2\\nSurveillance\\nVideo\\nPrediction', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 0}),\n",
       " Document(page_content='lar, V AE) to build a model that can be trained end-to-end.\\nAlthough sequential generative models have been previ-\\nously proposed for speech recognition and music generation\\n[23, 3], they have not been applied in anomaly detection. An\\nexample of our proposed video anomaly detection system\\ncan be seen in Fig 1. Given several consecutive frames, our\\nmodel learns to predict the next future frame. For normal\\nframes, our method is able to predict the next frame reason-\\nably well. When there is anomaly in the future frame, the\\nprediction is often distorted and blurry. By comparing the\\npredicted future frame with the actual future frame, our sys-\\ntem can detect suspicious behaviours or events (in this case,\\nthe man is throwing his bag up and down) are detected in a\\nvideo frame.\\nIn this paper, we make the following contributions. We\\npropose a sequential generative model for video anomaly\\ndetection using the future frame prediction framework. We\\ncombine ConvLSTM with V AE to better capture the tempo-\\nral relationship among frames in a video. Our experimen-\\ntal results demonstrate that the proposed model outperforms\\nexisting state-of-the-art approaches, even without using op-\\ntical ﬂow features.\\n2. Related Work\\nIn this section, we review several lines of prior research\\nrelated to our work.\\nAnomaly Detection with Hand-crafted Features : Early\\nwork in video anomaly detection uses hand-crafted fea-\\ntures. [27, 30] use trajectory features to represent normal\\nbehaviours. However, these methods can not be applied to\\ncrowded scenes. To address this limitation, low-level fea-\\ntures such as histogram of oriented gradient and histogram\\nof oriented ﬂows are also applied [5, 6] for human detection.\\n[33, 16, 4] represent each scene by a dictionary of temporal\\nand spatial information. These approaches have low perfor-\\nmance due to the fact that the dictionary does not ensure\\nthe capacity of normal events and cannot classify anomaly\\ncorrespondingly. Statistical-based models have also been\\nproposed. For example, [13] proposes an approach based\\non a mixture of probabilistic PCA (MPPCA) with optical\\nﬂow pattern. Gaussian mixture model [19] has also been\\napplied for anomaly detection.\\nAnomaly Detection with Deep Learning : In order to ad-\\ndress the limitation of hand-crafted features in anomaly de-\\ntection, there has been recent work that explores the use of\\ndeep learning approaches. A lot of these methods learn a\\ndeep learning model to reconstruct a frame and use the re-\\nconstruction error for anomaly detection. Inspired by [20],\\nHasan et al. [10] apply convolutional autoencoder for re-\\nconstructing normal frames. Some follow-up works [25, 2]\\npropose to build a more robust version. Xu et al. [32] use\\nstacked de-noising autoencoders [28] and optical ﬂow to\\ncapture both appearance and motion information.Some work considers using a future frame prediction ap-\\nproach for anomaly detection. Medel et al. [22] apply Con-\\nvLSTM as a backbone network and build a future predic-\\ntion model for anomaly detection. Luo et al. [17] combine\\nautoencoder and ConvLSTM to reconstruct the output of\\nConvLSTM to the original image size. Because the inner\\nstructure of ConvLSTM is entirely deterministic, these pre-\\ndictive modeling methods cannot predict highly structured\\nmoving objects, which results in inaccurate predictions of\\nanomalies.\\nGenerative models, such as V AE [14] and GAN [9], have\\nbeen applied for the purpose of learning the distribution of\\nregular frames. Sabokrou et al. [26] propose a one class\\nclassiﬁer using conditional adversarial networks [12]. Xie\\net al. [31] use a GAN-based image inpainting method to\\ndetect and localize the abnormal objects. Liu et al. [15]\\npropose a GAN-based future frame prediction network with\\noptical ﬂow network[8]. An et al. [1] apply V AE to build an\\nanomaly detection system, but the method is not performed\\non real-world datasets.\\nSequential Generative Models : There has been some\\nwork on incorporating sequential information in generative\\nmodels. Chung et al. [3] argue that latent random variables\\ncan play crucial roles in the dynamics of RNN. By combin-\\ning V AE and RNN, they are able to model sequences with\\nsigniﬁcant improvement on RNN. However, this model has\\nonly been used on simple tasks such as speech generation\\nor handwriting generation. [23] propose a sequential gen-\\nerative model using adversarial training on RNN. They ar-\\ngue that with the supervision of a discriminator, their pro-\\nposed generative model can be trained to be very expres-\\nsive with high ﬂexibility on continuous sequences such as\\nmusic. However, the potential of this model on computer\\nvision tasks has not yet been explored.\\n3. Background\\n3.1. Variational Autoencoder\\nVariational autoencoder (V AE) [14] has been shown to\\nbe effective in reconstructing complex distributions for non-\\nsequential data. Given an input x, V AE applies an encoder\\n(also known as inference model) qθ(z|x)to generate the la-\\ntent variable zthat captures the variation in x. It uses a\\ndecoderpφ(x|z)to approximate the observation given the\\nlatent variable. The inference model represents the approx-\\nimate posterior using the mean µand variance σ2calculated\\nby a neural network qθ(z|x)∼N (µx, σ2\\nx), whereµxand\\nσ2\\nxare outputs of some neural networks that take xas the\\ninput. A prior p(z)is chosen to be a simple Gaussian dis-\\ntribution. With the constraints of distribution on latent vari-\\nables, the complete objective function can be described as\\n', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 1}),\n",
       " Document(page_content='Figure 2. An overview of our proposed Conv-VRNN model at one time-step of a sequence. Our model requires 4 steps to process the input:\\n(a) calculating the prior distribution in V AE; (b) encoder for posterior distribution and latent variable; (c) recurrence module for sequence\\nmodelling; (d) decoder for prediction.\\nbelow:\\nL(x|θ,φ) =−KL(qθ(z|x)||p(z))+\\nEqθ(z|x)[logpφ(x|z)](1)\\nwhereKL(qθ(z|x)||p(z))is the Kullback-Leibler diver-\\ngence [11] between the prior and the posterior.\\n3.2. Variational Recurrent Neural Network\\nV AE is a generative model. It cannot directly be used\\nto model sequential data. For the problem of anomaly de-\\ntection, our data are inherently sequential since we need to\\nconsider the information in several consecutive frames in\\norder to predict the next frame. Variational Recurrent Neu-\\nral Network (VRNN) [3] is an extension of vanilla V AE. It\\ncombines V AE with a recurrent neural network in order to\\nmodel sequential data. Since this approach shares the same\\ninspiration with our Conv-VRNN approach, we will explain\\nthe technical details in the next section.\\n4. Approach\\nFollowing [15], we approach the anomaly detection\\nproblem using the future frame prediction framework. The\\ngoal is to build a model that takes several frames in a video\\nas the input and predict the future frame. The predicted fu-\\nture frame is then compared with the actual future frame.\\nIf their difference is signiﬁcant, we will consider it to be\\nan anomaly. The main difference from [15] is that our pro-\\nposed approach combines a recurrent network with a gen-\\nerative model. As a result, our approach can better capture\\ntemporal information in the video.\\nOur problem formulation is as follows. Given a se-\\nquence of frames x(1),...,x (T), we aim at predicting the\\nnext framex(T+ 1) . Note thatTis a constant which wedeﬁne as 4 in our case. We use x′(T+ 1) to denote the\\npredicted frame at time T+ 1. During training, we learn a\\nmodel that minimizes the difference between the predicted\\nand actual future frames, i.e. L=|(x(T+ 1)−x′(T+ 1)|.\\nDuring testing, if this difference is too large, we will con-\\nsiderx(T+ 1) to be an anomaly.\\nIn this section, we ﬁrst introduce our model Conv-\\nVRNN (Sec. 4.1) for future frame prediction. Our model\\ncombines V AE and a ConvLSTM module. We then describe\\nhow to use the proposed model to detect anomaly during\\ntesting (Sec. 4.2).\\n4.1. Conv-VRNN for Future Frame Prediction\\nTo extend V AE to model image sequences for anomaly\\ndetection, we use the idea of Variational Recurrent Neu-\\nral Network (VRNN) [3] and build a Conv-VRNN model\\nfor future frame prediction. An overview of our proposed\\nmodel is shown in Figure 2. Let x(t)∈RH×W×3be the\\ninput image at time t, whereH×Wis the spatial dimension\\nof the image. We deﬁne h(t)∈RH×W×3to be the hidden\\nstate of a ConvLSTM at time step t. Note that we choose\\nthe spatial dimension of h(t)to match the image size. Our\\nmethod consists of four components at each time step t:\\nPrior Distribution in VAE: This module takes the hid-\\nden stateh(t−1)from the previous time step as the in-\\nput. It then generates a distribution on the latent variable in\\nV AE. We ﬁrst extract a feature vector from h(t−1). Since\\nh(t−1)∈RH×W×3is a 3D tensor and can be treated as\\na image, we can use a standard convolutional neural net-\\nwork (CNN) to extract the feature from h(t−1). We de-\\nnote this feature as ϕh(h(t−1))∈RH′×W′×F, where\\nH′×W′andFcorrespond to the spatial dimension and\\nthe channel dimension of the CNN feature map. Here we\\nsetH′×W′×F= 16×16×32. We then apply two\\nz.(0)\\nH(0)\\nc(t)\\nPh(h(t --\\nPrior\\nh(t - 1)\\nx(t)\\nP. (r(0)\\nConvLSTM\\nh()\\n(1(t)2\\nh(t -\\n(a) Prior\\nKL-Divergence\\n(c) Recurrence\\nh(t - 1)\\nH2(0)\\nPr(h(T - 1)\\nz(t)\\nPosterior\\nx(T+ 1)\\nPrediction\\nx(t)\\n02 (0)2\\nZ(T)\\n(b) Encoder\\n(d) Decoder', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 2}),\n",
       " Document(page_content='different fully connected layers on ϕh(h(t−1))to produce\\ntwo vectors corresponding to the mean and the variance of a\\nGaussian distribution in V AE, denoted by µ1(t)andσ1(t).\\nIn our implementation, the dimension of µ1(t)andσ1(t)is\\nset to be 20, i.e. µ1(t),σ1(t)∈R20. We then use µ1(t)and\\nσl(t)to deﬁne a Gaussian distribution for the prior distribu-\\ntion on the latent variable in V AE as follows:\\nc(t)∼N(\\nµ1(t), diag(\\nσ1(t)2))\\n(2)\\nwherediag(·)creates a diagonal matrix from a vector and\\nc(t)represent the prior distribution on the latent variable.\\nEncoder: The module takes the hidden state h(t−1)of\\nprevious time step t−1and the frame x(t)at current time t\\nas the input. It then produces a vector of the latent variable\\nin V AE. We ﬁrst concatenate x(t)andh(t−1)along their\\nchannel dimensions, then apply a CNN to extract a feature\\nmap. Again, we apply two different fully connected layers\\non this feature map to produce µ2(t)andσ2(t). Similarly,\\nthe dimension of µ2(t)andσ2(t)to be 20. We then deﬁne\\nthe posterior of the latent variable z(t)in V AE as:\\nqθ(z(t)|concat (x(t),h(t−1)))\\n∼N(\\nµ2(t), diag(\\nσ2(t)2)) (3)\\nwherez(t)∈R20. To measure the distribution loss between\\nEq. 2 and Eq. 3 at time step t, we can use the KL-divergence\\nmetricKL(qθ(z(t)|x(t),h(t−1))||c(t)).\\nRecurrence: To capture the temporal information among\\nframes in a video, we use a ConvLSTM to represent the\\nrecurrent relationship among frames. From the current in-\\nput imagex(t), we apply a CNN to extract a feature map\\nwhich we denote as ϕx(x(t))∈RH′×W′×F. To match\\nthe dimension of this feature, we also resize the latent vari-\\nablez(t)(recallz(t)∈R20) as follows. We ﬁrst use\\nfully connected layers to map z(t)to a high-dimensional\\nspace R1024, then reshape to a 3D tensor of dimension\\nH′×W′×F= 16×16×32. We usezr(t)∈RH′×W′×Fto\\ndenote this reshaped tensor. We concatenate the input fea-\\ntureϕx(x(t))with thezr(t)along the channel dimension\\nand use it as the input to ConvLSTM at time t:\\nh(t) =fConvLSTM (concat (ϕx(x(t)),zr(t)),h(t−1))\\n(4)\\nDecoder: This module takes the resized hidden state zr(t)\\nas its input and produces a predicted frame x′(t+ 1) for\\nthe next time-step. Note that the dimensions of zr(t)\\nmatch those of the extracted feature of previous hidden state\\nϕh(h(t−1)). We concatenate zr(t)andϕh(h(t−1))along\\nthe channel dimension. The result is used as the input of this\\ndecoder module. The decoder is implemented as a decon-\\nvolutional nerual network that generates the predicted frame\\nx′(t+ 1)∈RH×W×3.Model Learning: For learning parameters in Conv-VRNN,\\nwe combine the least absolute deviation ( L1loss) [24],\\nmulti-scale structural similarity measurement (msssim loss)\\n[29] and gradient difference (gdl loss) [21] to deﬁne a loss\\nthat measure the quality of the predicted frame. These three\\nloss functions can be deﬁned as follows:\\n(1) L1 loss between ground-truth and prediction is the sum-\\nmation of the absolute value between every pixel of the two\\nimages.\\n(2)We use multi-scale SSIM to represent the structural dif-\\nference. MSSSIM is a multi-scale version of SSIM, which\\nperforms better on video sequences.\\n(3) Gradient difference is widely used for measuring the\\nperformance of a prediction. Gradient difference loss con-\\nsiders the intensities difference between neighbour pixels.\\nOverall, given the predicted frame x′(T+ 1) and the\\nground-truth x(T+1), the complete loss function is deﬁned\\nas:\\nLprediction =L1(x(T+ 1),x′(T+ 1))\\n+Lmsssim (x(T+ 1),x′(T+ 1))\\n+Lgdl(x′(T+ 1),x′(T+ 1))(5)\\nWe deﬁne the complete objective function as:\\nL=T∑\\nt=1(−KL(qθ(z(t)|x(t),h(t−1))||c(t))) +Lprediction.\\n(6)\\n4.2. Anomaly Detection\\nGiven an input sequence of frames x(1),x(2),...,x (T)\\nduring testing, we use our model to predict the next frame\\nx′(T+ 1) in the future. This predicted future frame x′(T+\\n1)is compared with the ground-truth future frame x(T+ 1)\\nby calculating Lprediction (see Eq. 5). Same as [15], after\\ncalculating the overall spatial loss of each testing video, we\\nnormalize the losses to get a score S(t)in the range of [0,1]\\nfor each frame in the video by:\\nS(t) =Lprediction (t)−minLprediction\\nmaxLprediction−minLprediction. (7)\\nWe then use S(t)as the score indicating how likely a par-\\nticular frame is an anomaly.\\n5. Experiments\\nIn this section, we ﬁrst discuss our experimental setup\\nin Sec. 5.1. Then we present both quantitative and qualita-\\ntive results in Sec. 5.2. We also perform extensive ablation\\nstudies in Sec. 5.3 to analyze our proposed approach.\\n', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 3}),\n",
       " Document(page_content='normal abnormal\\nFigure 3. Example frames from the three datasets. 1st row: UCSD\\nPedestrian 1 (Ped1) dataset; 2nd row: UCSD Pedestrian 2 (Ped2)\\ndataset; 3nd row: CUHK Avenue dataset. We show both nor-\\nmal and abnormal frames from these datasets. The abnormal be-\\nhaviours are indicated by the red bounding box. Note that the red\\nbounding box is only for visualization purpose and is not used dur-\\ning training.\\n5.1. Experimental Setup\\nDatasets: We evaluate our method on three benchmark\\ndatasets. (1) UCSD Pedestrian 1 (Ped 1) dataset[19]: this\\ndataset contains 34 training videos and 36 testing videos.\\nIn training videos, only pedestrians exist in the frames. Test\\nvideos include 40 abnormal events, such as moving bicycles\\nand vehicles. (2) UCSD Pedestrian 2 (Ped 2) dataset[19].\\nThis dataset considers the same set of anomalies with the\\nUCSD Ped 1 dataset. It consists of 16 training videos and\\n12 testing videos with 12 irregular occasions. (3) CUHK\\nAvenue (Avenue) dataset [16]. This dataset consists of 16\\ntraining videos and 21 testing videos. It contains 47 abnor-\\nmal events like throwing things, wandering, and running.\\nFigure 3 shows some example frames from these datasets.\\nEvaluation Metric: Following prior work [15] [18] [19],\\nwe evaluate our methods using the area under the ROC\\ncurve (AUC). The ROC curve is obtained by varying the\\nthreshold for the anomaly score. A higher AUC value repre-\\nsents a more accurate anomaly detection system. To ensure\\nthe comparability between different methods, we calculate\\nAUC from the frame-level prediction, which has been used\\nby different existing methods.\\n5.2. Experimental Results\\nTable 1 shows the results of our proposed method com-\\npared with existing state-of-the-art approaches. To be con-Table 1. Comparison of different methods in terms of AUC on\\nUCSD Ped1, UCSD Ped2 and CUHK Avenue datasets.\\nPed1 Ped2 Avenue\\nMPCCA [13] 59.0% 69.3% N/A\\nDel et al.[7] N/A N/A 78.3%\\nConv-AE [10] 75.0% 85.0% 80.0%\\nConvLSTM-AE [17] 75.5% 88.1% 77.0%\\nStacked RNN [18] N/A 92.2% 81.7%\\nLiu et al. [15] 83.1% 95.4% 84.9%\\nConv-VRNN (ours) 86.26% 96.06% 85.78%\\nTable 2. Comparision of Conv-V AEs versus Conv-VRNN in terms\\nof AUC on three datasets.\\nPed 1 Ped 2 Avenue\\nConv-V AE 82.42% 89.18% 81.82%\\nConv-VRNN 86.26% 96.06% 85.48%\\nsistent with [15], we have set T= 4. In other words, our\\nmodel takes 4 consecutive frames as the input and predicts\\nthe future frame at the next time step. It then compares\\nthe prediction with the actual frame at the next time step\\nto decide whether this frame is an anomaly. We can see\\nthat Conv-VRNN outperforms existing methods on all three\\ndatasets.\\nFigure 4 shows some qualitative examples of future\\nframe prediction. We can see that for a normal frame, the\\npredicted future frame tends to be close to the actual fu-\\nture prediction. For an abnormal frame, the predicted fu-\\nture frame tends to be blurry or distorted compared with\\nthe actual future frame. Figure 5 shows example of de-\\ntected anomaly by visualizing the anomaly score on differ-\\nent frames in a video.\\n5.3. Ablation Study\\nWe perform additional ablation study to gain further in-\\nsights of our proposed methods.\\n5.3.1 Conv-VAE vs Conv-VRNN\\nIn order to analyze the effect of incorporating temporal in-\\nformation, we implement a variant of our model without\\nRNN. We call this variant Conv-V AE. Conv-V AE uses the\\nencoder module to encode a latent variable and uses the de-\\ncoder module for prediction. We have experimented with\\nConv-V AE that takes either one input frame or four frames\\nto predict the next frame. The results are shown in Table 2.\\nWe can see that Conv-VRNN outperforms Conv-V AE. This\\ndemonstrates the importance of capturing the temporal in-\\nformation using RNN for anomaly detection.\\n', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 4}),\n",
       " Document(page_content='GT (normal) prediction (normal) GT (abnormal) prediction (abnormal)\\nFigure 4. Examples of frame predictions on three datasets. The 1st row shows predicted frames that are normal. The 2nd row shows\\npredicted frames with anomalies. For an abnormal frame, the predicted frame tends to be blurry and distorted. The bounding boxes are for\\nvisualization purpose and are not part of the model prediction.\\nTable 3. Evaluation of different combinations of various loss terms\\nin the objective functions in our Conv-VRNN network on the Ped1\\ndataset. The results show that the combination of all loss terms\\ngives the best performance.\\nL1 \\x13 \\x13 \\x13\\nLmsssim \\x17 \\x13 \\x13\\nLgdl \\x17 \\x17 \\x13\\nAUC 80.29% 83.34% 86.26%\\n5.3.2 Analysis on Losses\\nAs we mentioned in Sec 4, we apply three different losses\\nfor prediction. The analysis of the impact of the losses\\ncan be visualized in Table 3. We choose three combi-\\nnations of objective functions for evaluation: constraint\\nonly on intensity ( L1), constraint on intensity and structure\\n(L1+Lmsssim ), constraint on intensity, structure and gra-\\ndient (L1+Lmsssim +Lgdl). The results demonstrate that\\nthe appearance information is better captured by the model\\nwith more constraints.\\n5.3.3 Sequential Model vs Optical Flow\\nOur Conv-VRNN uses a RNN module to capture the tem-\\nporal information in a video. An alternative way of cap-\\nturing temporal information is to use optical ﬂow features.\\nWe have implemented a Conv-V AE model with such con-\\nstraint. Following [15], we apply the pretrained Flownet [8]\\nto estimate the optical ﬂow, and use the returned loss of\\nthe Flownet as a motion constraint of the network only in\\ntraining time. Table 4, Figure 6 show that although adding\\noptical ﬂow in our implementation of Conv-V AE improvesTable 4. Comparison between our Conv-VRNN model with dif-\\nferent V AE-based models (with or without optical ﬂow features).\\nOur proposed Conv-VRNN outperforms Conv-V AE (with optical\\nﬂow) even if our model does not use optical ﬂow features.\\nPed1 Ped2 Avenue\\nConv-V AE(w/o optical ﬂow) 80.15% 88.13% 80.92%\\nConv-V AE(with optical ﬂow) 81.36% 89.52% 82.23%\\nConv-VRNN 86.26% 96.06% 85.78%\\nthe performance compared with Conv-V AE applied on only\\nraw frames, our proposed Conv-VRNN approach still per-\\nforms better even if we do not use optical ﬂow features.\\nThis demonstrates that it is more effective to design the gen-\\nerative model to directly capture the temporal information\\ninstead of relying on low-level optical ﬂow features.\\n6. Conclusion\\nIn this paper, we have proposed a sequential genera-\\ntive network for anomaly detection based on convolutional\\nVRNN using the future frame prediction framework. By\\ncombining a ConvLSTM module with V AE, our approach\\ncan effectively capture the temporal information crucial\\nfor future frame prediction. On three benchmark datasets,\\nour proposed approach outperforms existing state-of-the-art\\nmethods.\\nAcknowledgement : This work was supported by the\\nNSERC and UMGF funding programs. We thank NVIDIA\\nfor donating some of the GPUs used in this work.\\n', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 5}),\n",
       " Document(page_content=\"Ped1 Ped2\\nAvenue\\nFigure 5. Examples of anomaly detection on three datasets. We plot the anomaly score of our model and the ground-truth anomaly score.\\nAgain, the bounding boxes are for visualization purpose.\\nPed1 Ped2 Avenue\\nFigure 6. ROC curves of our Conv-VRNN method, Conv-V AE (w/o optical ﬂow) and Conv-V AE (with optical ﬂow) on three datasets.1.0\\nGround Truth\\n>>>>\\nConv-VRNN\\n0.8\\n0.6\\nScore\\n0.4\\n0.2\\n0.0\\n25\\n100\\n125\\n50\\n75\\n150\\n175\\n200\\nFrames\\nDetection'of Anomaly\\nNormal Scenes1.0\\nGround Truth\\nConv-VRNN\\n0.8\\n0.6\\nScore\\n0.4\\n0.2\\n0.0\\n75\\n100\\n125\\n150\\n25\\n50\\n175\\nFrames\\nNormal Scenes\\nDetection of Anomaly1.0\\n0.8\\nTruePositiveRate\\n0.6\\n0.4\\n0.2\\nConv-VAE(w/0optical flow)(AUC=0.81)\\nConv-VAE(with optical flow)(AUC =0.82)\\nConv-VRNN(AUC = 0.86)\\n0.0\\n0.2\\n0.0\\n0.4\\n0.6\\n0.8\\n1.0\\nFalse Positive Rate1.0\\n0.8-\\nTruePositiveRate\\n0.6\\n0.4\\n0.2\\nConv-VAE(w/0optical flow)(AUC=0.88)\\nConv-VAE(with optical flow)(AUC =0.89)\\nConv-VRNN(AUC = 0.96)\\n0.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nFalse Positive Rate1.0\\n0.8\\nTrue Positive Rate\\n0.6\\n0.4\\n0.2\\nConv-VAE(w/0 optical flow)(AUC = 0.82)\\nConv-VAE(with optical flow)(AUC = 0.83)\\nConv-VRNN(AUC = 0.86)\\n0.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nFalse Positive Rate\", metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 6}),\n",
       " Document(page_content='References\\n[1] J. An and S. Cho. Variational autoencoder based anomaly\\ndetection using reconstruction probability. Special Lecture\\non IE , 2015.\\n[2] R. Chalapathy, A. K. Menon, and S. Chawla. Robust, deep\\nand inductive anomaly detection. In Joint European Con-\\nference on Machine Learning and Knowledge Discovery in\\nDatabases , 2017.\\n[3] J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and\\nY . Bengio. A recurrent latent variable model for sequential\\ndata. In Advances in Neural Information Processing Systems ,\\n2015.\\n[4] Y . Cong, J. Yuan, and J. Liu. Sparse reconstruction cost for\\nabnormal event detection. In IEEE Conference on Computer\\nVision and Pattern Recognition , 2011.\\n[5] N. Dalal and B. Triggs. Histograms of oriented gradients for\\nhuman detection. In IEEE Conference on Computer Vision\\nand Pattern Recognition , 2005.\\n[6] N. Dalal, B. Triggs, and C. Schmid. Human detection using\\noriented histograms of ﬂow and appearance. In European\\nConference on Computer Vision , 2006.\\n[7] A. Del Giorno, J. A. Bagnell, and M. Hebert. A discrimi-\\nnative framework for anomaly detection in large videos. In\\nEuropean Conference on Computer Vision , 2016.\\n[8] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,\\nV . Golkov, P. Van Der Smagt, D. Cremers, and T. Brox.\\nFlownet: Learning optical ﬂow with convolutional net-\\nworks. In IEEE International Conference on Computer Vi-\\nsion, 2015.\\n[9] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\\nD. Warde-Farley, S. Ozair, A. Courville, and Y . Bengio. Gen-\\nerative adversarial nets. In Advances in Neural Information\\nProcessing Systems , 2014.\\n[10] M. Hasan, J. Choi, J. Neumann, A. K. Roy-Chowdhury,\\nand L. S. Davis. Learning temporal regularity in video se-\\nquences. In IEEE Conference on Computer Vision and Pat-\\ntern Recognition , 2016.\\n[11] J. R. Hershey and P. A. Olsen. Approximating the kull-\\nback leibler divergence between gaussian mixture models.\\nInIEEE International Conference on Acoustics, Speech and\\nSignal Processing , 2007.\\n[12] P. Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros. Image-to-\\nimage translation with conditional adversarial networks. In\\nIEEE Conference on Computer Vision and Pattern Recogni-\\ntion, 2017.\\n[13] J. Kim and K. Grauman. Observe locally, infer globally: a\\nspace-time mrf for detecting abnormal activities with incre-\\nmental updates. In IEEE Conference on Computer Vision\\nand Pattern Recognition , 2009.\\n[14] D. P. Kingma and M. Welling. Auto-encoding variational\\nbayes. arXiv:1312.6114 , 2013.\\n[15] W. Liu, W. Luo, D. Lian, and S. Gao. Future frame prediction\\nfor anomaly detection–a new baseline. In IEEE Conference\\non Computer Vision and Pattern Recognition , 2018.\\n[16] C. Lu, J. Shi, and J. Jia. Abnormal event detection at 150 fps\\nin matlab. In IEEE International Conference on Computer\\nVision , 2013.[17] W. Luo, W. Liu, and S. Gao. Remembering history with con-\\nvolutional lstm for anomaly detection. In IEEE International\\nConference on Multimedia and Expo , 2017.\\n[18] W. Luo, W. Liu, and S. Gao. A revisit of sparse coding based\\nanomaly detection in stacked rnn framework. In IEEE Inter-\\nnational Conference on Computer Vision , 2017.\\n[19] V . Mahadevan, W. Li, V . Bhalodia, and N. Vasconcelos.\\nAnomaly detection in crowded scenes. In IEEE Conference\\non Computer Vision and Pattern Recognition , 2010.\\n[20] J. Masci, U. Meier, D. Cires ¸an, and J. Schmidhuber. Stacked\\nconvolutional auto-encoders for hierarchical feature extrac-\\ntion. In International Conference on Artiﬁcial Neural Net-\\nworks , 2011.\\n[21] M. Mathieu, C. Couprie, and Y . LeCun. Deep multi-scale\\nvideo prediction beyond mean square error. In International\\nConference on Learning Representations , 2016.\\n[22] J. R. Medel and A. Savakis. Anomaly detection in video\\nusing predictive convolutional long short-term memory net-\\nworks. arXiv:1612.00390 , 2016.\\n[23] O. Mogren. C-rnn-gan: Continuous recurrent neural net-\\nworks with adversarial training. Constructive Machine\\nLearning Workshop at NIPS , 2016.\\n[24] D. Pollard. Asymptotics for least absolute deviation regres-\\nsion estimators. Econometric Theory , 1991.\\n[25] M. Sabokrou, M. Fathy, and M. Hoseini. Video anomaly\\ndetection and localization based on the sparsity and recon-\\nstruction error of auto-encoder. Electronics Letters , 2016.\\n[26] M. Sabokrou, M. Khalooei, M. Fathy, and E. Adeli. Adver-\\nsarially learned one-class classiﬁer for novelty detection. In\\nIEEE Conference on Computer Vision and Pattern Recogni-\\ntion, 2018.\\n[27] F. Tung, J. S. Zelek, and D. A. Clausi. Goal-based trajec-\\ntory analysis for unusual behaviour detection in intelligent\\nsurveillance. Image and Vision Computing , 2011.\\n[28] P. Vincent, H. Larochelle, Y . Bengio, and P.-A. Manzagol.\\nExtracting and composing robust features with denoising au-\\ntoencoders. In International Conference on Machine Learn-\\ning, 2008.\\n[29] Z. Wang, E. P. Simoncelli, and A. C. Bovik. Multiscale struc-\\ntural similarity for image quality assessment. In The Thrity-\\nSeventh Asilomar Conference on Signals, Systems & Com-\\nputers , 2003.\\n[30] S. Wu, B. E. Moore, and M. Shah. Chaotic invariants\\nof lagrangian particle trajectories for anomaly detection in\\ncrowded scenes. In IEEE Conference on Computer Vision\\nand Pattern Recognition , 2010.\\n[31] J. Xie, L. Xu, and E. Chen. Image denoising and inpainting\\nwith deep neural networks. In Advances in Neural Informa-\\ntion Processing Systems , 2012.\\n[32] D. Xu, Y . Yan, E. Ricci, and N. Sebe. Detecting anomalous\\nevents in videos by learning deep representations of appear-\\nance and motion. Computer Vision and Image Understand-\\ning, 2017.\\n[33] B. Zhao, L. Fei-Fei, and E. P. Xing. Online detection of un-\\nusual events in videos via dynamic sparse coding. In IEEE\\nConference on Computer Vision and Pattern Recognition ,\\n2011.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 7}),\n",
       " Document(page_content='Future Frame Prediction for Anomaly Detection – A New Baseline\\nWen Liu∗, Weixin Luo∗, Dongze Lian, Shenghua Gao†\\nShanghaiTech University\\n{liuwen, luowx, liandz, gaoshh }@shanghaitech.edu.cn\\nAbstract\\nAnomaly detection in videos refers to the identiﬁcation of\\nevents that do not conform to expected behavior. However,\\nalmost all existing methods tackle the problem by minimiz-\\ning the reconstruction errors of training data, which can-\\nnot guarantee a larger reconstruction error for an abnor-\\nmal event. In this paper, we propose to tackle the anomaly\\ndetection problem within a video prediction framework. To\\nthe best of our knowledge, this is the ﬁrst work that lever-\\nages the difference between a predicted future frame and\\nits ground truth to detect an abnormal event. To predict a\\nfuture frame with higher quality for normal events, other\\nthan the commonly used appearance (spatial) constraints\\non intensity and gradient, we also introduce a motion (tem-\\nporal) constraint in video prediction by enforcing the opti-\\ncal ﬂow between predicted frames and ground truth frames\\nto be consistent, and this is the ﬁrst work that introduces\\na temporal constraint into the video prediction task. Such\\nspatial and motion constraints facilitate the future frame\\nprediction for normal events, and consequently facilitate\\nto identify those abnormal events that do not conform the\\nexpectation. Extensive experiments on both a toy dataset\\nand some publicly available datasets validate the effec-\\ntiveness of our method in terms of robustness to the un-\\ncertainty in normal events and the sensitivity to abnormal\\nevents. All codes are released in https://github.\\ncom/StevenLiuWen/ano_pred_cvpr2018 .\\n1. Introduction\\nAnomaly detection in videos refers to the identiﬁcation\\nof events that do not conform to expected behavior [3]. It\\nis an important task because of its applications in video\\nsurveillance. However, it is extremely challenging because\\nabnormal events are unbounded in real applications, and it\\nis almost infeasible to gather all kinds of abnormal events\\nand tackle the problem with a classiﬁcation method.\\n∗The authors contribute equally and are listed in alphabetical order.\\n†Corresponding author.\\nInput \\n\\x7f ,\\x7f!,…,\\x7f\"Ground truth \\n\\x7f\"# Prediction \\n$\\x7f\"# normal abnormal \\nFigure 1. Some predicted frames and their ground truth in nor-\\nmal and abnormal events. Here the region is walking zone. When\\npedestrians are walking in the area, the frames can be well pre-\\ndicted. While for some abnormal events (a bicycle intrudes/ two\\nmen are ﬁghting), the predictions are blurred and with color dis-\\ntortion. Best viewed in color.\\nLots of efforts have been made for anomaly detec-\\ntion [20][13][23]. Of all these work, the idea of feature\\nreconstruction for normal training data is a commonly used\\nstrategy. Further, based on the features used, all existing\\nmethods can be roughly categorized into two categories: i)\\nhand-crafted features based methods [6][20]. They repre-\\nsent each video with some hand-crafted features including\\nappearance and motion ones. Then a dictionary is learnt to\\nreconstruct normal events with small reconstruction errors.\\nIt is expected that the features corresponding to abnormal\\nevents would have larger reconstruction errors. But since\\nthe dictionary is not trained with abnormal events and it is\\nusually overcomplete, we cannot guarantee the expectation.\\nii) deep learning based methods [13][5][26]. They usually\\nlearn a deep neural network with an Auto-Encoder way and\\nthey enforce it to reconstruct normal events with small re-\\nconstruction errors. But the capacity of deep neural network\\nis high, and larger reconstruction errors for abnormal events\\ndo not necessarily happen. Thus, we can see that almost all\\ntraining data reconstruction based methods cannot guaran-\\n1\\narXiv:1712.09867v3  [cs.CV]  13 Mar 2018', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 8}),\n",
       " Document(page_content='Flownet Optical \\nFlow Loss \\n\\x7f\\x81\\x8d\\x8f\\x90\\x81\\x8d\\x8f\\x90Flownet \\nIntensity Loss and \\nGradient Loss Generator \\n(U-Net) Discriminator \\n\\x81\\x90,\\x81 ,…,\\x81\\x8d\\nReal or Fake \\x81\\x8d\\nFigure 2. The pipeline of our video frame prediction network. Here we adopt U-Net as generator to predict next frame. To generate\\nhigh quality image, we adopt the constraints in terms of appearance (intensity loss and gradient loss) and motion (optical ﬂow loss).\\nHere Flownet is a pretrained network used to calculate optical ﬂow. We also leverage the adversarial training to discriminate whether the\\nprediction is real or fake.\\ntee the ﬁnding of abnormal events.\\nIt is interesting that even though anomaly is deﬁned as\\nthose events do not conform the expectation, most existing\\nwork in computer vision solve the problem within a frame-\\nwork of reconstructing training data [20][38][13]. We pre-\\nsume it is probable that the video frame prediction is far\\nfrom satisfactory at that time. Recently, as the emergence\\nof Generative Adversarial Network (GAN) [12], the perfor-\\nmance of video prediction has been greatly advanced [25].\\nIn this paper, rather than reconstructing training data for\\nanomaly detection, we propose to identify abnormal events\\nby comparing them with their expectation, and introduce\\na future video frame prediction based anomaly detection\\nmethod. Speciﬁcally, given a video clip, we predict the fu-\\nture frame based on its historical observation. We ﬁrst train\\na predictor that can well predict the future frame for nor-\\nmal training data. In the testing phase, if a frame agrees\\nwith its prediction, it potentially corresponds to a normal\\nevent. Otherwise, it potentially corresponds to an abnormal\\nevent. Thus a good predictor is a key to our task. We im-\\nplement our predictor with an U-Net [28] network architec-\\nture given its good performance at image-to-image trans-\\nlation [15]. First, we impose a constraint on the appear-\\nance by enforcing the intensity and gradient maps of the\\npredicted frame to be close to its ground truth; Then, mo-\\ntion is another important feature for video characterization,\\nand a good prediction should be consistent with real object\\nmotion. Thus we propose to introduce a motion constraint\\nby enforcing the optical ﬂow between predicted frames to\\nbe close to their ground truth. Further, we also add a Gen-\\nerative Adversarial Network (GAN) [12] module into our\\nframework in light of its success for video generation [25]\\nand image generation [9].\\nWe summarize our contributions as follows: i) We\\npropose a future frame prediction based framework foranomaly detection. Our solution agrees with the concept of\\nanomaly detection that normal events are predictable while\\nabnormal ones are unpredictable. Thus our solution is more\\nsuitable for anomaly detection. To the best of our knowl-\\nedge, it is the ﬁrst work that leverages video prediction for\\nanomaly detection; ii) For the video frame prediction frame-\\nwork, other than enforcing predicted frames to be close to\\ntheir ground truth in spatial space, we also enforce the opti-\\ncal ﬂow between predicted frames to be close to their optical\\nﬂow ground truth. Such a temporal constraint is shown to\\nbe crucial for video frame prediction, and it is also the ﬁrst\\nwork that leverages a motion constraint for anomaly detec-\\ntion; iii) Experiments on toy dataset validate the robustness\\nto the uncertainty for normal events, which validates the ro-\\nbustness of our method. Further, extensive experiments on\\nreal datasets show that our method outperforms all existing\\nmethods.\\n2. Related Work\\n2.1. Hand-crafted Features Based Anomaly Detec-\\ntion\\nHand-crafted features based anomaly detection is mainly\\ncomprised of three modules: i) extracting features; In this\\nmodule, the features are either hand-crafted or learnt on\\ntraining set; ii) learning a model to characterize the distri-\\nbution of normal scenarios or encode regular patterns; iii)\\nidentifying the isolated clusters or outliers as anomalies.\\nFor feature extraction module, early work usually utilizes\\nlow-level trajectory features, a sequence of image coordi-\\nnates, to represent the regular patterns [32][35]. However,\\nthese methods are not robust in complex or crowded scenes\\nwith multiple occlusions and shadows, because trajectory\\nfeatures are based on object tracking and it is very easy to\\nfail in these cases. Taking consideration of the shortcom-\\n2', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 9}),\n",
       " Document(page_content='ings of trajectory features, low-level spatial-temporal fea-\\ntures, such as histogram of oriented gradients (HOG) [27],\\nhistogram of oriented ﬂows (HOF) [7] are widely used.\\nBased on spatial-temporal features, Zhang et al. [37] ex-\\nploit a Markov random ﬁled (MRF) for modeling the nor-\\nmal patterns. Adam et al. [2] characterize the regularly lo-\\ncal histograms of optical ﬂow by an exponential distribu-\\ntion. Kim and Grauman [16] model the local optical ﬂow\\npattern with a mixture of probabilistic PCA (MPPCA). Ma-\\nhadevan et al. [23] ﬁt a Gaussian mixture model to mixture\\nof dynamic textures (MDT). Besides these statistic models,\\nsparse coding or dictionary learning is also a popular ap-\\nproach to encode the normal patterns [38][20][6]. The fun-\\ndamental underlying assumption of these methods is that\\nany regular pattern can be linearly represented as a linear\\ncombination of basis of a dictionary which encodes normal\\npatterns on training set. Therefore, a pattern is considered as\\nan anomaly if its reconstruction error is high and vice verse.\\nHowever, optimizing the sparse coefﬁcients is usually time-\\nconsuming in sparse reconstruction based methods. In order\\nto accelerate both in training and testing phase, Lu et al [20]\\npropose to discard the sparse constraint and learn multiple\\ndictionaries to encode normal scale-invariant patches.\\n2.2. Deep Learning Based Anomaly Detection.\\nDeep learning approaches have demonstrated their suc-\\ncesses in many computer vision tasks [18][11] as well as\\nanomaly detection [13]. In the work [36], Xu et al . de-\\nsign a multi-layer auto-encoder for feature learning, which\\ndemonstrates the effectiveness of deep learning features. In\\nanother work [13], a 3D convolutional auto-encoder (Conv-\\nAE) is proposed by Hasan to model regular frames. Fur-\\nther, motivated by the observation that Convolutional Neu-\\nral Networks (CNN) has strong capability to learn spatial\\nfeatures, while Recurrent Neural Network (RNN)and its\\nlong short term memory (LSTM) variant have been widely\\nused for sequential data modeling. Thus, by taking both\\nadvantages of CNN and RNN, [5][21] leverage a Convo-\\nlutional LSTMs Auto-Encoder (ConvLSTM-AE) to model\\nnormal appearance and motion patterns at the same time,\\nwhich further boosts the performance of the Conv-AE based\\nsolution. In [22], Luo et al. propose a temporally coherent\\nsparse coding based method which can map to a stacked\\nRNN framework. Besides, Ryota et al. [14] combine de-\\ntection and recounting of abnormal events. However, all\\nthese anomaly detections are based on the reconstruction of\\nregular training data, even though all these methods assume\\nthat abnormal events would correspond to larger reconstruc-\\ntion errors, due to the good capacity and generalization of\\ndeep neural network, this assumption does not necessarily\\nhold. Therefore, reconstruction errors of normal and abnor-\\nmal events will be similar, resulting in less discrimination.2.3. Video Frame Prediction\\nRecently, prediction learning is attracting more and more\\nresearchers’ attention in light of its potential applications in\\nunsupervised feature learning for video representation [25].\\nIn [29], Shi et al. propose to modify original LSTM with\\nConvLSTM and use it for precipitation forecasting. In [25],\\na multi-scale network with adversarial training is proposed\\nto generate more natural future frames in videos. In [19],\\na predictive neural network is designed and each layer in\\nthe network also functions as making local predictions and\\nonly forwarding deviations. All aforementioned work fo-\\ncuses on how to directly predict future frames. Different\\nfrom these work, recently, people propose to predict trans-\\nformations needed for generating future frames [33] and [4],\\nwhich further boosts the performance of video prediction.\\n3. Future Frame Prediction Based Anomaly\\nDetection Method\\nSince anomaly detection is the identiﬁcation of events\\nthat do not conform the expectation, it is more natural\\nto predict future video frames based on previous video\\nframes, and compare the prediction with its ground truth\\nfor anomaly detection. Thus we propose to leverage video\\nprediction for anomaly detection. To generate a high qual-\\nity video frame, most existing work [15][25] only consid-\\ners appearance constraints by imposing intensity loss [25],\\ngradient loss [25], or adversarial training loss [15]. How-\\never, only appearance constraints cannot guarantee to char-\\nacterize the motion information well. Besides spatial in-\\nformation, temporal information is also an important fea-\\nture of videos. So we propose to add an optical ﬂow con-\\nstraint into the objective function to guarantee the motion\\nconsistency for normal events in training set, which further\\nboosts the performance for anomaly detection, as shown in\\nthe experiment section (section 4.5 and 4.6). It is worth\\nnoting abnormal events can be justiﬁed by either appear-\\nance (A giant monster appears in a shopping mall) or mo-\\ntion (A pickpocket walks away from an unlucky guy), and\\nour future frame prediction solution leverages both the ap-\\npearance and motion loss for normal events, therefore these\\nabnormal events can be easily identiﬁed by comparing the\\nprediction and ground truth. Thus the appearance and mo-\\ntion losses based video prediction are more consistent with\\nanomaly detection.\\nMathematically, given a video with consecutive tframes\\nI1,I2,...,I t, we sequentially stack all these frames and use\\nthem to predict a future frame It+1. We denote our predic-\\ntion as ˆIt+1. To make ˆIt+1close toIt+1, we minimize their\\ndistance regarding intensity as well as gradient. To pre-\\nserve the temporal coherence between neighboring frames,\\nwe enforce the optical ﬂow between It+1andItand that\\nbetween ˆIt+1andItto be close. Finally, the difference\\n3', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 10}),\n",
       " Document(page_content='Max pooling \\nConvolution \\nDeconvolution \\nConcatenate 256x256 3 64 64 128x128 64 128 128 \\n128 64x64 256 256 \\n256 32x32 512 512 512 \\n128x128 256 128 \\n256 256 128 128 64 64 3256x256 \\n256x256 \\n128x128 \\n128x128 \\n64x64 64x64 \\n32x32 32x32 64x64 \\n64x64 64x64 \\n128x128 \\n128x128 256x256 \\n256x256 \\n256x256 \\n256x256 Figure 3. The network architecture of our main prediction network\\n(U-Net). The resolutions of input and output are the same.\\nbetween a future frame’s prediction and itself determines\\nwhether it is normal or abnormal. The network architecture\\nof our framework is shown in Fig. 2. Next, we will intro-\\nduce all the components of our framework in details.\\n3.1. Future Frame Prediction\\nThe network commonly used for frame generation or\\nimage generation in existing work [25][13] usually con-\\ntains two modules: i) an encoder which extracts features by\\ngradually reducing the spatial resolution; and ii) a decoder\\nwhich gradually recovers the frame by increasing the spa-\\ntial resolution. However, such a solution confronts with the\\ngradient vanishing problem and information imbalance in\\neach layer. To avoid this, U-Net[28] is proposed by adding\\na shortcut between a high level layer and a low level layer\\nwith the same resolution. Such a manner suppresses gra-\\ndient vanishing and results in information symmetry. We\\nslightly modify U-Net for future frame prediction in our\\nimplementation. Speciﬁcally, for each two convolution lay-\\ners, we keep output resolution unchanged. Consequently, it\\ndoes not need the crop and resize operations anymore when\\nadding shortcuts. The details of this network are illustrated\\nin Figure 3. The kernel sizes of all convolution and decon-\\nvolution are set to 3×3and that of max pooling layers are\\nset to 2×2.\\n3.2. The Constraints on Intensity and Gradient\\nTo make the prediction close to its ground truth, follow-\\ning the work [25], intensity and gradient difference are used.\\nThe intensity penalty guarantees the similarity of all pixels\\nin RGB space, and the gradient penalty can sharpen the gen-\\nerated images. Speciﬁcally, we minimize the ℓ2distance be-\\ntween a predicted frame ˆIand its ground true Iin intensity\\nspace as follows:\\nLint(ˆI,I) =∥ˆI−I∥2\\n2 (1)Further, we deﬁne the gradient loss by following previous\\nwork [25] as follows:\\nLgd(ˆI,I) =∑\\ni,j\\ued79\\ued79|ˆIi,j−ˆIi−1,j|−|Ii,j−Ii−1,j|\\ued79\\ued79\\n1\\n+\\ued79\\ued79|ˆIi,j−ˆIi,j−1|−|Ii,j−Ii,j−1|\\ued79\\ued79\\n1(2)\\nwherei,jdenote the spatial index of a video frame.\\n3.3. The Constraint on Motion\\nPrevious work [25] only considers the difference be-\\ntween intensity and gradient for future frame generation,\\nand it can not guarantee to predict a frame with the correct\\nmotion. This is because even a small change occurs in terms\\nof the pixel intensity of all pixels in a predicted frame, even\\nthough it corresponds to a small prediction error in terms\\nof gradient and intensity, it may result in totally different\\noptical ﬂow, which is a good estimator of motion [30]. So\\nit is desirable to guarantee the correctness of motion pre-\\ndiction. Especially for anomaly detection, the coherence of\\nmotion is an important factor for the evaluation of normal\\nevents. Therefore, we introduce a temporal loss deﬁned as\\nthe difference between optical ﬂow of prediction frames and\\nground truth. However, the calculation of optical ﬂow is not\\neasy. Recently, a CNN based approach has been proposed\\nfor optical ﬂow estimation [8]. Thus we use the Flownet\\n[8] for optical ﬂow estimation. We denote fas the Flownet,\\nthen the loss in terms of optical ﬂow can be expressed as\\nfollows:\\nLop=∥f(ˆIt+1,It)−f(It+1,It)∥1 (3)\\nIn our implementation, fis pre-trained on a synthesized\\ndataset [8], and all the parameters in fare ﬁxed.\\n3.4. Adversarial Training\\nGenerative adversarial networks (GAN) have demon-\\nstrated its usefulness for image and video generation\\n[9][25]. By following [25], we also leverage a variant of\\nGAN (Least Square GAN [24]) module for generating a\\nmore realistic frame. Usually GAN contains a discrimi-\\nnative networkDand a generator network G.Glearns to\\ngenerate frames that are hard to be classiﬁed by D, while\\nDaims to discriminate the frames generated by G. Ideally,\\nwhenGis well trained,Dcannot predict better than chance.\\nIn practice, adversarial training is implemented with an al-\\nternative update manner. Moreover, we treat the U-Net\\nbased prediction network as G. As forD, we follow [15]\\nand utilize a patch discriminator which means each output\\nscalar ofDcorresponds a patch of an input image. Totally,\\nthe training schedule is illustrated as follows:\\nTrainingD. The goal of training Dis to classify It+1\\ninto class 1 andG(I1,I2,...,I t) =ˆIt+1into class 0, where 0\\nand 1 represent fake and genuine labels, respectively. When\\n4', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 11}),\n",
       " Document(page_content='trainingD, we ﬁx the weights of G, and a Mean Square\\nError (MSE) loss function is imposed:\\nLD\\nadv(ˆI,I) =∑\\ni,j1\\n2LMSE(D(I)i,j,1)\\n+∑\\ni,j1\\n2LMSE(D(ˆI)i,j,0)(4)\\nwherei,jdenotes the spatial patches indexes and LMSE is\\na MSE function, which is deﬁned as follows:\\nLMSE(ˆY,Y) = ( ˆY−Y)2(5)\\nwhereYtakes values in{0,1}andˆY∈[0,1]\\nTrainingG. The goal of training Gis to generate frames\\nwhereDclassify them into class 1. When training G, the\\nweights ofDare ﬁxed. Again, a MSE function is imposed\\nas follows:\\nLG\\nadv(ˆI) =∑\\ni,j1\\n2LMSE(D(ˆI)i,j,1) (6)\\n3.5. Objective Function\\nWe combine all these constraints regarding appearance,\\nmotion, and adversarial training, into our objective function,\\nand arrive at the following objective function:\\nLG=λintLint(ˆIt+1,It+1)\\n+λgdLgd(ˆIt+1,It+1)\\n+λopLop\\n+λadvLG\\nadv(ˆIt+1)(7)\\nWhen we trainD, we use the following loss function:\\nLD=LD\\nadv(ˆIt+1,It+1) (8)\\nTo train the network, the intensity of pixels in all frames\\nare normalized to [-1, 1] and the size of each frame is re-\\nsized to 256×256. We sett= 4 and use a random\\nclip of 5 sequential frames which is the same with [25].\\nAdam [17] based Stochastic Gradient Descent method is\\nused for parameter optimization. The mini-batch size is 4.\\nFor gray scale datasets, the learning rate of generator and\\ndiscriminator are set to 0.0001 and 0.00001, respectively.\\nWhile for color scale datasets, the learning rate of gener-\\nator and discriminator start from 0.0002 and 0.00002, re-\\nspectively. For different datasets, the coefﬁcient factors of\\nλint,λgd,λopandλadvare slightly different. An easy way\\nis to setλint,λgd,λopandλadvas 1.0, 1.0, 2.0 and 0.05,\\nrespectively.3.6. Anomaly Detection on Testing Data\\nWe assume that normal events can be well predicted.\\nTherefore, we can use the difference between predicted\\nframe ˆIand its ground truth Ifor anomaly prediction. MSE\\nis one popular way to measure the quality of predicted im-\\nages by computing a Euclidean distance between the predic-\\ntion and its ground truth of all pixels in RGB color space.\\nHowever, Mathieu [25] shows that Peak Signal to Noise Ra-\\ntio (PSNR) is a better way for image quality assessment,\\nshown as following:\\nPSNR (I,ˆI) = 10 log10[max ˆI]2\\n1\\nN∑N\\ni=0(Ii−ˆIi)2\\nHigh PSNR of the t-th frame indicates that it is more likely\\nto be normal. After calculating each frame’s PSNR of each\\ntesting video, following the work [25], we normalize PSNR\\nof all frames in each testing video to the range [0, 1] and\\ncalculate the regular score for each frame by using the fol-\\nlowing equation:\\nS(t) =PSNR (It,ˆIt)−mintPSNR (It,ˆIt)\\nmax tPSNR (It,ˆIt)−mintPSNR (It,ˆIt)\\nTherefore, we can predict whether a frame is normal or ab-\\nnormal based its score S(t). One can set a threshold to dis-\\ntinguish regular or irregular frames.\\n4. Experiments\\nIn this section, we evaluate our proposed method as\\nwell as the functionalities of different components on three\\npublicly available anomaly detection datasets, including\\nthe CUHK Avenue dataset [20], the UCSD Pedestrian\\ndataset [23] and the ShanghaiTech dataset [22]. We further\\nuse a toy dataset to validate the robustness of our method,\\ni.e., even if there exists some uncertainties in normal events,\\nour method can still correctly classify normal and abnormal\\nevents.\\n4.1. Datasets\\nHere we brieﬂy introduce the datasets used in our exper-\\niments. Some samples are shown in Fig. 4.\\n•CUHK Avenue dataset contains 16 training videos and\\n21 testing ones with a total of 47 abnormal events, in-\\ncluding throwing objects, loitering and running. The\\nsize of people may change because of the camera po-\\nsition and angle.\\n•The UCSD dataset contains two parts: The UCSD\\nPedestrian 1 (Ped1) dataset and the UCSD Pedestrian 2\\n(Ped2) dataset. The UCSD Pedestrian 1 (Ped1) dataset\\nincludes 34 training videos and 36 testing ones with\\n5', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 12}),\n",
       " Document(page_content='Table 1. AUC of different methods on the Avenue, Ped1, Ped2 and ShanghaiTech datasets.\\nCUHK Avenue UCSD Ped1 UCSD Ped2 ShanghaiTech\\nMPPCA [16] N/A 59.0% 69.3% N/A\\nMPPC+SFA [23] N/A 66.8% 61.3% N/A\\nMDT [23] N/A 81.8% 82.9% N/A\\nConv-AE [13] 80.0% 75.0% 85.0% 60.9%\\nDelet al. [10] 78.3% N/A N/A N/A\\nConvLSTM-AE [21] 77.0% 75.5% 88.1% N/A\\nUnmasking [31] 80.6% 68.4% 82.2% N/A\\nHinami et al. [14] N/A N/A 92.2% N/A\\nStacked RNN [22] 81.7% N/A 92.2% 68.0%\\nOur proposed method 84.9% 83.1% 95.4% 72.8%\\nNormal Abnormal UCSD Ped1 UCSD Ped2 CUHK Avenue \\n ShanghaiTech \\nFigure 4. Some samples including normal and abnormal frames in\\nthe UCSD, CUHK Avenue and ShanghaiTech datasets are illus-\\ntrated. Red boxes denote anomalies in abnormal frames.\\n40 irregular events. All of these abnormal cases are\\nabout vehicles such as bicycles and cars. The UCSD\\nPedestrian 2 (Ped2) dataset contains 16 training videos\\nand 12 testing videos with 12 abnormal events. The\\ndeﬁnition of anomaly for Ped2 is the same with Ped1.\\nUsually different methods are evaluated on these two\\nparts separately.\\n•The ShanghaiTech dataset is a very challenging\\nanomaly detection dataset. It contains 330 training\\nvideos and 107 testing ones with 130 abnormal events.\\nTotally, it consists of 13 scenes and various anomaly\\ntypes. Following the setting used in [22], we train the\\nmodel on all scenes.\\n4.2. Evaluation Metric\\nIn the literature of anomaly detection [20][23], a popu-\\nlar evaluation metric is to calculate the Receiver Operation\\nCharacteristic (ROC) by gradually changing the threshold\\nof regular scores. Then the Area Under Curve (AUC) is cu-\\nmulated to a scalar for performance evaluation. A higher\\nvalue indicates better anomaly detection performance. In\\nthis paper, following the work [22], we leverage frame-level\\nAUC for performance evaluation.4.3. Comparison with Existing Methods\\nIn this section, we compare our method with different\\nhand-craft features based method [16][23][34][10] and lat-\\nest deep learning based methods [13][31][14][22]. The\\nAUC of different methods is listed in Table 1. We can see\\nthat our method outperforms all existing methods (around\\n(3–5)% on all datasets), which demonstrates the effective-\\nness of our method.\\n4.4. The Design of Prediction Network\\nIn our anomaly detection framework, the future frame\\nprediction network is an important module. To evaluate\\nhow different prediction networks affect the performance\\nof anomaly detection, we compare our U-Net prediction\\nnetwork with Beyond Mean Square Error (Beyond-MSE)\\n[25] which achieves state-of-the-art performance for video\\ngeneration. Beyond-MSE leverages a multi-scale prediction\\nnetwork to gradually generate video frames with larger spa-\\ntial resolution. Because of its multi-scale strategy, it is much\\nslower than U-Net. To be consistent with Beyond-MSE, we\\nadapt our network architecture by removing the motion con-\\nstraint and only use the intensity loss, the gradient loss and\\nadversarial training in our U-Net based solution.\\nQuantitative comparison for anomaly detection. We\\nﬁrst compute the gap between average score of normal\\nframes and that of abnormal frames, denoted as ∆s. We\\ncompare the result of U-Net with that of Beyond-MSE on\\nthe Ped1 and Ped2 datasets, respectively. Larger ∆smeans\\nthe network can be more capable to distinguish normal and\\nabnormal patterns. Then, we also compare the U-Net based\\nsolution and Beyond-MSE with the AUC metric on the Ped1\\nand Ped2 datasets, respectively. We demonstrate the results\\nin Table 2. We can see that our method both achieves a\\nlarger ∆sand higher AUC than Beyond-MSE, which show\\nthat our network is more suitable for anomaly detection than\\nBeyond-MSE. Therefore, we adapt U-Net architecture as\\nour prediction network. As we aforementioned, the results\\nlisted here do not contain motion constraint, which would\\n6', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 13}),\n",
       " Document(page_content='further boost the AUC.\\nTable 2. The gap ( ∆s) and AUC of different prediction networks\\nin the Ped1 and Ped2 datasets.\\nPed1 Ped2\\n∆s AUC ∆s AUC\\nBeyond-MSE 0.200 75.8% 0.396 88.5%\\nU-Net 0.243 81.8% 0.435 93.5%\\n\\x7f !\" P P P P\\n\\x7f#$ — P P P\\n\\x7f%$& — — P P\\n\\x7f\\'( — — — P\\n)*+ 82.0% 82.6% 83.7% 84.9% 0.745 \\n0.718 \\n0.757 \\n0.844 0.487 \\n0.456 \\n0.487 \\n0.569 0.258 \\n0.262 \\n0.270\\n0.275 \\n00.5 SCORE normal abnormal gap \\nFigure 5. The evaluation of different components in our future\\nframe prediction network in the Avenue dataset. Each column in\\nthe histogram corresponds to a method with different loss func-\\ntions. We calculate the average scores of normal and abnormal\\nevents in the testing set. The gap is calculated by subtracting the\\nabnormal score from the normal one.\\n0.243 0.384 \\n0.256 0.259 0.469 \\n0.275 \\n0.2 0.25 0.3 0.35 0.4 0.45 0.5 \\nPed1 Ped2 CUHK Avenue Score Gap \\nConv-AE Our method \\nFigure 6. We ﬁrstly compute the average score for normal frames\\nand that for abnormal frames in the testing set of the Ped1, Ped2\\nand Avenue datasets. Then, we calculate the difference of these\\ntwo scores( ∆s) to measure the ability of our method and Conv-AE\\nto discriminate normal and abnormal frames. A larger gap( ∆s)\\ncorresponds to small false alarm rate and higher detection rate.\\nThe results show that our method consistently outperforms Conv-\\nAE in term of the score gap between normal and abnormal events.Table 3. AUC for anomaly detection of networks with/wo the mo-\\ntion constraint in Ped1 and Ped2.\\nPed1 Ped2\\nwithout motion constraint 81.8% 93.5%\\nwith motion constraint 83.1% 95.4%\\n4.5. Impact of Constraint on Motion.\\nTo evaluate the importance of motion constraint for\\nvideo frame generation as well as anomaly detection, we\\nconduct the experiment by removing the constraint from the\\nobjective in the training. Then we compare such a baseline\\nwith our method.\\nEvaluation of motion constraint with optical ﬂow\\nmaps. We show the optical ﬂow maps generated\\nwith/without motion constraint in Fig. 7, we can see that\\nthe optical ﬂow generated with motion constraint is more\\nconsistent with ground truth, which shows that such motion\\nconstraint term helps our prediction network to capture mo-\\ntion information more precisely. We also compare the MSE\\nbetween optical ﬂow maps generated with/without motion\\nconstraint and the ground truth, which is 7.51 and 8.26, re-\\nspectively. This further shows the effectiveness of motion\\nconstraint.\\nQuantitatively evaluation of motion with anomaly de-\\ntection. The result in Table 3 shows that the model trained\\nwith motion constraint consistently achieves higher AUC\\nthan that without the constraint on Ped1 and Ped2 dataset.\\nThis also proves that it is necessary to explicitly impose\\nthe motion consistency constraint into the objective for\\nanomaly detection.\\n4.6. Impact of Different Losses for Anomaly Detec-\\ntion.\\nWe also analyze the impact of different loss functions\\nfor anomaly detection by ablating different terms gradu-\\nally. We combine different losses to conduct experiments\\non the Avenue dataset. To evaluate how different losses af-\\nfect the performance of anomaly detection, we also utilize\\nthe score gap( ∆s) mentioned above. The larger gap repre-\\nsents the more discriminations between normal and abnor-\\nmal frames. The results in Figure 5 show more constraints\\nusually achieve a higher gap as well as AUC value, and our\\nmethod achieves the highest value under all settings.\\n4.7. Comparison of Prediction Network and Auto-\\nEncoder Networks for Anomaly Detection\\nWe also compare the video prediction network based and\\nAuto-Encoder network based anomaly detection. Here for\\nAuto-Encoder network based anomaly detection, we use\\nthe Conv-AE [13] which is the latest work and achieves\\nstate-of-the-art performance for anomaly detection. Be-\\ncause of the capacity of deep neural network, Auto-Encoder\\n7', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 14}),\n",
       " Document(page_content='Optical flow \\nwith motion constraint \\nPredicted image \\nwith motion constraint Optical flow \\nground truth Optical flow\\nwithout motion constraint \\nPredicted image \\nwithout motion constraintFigure 7. The visualization of optical ﬂow and the predicted images on the Ped1 dataset. The red boxes represent the difference of optical\\nﬂow predicted by the model with/without motion constraint. We can see that the optical ﬂow predicted by the model with motion constraint\\nis closer to ground truth. Best viewed in color.\\n020 40 60 \\n19\\n17 25 33 41 49 57 65 73 81 89 97 \\n105 113 121 129 137 145 153 161 169 177 PSNR #Frame crossroad Vehicle Intrude \\ncrossroad \\n\\x7f !\\x7f\"\"# \\x7f$% \\n\\x7f&& \\n\\x7f\"$$ \\x7f&% \\n\\x7f\"\\'! \\n\\x7f\"($ prediction ground truth Uncertainty in A Normal Event & Vehicle Intruding \\n020 40 60 \\n1\\n15 29 43 57 71 85 99 \\n113 127 141 155 169 183 197 211 225 239 253 267 281 \\n295 \\n309 323 337 351 PSNR \\n#Frame Jump fight crossroad normal abnormal normal abnormal \\nprediction ground truth Fighting \\nFigure 8. The visualization of predicted testing frames in our toy pedestrian dataset. There are two abnormal cases including vehicle\\nintruding(left column) and humans ﬁghting(right column). The orange circles correspond to normal events with uncertainty in prediction\\nwhile the red ones correspond to abnormal events. It is noticeable that the predicted truck is blurred, because no vehicles appear in the\\ntraining set. Further, in the ﬁghting case, two persons cannot be predicted well because ﬁghting motion never appear in the training phase.\\nbased methods may well reconstruct normal and abnormal\\nframes in the testing phase. To evaluate the performance of\\nprediction network and Auto-Encoder one, we also utilize\\nthe aforementioned gap( ∆s) between normal and abnormal\\nscores. The result in Fig. 6 shows that our solution always\\nachieves higher gaps than Conv-AE, which validates the ef-\\nfectiveness of video prediction for anomaly detection.\\n4.8. Evaluation with A Toy Dataset\\nWe also design a toy pedestrian dataset for performance\\nevaluation. In the training set, only a pedestrian walks on\\nthe road and he/she can choose different directions when\\nhe/she comes to a crossroad. In the testing set, there are\\nsome abnormal cases such as vehicles intruding, humans\\nﬁghting, etc.. We have uploaded our toy dataset in the sup-\\nplementary material. Totally, the training data contains 210frames and testing data contains 1242 frames.\\nIt is interesting that the motion direction is sometimes\\nalso uncertain for normal events, for example, a pedestrian\\nstands at the crossroad. Even though we cannot predict the\\nmotion well, we only cannot predict the next frame at a mo-\\nment which leads a slightly instant drop in terms of PSNR.\\nAfter observing the pedestrian for a while when the pedes-\\ntrian has made his or her choice, it becomes predictable and\\nPSNR would go up, shown in Fig. 8. Therefore the un-\\ncertainty of normal events does not affect our solution too\\nmuch. However, for the real abnormal events, for exam-\\nple, a truck breaks into the scene and hits the pedestrian, it\\nwould leads to a continuous lower PSNR, which facilitates\\nthe anomaly prediction. Totally, the AUC is 98.9%.\\n8', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 15}),\n",
       " Document(page_content='4.9. Running Time\\nOur framework is implemented with NVIDIA GeForce\\nTITAN GPUs and Tensorﬂow [1]. The average running\\ntime is about 25 fps, which contains both the video frame\\ngeneration and anomaly prediction. We also report the run-\\nning time of other methods such as 20 fps in [31], 150 fps\\n[20] and 0.5 fps in [38].\\n5. Conclusion\\nSince normal events are predictable while abnormal\\nevents do not conform to the expectation, therefore we pro-\\npose a future frame prediction network for anomaly detec-\\ntion. Speciﬁcally, we use a U-Net as our basic prediction\\nnetwork. To generate a more realistic future frame, other\\nthan adversarial training and constraints in appearance, we\\nalso impose a loss in temporal space to ensure the optical\\nﬂow of predicted frames to be consistent with ground truth.\\nIn this way, we can guarantee to generate the normal events\\nin terms of both appearance and motion, and the events\\nwith larger difference between prediction and ground truth\\nwould be classiﬁed as anomalies. Extensive experiments on\\nthree datasets show our method outperforms existing meth-\\nods by a large margin, which proves the effectiveness of our\\nmethod for anomaly detection.\\nReferences\\n[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,\\nC. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al.\\nTensorﬂow: Large-scale machine learning on heterogeneous\\ndistributed systems. arXiv preprint arXiv:1603.04467 , 2016.\\n[2] A. Adam, E. Rivlin, I. Shimshoni, and D. Reinitz. Ro-\\nbust real-time unusual event detection using multiple ﬁxed-\\nlocation monitors. TPAMI , 30(3):555–560, 2008.\\n[3] V . Chandola, A. Banerjee, and V . Kumar. Anomaly detec-\\ntion: A survey. ACM computing surveys (CSUR) , 41(3):15,\\n2009.\\n[4] B. Chen, W. Wang, J. Wang, X. Chen, and W. Li. Video\\nimagination from a single image with transformation gener-\\nation. arXiv preprint arXiv:1706.04124 , 2017.\\n[5] Y . S. Chong and Y . H. Tay. Abnormal event detection in\\nvideos using spatiotemporal autoencoder. arXiv preprint\\narXiv:1701.01546 , 2017.\\n[6] Y . Cong, J. Yuan, and J. Liu. Sparse reconstruction cost\\nfor abnormal event detection. In CVPR , pages 3449–3456.\\nIEEE, 2011.\\n[7] N. Dalal, B. Triggs, and C. Schmid. Human detection using\\noriented histograms of ﬂow and appearance. In ECCV , pages\\n428–441. Springer, 2006.\\n[8] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,\\nV . Golkov, P. van der Smagt, D. Cremers, and T. Brox.\\nFlownet: Learning optical ﬂow with convolutional networks.\\nInICCV , pages 2758–2766, 2015.\\n[9] J. Gauthier. Conditional generative adversarial nets for\\nconvolutional face generation. Class Project for StanfordCS231N: Convolutional Neural Networks for Visual Recog-\\nnition, Winter semester , 2014(5):2, 2014.\\n[10] A. D. Giorno, J. A. Bagnell, and M. Hebert. A discriminative\\nframework for anomaly detection in large videos. In ECCV ,\\npages 334–349. Springer, 2016.\\n[11] R. Girshick. Fast r-cnn. In ICCV , pages 1440–1448, 2015.\\n[12] I. J. Goodfellow. Generative adversarial networks. CoRR ,\\n2014.\\n[13] M. Hasan, J. Choi, J. Neumann, A. K. Roy-Chowdhury,\\nand L. S. Davis. Learning temporal regularity in video se-\\nquences. In CVPR , 2016.\\n[14] R. Hinami, T. Mei, and S. Satoh. Joint detection and recount-\\ning of abnormal events by learning deep generic knowledge.\\nInICCV , Oct 2017.\\n[15] P. Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros. Image-to-image\\ntranslation with conditional adversarial networks. In CVPR ,\\nJuly 2017.\\n[16] J. Kim and K. Grauman. Observe locally, infer globally: a\\nspace-time mrf for detecting abnormal activities with incre-\\nmental updates. In CVPR , pages 2921–2928. IEEE, 2009.\\n[17] D. Kingma and J. Ba. Adam: A method for stochastic opti-\\nmization. arXiv preprint arXiv:1412.6980 , 2014.\\n[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\\nclassiﬁcation with deep convolutional neural networks. In\\nNIPS , pages 1097–1105, 2012.\\n[19] W. Lotter, G. Kreiman, and D. Cox. Deep predictive cod-\\ning networks for video prediction and unsupervised learning.\\narXiv preprint arXiv:1605.08104 , 2016.\\n[20] C. Lu, J. Shi, and J. Jia. Abnormal event detection at 150 fps\\nin matlab. In ICCV , pages 2720–2727, 2013.\\n[21] W. Luo, W. Liu, and S. Gao. Remembering history with\\nconvolutional lstm for anomaly detection. In Multimedia\\nand Expo (ICME), 2017 IEEE International Conference on ,\\npages 439–444. IEEE, 2017.\\n[22] W. Luo, W. Liu, and S. Gao. A revisit of sparse coding based\\nanomaly detection in stacked rnn framework. In ICCV , Oct\\n2017.\\n[23] V . Mahadevan, W. Li, V . Bhalodia, and N. Vasconcelos.\\nAnomaly detection in crowded scenes. In CVPR , volume\\n249, page 250, 2010.\\n[24] X. Mao, Q. Li, H. Xie, R. Y . Lau, Z. Wang, and S. P. Smol-\\nley. Least squares generative adversarial networks. arXiv\\npreprint ArXiv:1611.04076 , 2016.\\n[25] M. Mathieu, C. Couprie, and Y . LeCun. Deep multi-scale\\nvideo prediction beyond mean square error. arXiv preprint\\narXiv:1511.05440 , 2015.\\n[26] J. R. Medel and A. Savakis. Anomaly detection in video\\nusing predictive convolutional long short-term memory net-\\nworks. arXiv preprint arXiv:1612.00390 , 2016.\\n[27] N. Navneet and B. Triggs. Histograms of oriented gradients\\nfor human detection. In CVPR 2005. IEEE Computer Society\\nConference on , volume 1, pages 886–893. IEEE, 2005.\\n[28] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu-\\ntional networks for biomedical image segmentation. In In-\\nternational Conference on Medical Image Computing and\\nComputer-Assisted Intervention , pages 234–241. Springer,\\n2015.\\n9', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 16}),\n",
       " Document(page_content='[29] X. Shi, Z. Chen, H. W, D. Yeung, D. Wong, and W. Woo.\\nConvolutional lstm network: A machine learning approach\\nfor precipitation nowcasting. In NIPS , pages 802–810, 2015.\\n[30] K. Simonyan and A. Zisserman. Two-stream convolutional\\nnetworks for action recognition in videos. In NIPS , pages\\n568–576, 2014.\\n[31] R. Tudor Ionescu, S. Smeureanu, B. Alexe, and M. Popescu.\\nUnmasking the abnormal events in video. In ICCV , Oct\\n2017.\\n[32] F. Tung, J. S. Zelek, and D. A. Clausi. Goal-based trajec-\\ntory analysis for unusual behaviour detection in intelligent\\nsurveillance. Image and Vision Computing , 29(4):230–240,\\n2011.\\n[33] J. van Amersfoort, A. Kannan, M. Ranzato, A. Szlam,\\nD. Tran, and S. Chintala. Transformation-based models of\\nvideo sequences. arXiv preprint arXiv:1701.08435 , 2017.\\n[34] T. Wang and H. Snoussi. Histograms of optical ﬂow orienta-\\ntion for abnormal events detection. In Performance Evalua-\\ntion of Tracking and Surveillance (PETS), 2013 IEEE Inter-\\nnational Workshop on , pages 45–52. IEEE, 2013.\\n[35] S. Wu, B. E. Moore, and M. Shah. Chaotic invariants\\nof lagrangian particle trajectories for anomaly detection in\\ncrowded scenes. In CVPR , pages 2054–2060. IEEE, 2010.\\n[36] D. Xu, E. Ricci, Y . Yan, J. Song, and N. Sebe. Learning deep\\nrepresentations of appearance and motion for anomalous\\nevent detection. arXiv preprint arXiv:1510.01553 , 2015.\\n[37] D. Zhang, D. Gatica-Perez, S. Bengio, and I. McCowan.\\nSemi-supervised adapted hmms for unusual event detection.\\nInCVPR , volume 1, pages 611–618. IEEE, 2005.\\n[38] B. Zhao, F. Li, and E. P. Xing. Online detection of unusual\\nevents in videos via dynamic sparse coding. In CVPR , pages\\n3313–3320. IEEE, 2011.\\n10', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 17}),\n",
       " Document(page_content='VPTR: Efﬁcient Transformers for Video Prediction\\nXi Ye\\nLITIV Laboratory, Polytechnique Montr ´eal\\nMontr ´eal, Canada\\nEmail: xi.ye@polymtl.caGuillaume-Alexandre Bilodeau\\nLITIV Laboratory, Polytechnique Montr ´eal\\nMontr ´eal, Canada\\nEmail: gabilodeau@polymtl.ca\\nAbstract —In this paper, we propose a new Transformer block\\nfor video future frames prediction based on an efﬁcient local\\nspatial-temporal separation attention mechanism. Based on this\\nnew Transformer block, a fully autoregressive video future\\nframes prediction Transformer is proposed. In addition, a non-\\nautoregressive video prediction Transformer is also proposed\\nto increase the inference speed and reduce the accumulated\\ninference errors of its autoregressive counterpart. In order to\\navoid the prediction of very similar future frames, a contrastive\\nfeature loss is applied to maximize the mutual information\\nbetween predicted and ground-truth future frame features. This\\nwork is the ﬁrst that makes a formal comparison of the two types\\nof attention-based video future frames prediction models over\\ndifferent scenarios. The proposed models reach a performance\\ncompetitive with more complex state-of-the-art models. The\\nsource code is available at https://github.com/XiYe20/VPTR .\\nI. I NTRODUCTION\\nVideo future frames prediction (VFFP) is applied to many\\nresearch areas, for instance, intelligent agents [1], [2], au-\\ntonomous vehicles [3], model-based reinforcement learning\\n[4]. More recently, it has drawn a lot of attention since it\\nis naturally a good self-supervised learning task [5], [6].\\nIn this paper, we focus on the most common video pre-\\ndiction task, i.e. predicting Nfuture frames given Lpast\\nframes, with LandNgreater than 1. For training a deep\\nlearning VFFP model, we can formalize the task to be\\narg maxθp(ˆxL+N,...,ˆxL+1|xL,...,x 1;θ), where ˆxtandxt\\ndenote the predicted future frames and input past frames\\nrespectively, θdenotes the model parameters.\\nEven though many deep learning-based VFFP models have\\nbeen proposed, some challenges still remain to be solved. Al-\\nmost all the state-of-the-art (SOTA) VFFP models are based on\\nConvLSTMs, i.e. convolutional short-term memory networks,\\nwhich are efﬁcient and powerful. Nevertheless, they suffer\\nfrom some inherent problems of recurrent neural networks\\n(RNNs), such as slow training and inference speed, error\\naccumulation during inference, gradient vanishing, and pre-\\ndicted frames quality degradation. Researchers keep improving\\nthe performance by developing more and more sophisticated\\nConvLSTM-based models. For instance, by integrating custom\\nmotion-aware units into ConvLSTM [7], or building complex\\nmemory modules to store the motion context [8].\\nInspired by the great success of Transformers in NLP, more\\nand more researchers are starting to adapt Transformers for\\nvarious computer vision tasks [9], [10], [11], [12], including\\nfew recent works for VFFP [13], [14], [15]. However, itis computational expensive to apply Transformer to high\\ndimensional visual features. We still need further research\\nabout more efﬁcient visual Transformers, especially for videos.\\nTherefore, we propose a novel efﬁcient Transformer block with\\nsmaller complexity, and we developed a new video prediction\\nTransformer (VPTR) based on it.\\nAmong the Transformers-based VFFP models [13], [14],\\n[15] that we mentioned earlier, some of them are autoregres-\\nsive models while some others are non-autoregressive models,\\nand they are based on different attention mechanisms, e.g.\\na custom convolution multi-head attention (MHA) [13] and\\nstandard dot-product MHA [14], [15]. There is no formal\\ncomparison of the two typical approaches (autoregressive vs\\nnon-autoregressive) to use Transformer-based VFFP models so\\nfar. Thus, we developed an fully autoregressive VPTR (VPTR-\\nFAR) and a non-autoregressive VPTR (VPTR-NAR). The two\\nVPTR variants share the same attention mechanism and same\\nnumber of Transformer block layers, which guarantees a fair\\ncomparison between the two approaches.\\nOur main contributions are summarized as:\\n1) We proposed a new efﬁcient Transformer block for\\nspatio-temporal feature learning by combining spatial lo-\\ncal attention and temporal attention in two steps. The new\\nTransformer block successfully reduces the complexity of\\na standard Transformer block with respect to same input\\nspatio-temporal feature size, speciﬁcally, from O((THW )2)\\ntoO(H2W2\\nP2+T2).\\n2) Two VPTR models, VPTR-NAR and VPTR-FAR, were\\ndeveloped. We show that the proposed simple attention-based\\nVPTRs are capable of reaching and outperforming more\\ncomplex SOTA ConvLSTM-based VFFP models.\\n3) A formal comparison of two VPTR variants was con-\\nducted. The results show that VPTR-NAR has a faster infer-\\nence speed and smaller accumulation of errors during infer-\\nence, but it is more difﬁcult to train. We solved the training\\nproblem of VPTR-NAR by employing a contrastive feature\\nloss which maximizes the mutual information of predicted and\\nground-truth future frame features.\\n4) We found that given the same number of Transformer\\nblock layers, VPTR-FAR has a worse generalization perfor-\\nmance due to the accumulated inference errors, which are\\nintroduced by the discrepancy between train and test behav-\\niors. We also found that recurrent inference over pixel space\\nintroduces less accumulation errors than recurrent inference\\nover latent space in the case of VPTR-FAR.arXiv:2203.15836v1  [cs.CV]  29 Mar 2022\\n', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 18}),\n",
       " Document(page_content='II. R ELATED WORK\\nAlmost all the SOTA deep learning-based VFFP models are\\nConvLSTM-based autoencoders, where the encoder extracts\\nthe representations of past frames, and then the decoder\\ngenerates future frame pixels based on those representations\\n[16], [17], [18], [7], [8]. In general, the SOTA models rely on\\ncomplex ConvLSTM models that integrates attention mech-\\nanism or memory augmented modules. For example, LMC-\\nMemory model [8] stores the long-term motion context by a\\nnovel memory alignment learning, and the motion information\\nis recalled during test to facilitate the long-term prediction.\\nZhang et al. [7] proposed a attention-based motion-aware unit\\nto increase the temporal receptive ﬁeld of RNNs.\\nThe ConvLSTM-based models are ﬂexible and efﬁcient, but\\nrecurrent prediction is slow. Therefore, standard CNNs or 3D-\\nCNNs are also used as the backbones of VFFP to generate\\nmultiple future frames simultaneously [19], [20], [21], [22].\\nBesides, the future prediction is by nature multimodal [23],\\ni.e. stochastic. Some VFFP models aim to solve this problem\\nbased on V AEs, such as SV2P [24], SVG-LP [25], improved\\nconditional VRNNs [26]. Stochasticity learning is challenging\\nand thus most VFFP models ignore it. A detail survey of VFFP\\nmodels can be found in [23].\\nRecently, Transformers were applied for VFFP. The Con-\\nvTransformer [13] model follows the architecture of DETR\\n[9]. DETR follows a classical neural machine translation\\n(NMT) Transformer architecture. It also inspired the develop-\\nment of our VPTR-NAR. Despite the similarities, our VPTR-\\nNAR is different from ConvTransformer with respect to the\\nfundamental attention mechanism. Speciﬁcally, ConvTrans-\\nformer proposed a custom hybrid multi-head attention module\\nbased on convolution, but our VPTR-NAR uses the standard\\nmulti-head dot-product attention. Another more recent model\\n(VideoGPT) [14] takes a 3D-CNN as backbone to encode\\nvideo clips into spatial-temporal features, which are then\\nﬂattened to be a sequence to train a standard Transformer\\nwith the GPT manner [27], [28]. VideoGPT shares a similar\\narchitecture and train/test behaviours as our VPTR-FAR. But\\nVideoGPT performs the attention along the spatial and tem-\\nporal dimensions jointly while our VPTR-FAR performs the\\nattention along the spatial and temporal dimensions separately.\\nMore importantly, VideoGPT downsamples the time dimen-\\nsion of input videos by 3D-CNN and thus helps the temporal\\ninformation modeling. In contrast, our VPTR models solely\\ndepend on attention for a full temporal information modeling,\\nwithout downsampling. Another recent work N ¨UWA [15]\\nshares a similar idea to VideoGPT.\\nEfﬁcient visual Transformers. To reduce the computation\\ncost for visual Transformers, some models reduce the ﬂattened\\nsequence length by different methods. ViT and the succes-\\nsive works [10], [29], [12] divided input features into local\\npatches, either 2D or 3D, and then tokenize the local patch by\\nconcatenation or Pooling [30]. Some other models introduce\\nsparse attention to reduce the complexity, e.g. restricting the\\nattention over a local region [31], [32], [33], or decomposingthe global attention into a series of axial-attention [34], [35],\\n[12]. HRFormer [33] is an example of local region attention-\\nbased Transformers, which is designed for image classiﬁcation\\nand dense prediction.\\nSpeciﬁcally, a HRFormer block is composed of a local-\\nwindow multi-head self attention layer and a depth-wise con-\\nvolution feed-forward network. The input feature maps Z∈\\nRH×W×Care ﬁrstly evenly divided into Pnon-overlapping\\nlocal patches, each patch is Zp∈RH\\nP×W\\nP×C. Then a multi-\\nhead self attention is performed for each patch. Finally, the\\ndepth-wise convolution is used to exchange information among\\ndifferent local patches.\\nIII. T HE PROPOSED VPTR MODELS\\nA. Overall framework of VPTR\\nOur overall VPTR framework is illustrated in Fig. 1. A\\nCNN encoder shared by all the past frames extracts the visual\\nfeatures of each frame. Then a VPTR is taken to predict\\nthe visual features of each future frame based on the past\\nframe features. The detail architectures of two different VPTR\\nvariants are described in the following subsections. In order to\\nmake the model architecture simple and easier to train, there\\nis no skip connections between encoder and decoder.\\nB. Encoder and decoder\\nWe adapted the ResNet-based autoencoder from the Pix2Pix\\nmodel [36]. The output feature channels of the encoder and\\ninput feature channels of the decoder are modiﬁed to be of\\nsizedmodel to match with the VPTR input and output size.\\nThe loss function to train the encoder and decoder is deﬁned\\nas follows,\\nLrec=L2(X,ˆX) +Lgdl(X,ˆX)\\n+λ1arg min\\nGmax\\nDLGAN(G,D ),(1)\\nwhereL2denotes the MSE loss (Eq. 2) and Lgdldenotes\\nimage gradient difference loss [19] (Eq. 3), Xand ˆXdenote\\nthe original frames and reconstructed frames respectively, xi\\ndenotes a single frame, λ1andαare hyperparameters. LGAN\\ndenotes the GAN loss (Eq. 4), where Ddenotes a discrimina-\\ntor, which is not shown in Fig. 1, and the combination of the\\nencoder and decoder is considered to be a generator G. We\\ntrainLGAN with the PatchGAN [36] manner.\\nL2(X,ˆX) =n∑\\ni=1∥xi−ˆxi∥2\\n2 (2)\\nLgdl(X,ˆX) =n∑\\ni=1∑\\ni,j⏐⏐|xi,j−xi−1,j|−|ˆxi,j−ˆxi−1,j|⏐⏐α\\n+⏐⏐|xi,j−1−xi,j|−|ˆxi,j−1−ˆxi,j|⏐⏐α(3)\\nLGAN(G,D ) =EX[logD (X)]+EˆX[log(1−D(G(X))](4)\\n', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 19}),\n",
       " Document(page_content='EncoderDecoderVPTRPast framesFuture framesFig. 1: Overall framework of VPTR. Green squares and blue\\nsquares denote the past frame features and future frames\\nfeatures respectively.\\nC. VidHRFormer Block\\nWe proposed a new Transformer block based on the\\nHRFormer block [33] for video processing, which is\\nnamed VidHRFormer block. The detail architecture of a\\nVidHRFormer block is shown in the gray area of Fig. 2(a).\\nEssentially, we integrate a temporal multi-head attention layer,\\ntogether with some other necessary feed-forward and normal-\\nization layers, into the HRFormer block.\\nLocal spatial multi-head self-attention (MHSA). Given\\na spatiotemporal feature map Z∈RN×T×H×W×dmodel ,\\nwe ﬁrstly reshape and evenly divide it into Plocal\\npatches{Z1,Z2,...,ZP}along theHandWdimensions,\\nwhereZp∈R(NT)×K2×dmodel , and each local patch\\nis of size K×K, withP=HW\\nK2patches in total.\\nMHSA (Zp) =Concat [head (Zp)1,...,head (Zp)h], where\\nhead (Zp)i∈RK2×dmodel\\nh is formulated as\\nhead (Zp)i=softmax [((ZQ\\npWQ\\ni)(ZK\\npWK\\ni)√\\ndmodel/h]ZpWV\\ni,(5)\\nwhereWQ\\ni,WK\\ni,WV\\niare linear projection matrices for the\\nquery, key and value of each head irespectively, ZQ\\npand\\nZK\\npdenote the key and query for attention. We may use\\na ﬁxed absolute 2D positional encoding [37], or a relative\\npositional encoding (RPE) [38] of the local patch to get ZQ\\np\\nandZK\\np. We compared the two different positional encodings\\nin the experiments. The complexity of local spatial MHSA is\\nO(H2W2\\nP2).\\nConvolutional feed-forward neural network (Conv FFN).\\nAfter the local spatial MHSA, {Z1,Z2,...,ZP}are assembled\\nback to beZ∈R(NT)×H×W×dmodel . The Conv FFN layer is\\ncomposed of a 3×3depth-wise convolution and two point-\\nwise MLPs. Note that all the normalization layers in Conv\\nFFN are layer normalization, instead of batch normalization\\nused in the original HRFormer block.\\nTemporal MHSA. The local spatial MHSA and Conv-\\nFFN are shared by every frame feature. A temporal MHSA\\nis placed on top of them to model the temporal dependency\\nbetween frames. We reshape the input feature map Z∈\\nR(NT)×H×W×dmodel to beZ∈R(NHW )×T×dmodel . Temporal\\nMHSA is a standard multi-head self-attention similar to the\\nlocal spatial MHSA, except that there is no local patch division\\nand it takes a ﬁxed absolute 1D positional encoding of time.\\nThe complexity of temporal MHSA is O(T2). The temporal\\nMHSA is followed by a MLP feed-forward neural networkas in a standard Transformer, and the output feature map is\\nreshaped back to be Z∈RN×T×H×W×dmodel for the next\\nlayer of VidHRFormer block.\\nIn summary, the proposed VidHRFormer block reduces the\\ncompute complexity from O((THW )2)to beO(H2W2\\nP2+T2)\\nby combining spatial local window attention and temporal\\nattention in two steps. Based on the VidHRFormer, we develop\\ntwo different VPTR models.\\nD. VPTR-FAR\\nThe fully autoregressive VPTR model is simply a stack of\\nmultiple VidHRFormer blocks. The architecture is shown in\\nFig. 2(a). Theoretically, given a well-trained CNN encoder\\nand decoder, the VPTR-FAR parameterizes the following\\ndistribution:\\np(x1,...,xL,...,xL+N) =L+N∏\\nt=1p(xt|xt−1,...x 1) (6)\\nIn other words, VPTR-FAR predicts the next frame condi-\\ntioned on all previous frames, which is also the most common\\nparadigm for most SOTA VFFP models. An attention mask\\nis applied to the temporal MHSA module to impose the\\nconditional dependency between the next frame and previous\\nframes.\\nDuring training, we feed the ground-truth frames\\n{x1,...,xL+N−1}into the encoder, which generates the\\nfeature sequence{z1,...,zL+N−1}. VPTR-FAR then predicts\\nthe future feature sequence {ˆz2,...,ˆzL+N}, which is then\\ndecoded by the decoder to generate frames {ˆx2,...,ˆxL+N}.\\nThe training loss of VPTR-FAR is:\\nLFAR =L+N∑\\nt=2L2(xt,ˆxt) +L+N∑\\nt=2Lgdl(xt,ˆxt) (7)\\nDuring test, we ﬁrstly get the ground-truth past frames\\nfeatures{z1,...,zL}. Then there are two different ways of\\nrecurrently predicting the future frames. The ﬁrst method\\nis recurrently generating all the future frame features only\\nby the VPTR-FAR module, i.e. ˆzt=T(z1,...,zt−1),t∈\\n[L+ 1,...L +N], whereTdenotes the VPTR-FAR module.\\nThen we get ˆxt=Dec(ˆzt),t∈[L+ 1,...L +N], where\\nDec denotes the CNN frame decoder. The second prediction\\nmethod introduces two additional steps. Particularly, ˆzt=\\nEnc(Dec(T(z1,...,zt−1))),t∈[L+ 1,...L +N], whereEnc\\ndenotes the CNN frame encoder. In short, we decode each\\nfuture feature to be frame ˆxt, and then encode the frame back\\ninto a latent feature before the prediction of next future frame\\nfeature. The second way signiﬁcantly reduces the accumulated\\nerror during inference, and the reasons are analyzed in the\\nexperiments section.\\nE. VPTR-NAR\\nInspired by the achitecture of DETR [37], a non-\\nautoregressive variant is proposed to increase the inference\\nspeed and reduce inference accumulation error of autore-\\ngressive models. VPTR-NAR is illustrated in Fig. 2(b). It\\n', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 20}),\n",
       " Document(page_content='Layer NormLocal spatial MHSAConv FFNTemporal MHSALayer NormLayer NormLayer NormMLP FFNN×1D PE2D PE……\\n𝑧\"𝑧#𝑧$%&\\'#𝑧$%&\\'\"̂𝑧#̂𝑧(̂𝑧$%&\\'\"̂𝑧$%&(a)\\nLayer NormLocal spatial MHSAConv FFNTemporal MHSALayer NormLayer NormLayer NormMLP FFNN×1D PE2D PE……\\n𝑧\"𝑧#𝑧$%\"𝑧$𝑒\"𝑒#𝑒$%\"𝑒$\\nLayer NormLocal spatial MHSAConv FFNTemporal MHSALayer NormLayer NormLayer NormMLP FFN1D PE2D PETemporal MHALayer Norm𝑄𝐾𝑉Layer NormConv FFN…̂𝑧$)\"̂𝑧$)#̂𝑧$)*%\"̂𝑧$)*1D PE\\nN×…𝑞$)\"𝑞$)#𝑞$)*%\"𝑞$)*++𝑉𝐾𝑄+TDTE (b)\\nFig. 2: (a) VPTR-FAR. The gray area indicates the proposed basic VidHRFormer block. A temporal attention mask is applied\\nto the Temporal MHSA module for VPTR-FAR. (b)VPTR-NAR. The left part is the Transformer encoder and right part is the\\nnon-autoregressive Transformer decoder.\\nconsists of a Transformer encoder and decoder, where the\\nencoderTEencodes all past frame features zt,t∈[1,L]to\\nbeet,t∈[1,L], which are normally named as ”memories” in\\nNLP. The architecture of TE, left part of Fig. 2(b), is the same\\nas the VPTR-FAR, except that there is no temporal attention\\nmask for the temporal MHSA module.\\nThe decoderTDof VPTR-NAR, right part of Fig. 2(b),\\nincludes two more layers compared with TE. A temporal\\nmulti-head attention (MHA) layer and another output Conv\\nFFN layer. The Temporal MHA layer is also called the\\nencoder-decoder attention layer, which takes the memories as\\nvalue and key, while the query is derived from the future frame\\nquery sequence{qL+1,...,qL+N}, whereqt∈RH×W×C,t∈\\n[L+ 1,L+N].{qL+1,...,qL+N}is randomly initialized\\nand updated during training. Note that there is no temporal\\nattention mask for Temporal MHSA layer, since we do not\\nneed to impose conditional dependency between each future\\nframe query. Theoretically, VPTR-NAR directly models the\\nfollowing conditional distribution:\\np(xL+N,...,xL+1|xL,...,x 1) (8)\\nContrastive feature loss for VPTR-NAR. We failed to\\ntrain VPTR-NAR with a loss only composed by MSE and\\nGDL, i.e.,L=∑L+N\\nt=L+1L2(xt,ˆxt) +Lgdl(xt,ˆxt), since it\\nis easy to fall into some local minimums. Speciﬁcally, all\\nthe predicted future frames are somewhat similar to each\\nother. A similar phenomenon is also observed in the non-\\nautoregressive NMT models, where the Transformer decoder\\nfrequently generate repeated tokens [39]. To solve this prob-\\nlem, we impose another contrastive feature loss Lc[40] to\\nmaximize the mutual information between predicted futureframe feature ˆztand the future frame feature zt(ground-truth)\\ngenerated by the CNN encoder, where t∈[L+ 1,L+N].Lc\\nis formulated as follows,\\nLc(zt,ˆzt) =1\\n2Sl∑\\ns=1lc(ˆvs,vs,sg(¯vs)) +lc(vs,ˆvs,sg(ˆ¯vs)),(9)\\nwherevs∈Rdmodel denotes a feature vector at spatial location\\nsofzt,¯vs∈R(Sl−1)×dmodel denotes the collection of feature\\nvectors at all other spatial locations of zt.Sl=H×Wis\\nthe total number of spatial locations in a feature map. ˆvsand\\nˆ¯vsofˆztare deﬁned in the same way. sgis the stop gradient\\noperation and lcis the info-NCE based contrastive loss deﬁned\\nby\\nlc(v,v+,v−) =\\n−logexp(s(v,v+))\\nexp(s(v,v+)) +∑M\\nm=1exp(s(v,v−)).(10)\\nGiven a feature vector v∈Rdmodel ,v+∈Rdmodel is\\nthe spatially-corresponding ground-truth feature vector, and\\nv−∈RM×dmodel denotes the Mother spatially different\\nground-truth feature vectors. s(v1,v2)measures the feature\\ndot-product similarity. Finally, the training loss function for\\nVPTR-NAR is deﬁned as\\nLNAR =L+N∑\\nt=L+1L2(xt,ˆxt) +Lgdl(xt,ˆxt) +λ2Lc(zt,ˆzt).\\n(11)\\nDuring test, VPTR-NAR predicts Nfuture frames simulta-\\nneously, instead of recurrently.\\n', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 21}),\n",
       " Document(page_content='F . Training strategy\\nThe whole VFFP model training process is divided into two\\nstages. For stage one, we ignore the VPTR module and only\\ntrain the encoder and decoder as a normal autoencoder with\\nthe loss function in Eq. 1, which aims to reconstruct all the\\nframes of the whole training set perfectly. During stage two,\\nwe only update parameters of the VPTR module while the\\nwell-trained encoder and decoder are ﬁxed. VPTR-FAR and\\nVPTR-NAR are trained with the loss function in Eq. 7 and Eq.\\n11 respectively. It is well-known that Transformers are hard\\nto train, therefore we proposed this two-stage training strategy\\nto ease the training. Besides, the two-stage training strategy is\\nﬂexible and allows us to test different VPTR variants without\\nrepetitive training of the encoder and decoder. Experimental\\nresults show that a ﬁnal joint ﬁnetuning of autoencoder and\\nVPTR is not helpful.\\nIV. E XPERIMENTS\\nA. Datasets and Metrics\\nWe evaluate the proposed VPTR models over three datasets,\\nKTH [41], MovingMNIST [42] and BAIR [43]. For KTH and\\nMoving MNIST, VPTR models are trained to predict 10 future\\nframes given 10 past frames. For BAIR dataset, VPTR models\\nare trained to predict 10 future frames given 2 past frames.\\nAll datasets are trained with a resolution of 64×64. We\\nprocess the KTH dataset as previous works [44], [17]. Random\\nhorizontal and vertical ﬂip of each video clip are utilized\\nas data augmentation. We use the MovingMNIST created by\\nE3D-LSTM [45], which takes the same data augmentation\\nmethod as KTH. There is no data augmentation for BAIR.\\nMetrics. Learned Perceptual Image Patch Similarity\\n(LPIPS)[46] and Structural Similarity Index Measure (SSIM)\\nare used to evaluate all the three datasets. Peak Signal-to-\\nNoise Ratio (PSNR) is used to evaluate the KTH and BAIR\\ndataset, and Mean Square Error (MSE) is used to evaluate the\\nMovingMNIST dataset. All the LPIPS values are presented in\\n10−3scale.\\nB. Implementation\\nTraining stage one. In Eq. 1,λ1= 0.01for KTH and\\nMovingMNIST, λ1= 0 for the BAIR dataset. The optimizer\\nis Adam [47], with a learning rate of 2e−4.Training stage\\ntwo. For the visual features of each frame, H= 8,W=\\n8,dmodel = 528 .K= 4 for the local spatial MHSA. The\\nTransformer of VPTR-FAR includes 12 layers. For VPTR-\\nNAR, the number of layers of TEis 4, and the number of\\nlayers ofTDis 8. We take AdamW [48] with a learning rate\\nof1e−4for the optimization of all Transformers. Gradient\\nclipping is taken to stabilize the training. For the loss function\\nof VPTR-NAR (Eq. 11), λ2= 0.1.\\nC. Results\\nResults on KTH. The best results of the two VPTR variants\\nare recorded in Table I. Following the evaluation protocol of\\nprevious works, we extend the prediction length to be 20\\nframes during test. Compared with the SOTA models, theproposed VPTR models reach competitive performances in\\nterms of PSNR and SSIM. Notably, both two VPTR variants\\noutperform the SOTAs in terms of LPIPS by a large margin.\\nSome prediction examples are shown in Fig. 3. It shows\\nthat the predicted arm motion by VPTRs is more aligned\\nwith the ground-truth, which indicates that the VPTRs more\\nsuccessfully capture the cyclic hand waving movements that\\nonly depends on the past frames, in contrast to LMC-Memory\\nthat recalls some inaccurate motion from the memory bank.\\nTABLE I: Results on KTH and MovingMNIST. ↑: higher is\\nbetter,↓: lower is better. Boldface : best results.\\nMethodsKTH MovingMNIST\\n10→20 10 →10\\nPSNR↑SSIM↑LPIPS↓MSE↓SSIM↑LPIPS↓\\nMCNET [44] 25.95 0.804 - - - -\\nPredRNN++ [49] 28.47 0.865 228.9 46.5 0.898 59.5\\nE3D-LSTM [45] 29.31 0.879 - 41.3 0.910 -\\nSTMFANet [17] 29.85 0.893 118.1 - - -\\nConv-TT-LSTM [50] 28.36 0.907 133.4 53.0 0.915 40.5\\nLMC-Memory [8] 28.61 0.894 133.3 41.5 0.924 46.9\\nVPTR-NAR 26.96 0.879 86.1 63.6 0.882 107.5\\nVPTR-FAR 26.13 0.859 79.6 107.2 0.844 157.8\\nVPTR-NARVPTR-FARLMC-Memory\\nFig. 3: Qualitative results on KTH dataset. The ﬁrst row is\\nground-truth. For the past frames, t∈[1,10]. For the future\\nframes,t∈[11,30].\\nResults on MovingMNIST. The right part of Table I shows\\nthe results on the MovingMNIST dataset. We observe that the\\nSSIM of the VPTR variants is close to the SOTAs, but there\\nare large gaps in terms of MSE and LPIPS, especially for\\nVPTR-FAR. Qualitative examination shows that VPTRs make\\npoor predictions for the overlapping characters.\\nResults on BAIR. Compared with KTH and MovingM-\\nNIST, BAIR is more challenging, because the robot arm\\nmotion is random and only two past frames are given for the\\nprediction. From Table II, we ﬁnd that VPTR-NAR outper-\\nforms STMFANet [17] in terms of SSIM and LPIPS. Our\\ngood performance can be attribute to the large model capacity\\nof VPTRs and the large size of BAIR dataset. We note however\\nthat the predicted robot arm becomes blurry after the ﬁrst few\\nframes, due the deterministic nature of VPTRs. Our VPTRs\\ncould be extended to be stochastic models easily, and we\\nexpect that the stochastic version of VPTRs would achieve\\nan even better performance on the BAIR dataset.\\nD. Ablation Study\\nRPE. The VPTRs with ﬁxed absolute positional encodings\\nare taken as the base models, i.e. VPTR-NAR-BASE and\\n', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 22}),\n",
       " Document(page_content='TABLE II: Results on BAIR. ↑: higher is better, ↓: lower is\\nbetter. Boldface : best results.\\nMethods PSNR ↑SSIM↑LPIPS↓\\nSV2P [24] 20.36 0.817 91.4\\nSVG-LP [25] 17.72 0.815 60.3\\nImproved VRNN [26] - 0.822 55.0\\nSTMFANet [17] 21.02 0.844 93.6\\nVPTR-NAR 19.40 0.852 53.9\\nVPTR-FAR 18.63 0.824 69.3\\nTABLE III: Ablation study on KTH and MovingMNIST. ↑:\\nhigher is better,↓: lower is better. Boldface : best results.\\nMethodsKTH MovingMNIST\\n10→20 10 →10\\nPSNR↑SSIM↑LPIPS↓MSE↓SSIM↑LPIPS↓\\nVPTR-NAR-BASE 26.92 0.881 94.6 64.2 0.880 114.2\\nVPTR-NAR-RPE 26.96 0.879 86.1 63.6 0.882 107.5\\nVPTR-NAR-FEDA 26.25 0.872 101.1 68.0 0.872 128.7\\nVPTR-FAR-BASE 25.71 0.816 79.5 108.3 0.843 157.3\\nVPTR-FAR-RPE 26.13 0.859 79.6 107.2 0.844 157.8\\nVPTR-FAR-RIL 21.61 0.678 192.7 138.2 0.821 445.7\\nVPTR-FAR-BASE in Table III. To investigate the inﬂuence\\nof relative positional encodings, we get VPTR-NAR-RPE and\\nVPTR-FAR-RPE by substituting the 2D absolute positional\\nencoding of all local spatial MHSA module with a learned\\n2D RPE. We argue that RPE is beneﬁcial because both VPTR-\\nFAR-RPE and VPTR-NAR-RPE outperform the base models\\nwith regard to most metrics on the two datasets.\\nSpatial-temporal separation attention. The separation of\\nspatial and temporal attention reduces the complexity, but it\\nalso means that a feature at one location only attends to partial\\nlocations of the whole spatiotemporal space. To investigate the\\ninﬂuence of the separated attention, we replace the encoder-\\ndecoder attention layers of VPTR-NAR with a full spatiotem-\\nporal attention, which has a complexity of O(H2W2T2\\nP2). The\\nincreased computation cost is affordable as we only replace\\nthe encoder-decoder attention layers. Comparing the VPTR-\\nNAR-FEDA with the base model, where FEDA denotes “full\\nencoder-decoder attention”, we ﬁnd that FEDA is not beneﬁ-\\ncial. It indicates that the alternate stacking of multiple spatial\\nand temporal attention layers is capable of propagating global\\ninformation from past frames to future frames.\\nAutoregressive inference methods. As we have described\\nin Section III-D, we can perform recurrently inference over\\nlatent space (RIL) or recurrently inference over pixel space\\n(RIP) for VPTR-FAR. VPTR-FAR-BASE is evaluated by\\nRIP. Even though RIL is little faster than RIP, VPTR-FAR-\\nBASE outperforms VPTR-FAR-RIL by a large margin. Severe\\naccumulation of errors is observed for VPTR-FAR-RIL.\\nWe believe the reason is that VPTR-FAR receive only\\nsupervision from the pixel space during training. There is no\\ndirect constraints on the distance between the feature space\\npredicted by the Transformer and the feature space generated\\nby the CNN encoder. Furthermore, the latent space dimension\\nof the autoencoder is greater than the pixel space dimension,\\nwhich is a common case for VFFP, as we expect a good\\nreconstruction visual quality. Therefore, recurrent inferenceonly depending on the Transformer predictor would make the\\npredicted features deviate from the ground-truth (learned by\\nautoencoder during stage one) features quickly. But decoding\\nthe feature ﬁrstly and then encoding it back into latent space\\nrestrict the deviation to some degree.\\nE. Comparison of VPTR variants\\n2 6 10 14 18\\nFuture time step22.525.027.530.032.535.037.5PSNR NAR\\nFAR\\n2 6 10 14 18\\nFuture time step0.800.850.900.95SSIM NAR\\nFAR\\n2 6 10 14 18\\nFuture time step−0.12−0.10−0.08−0.06−0.04−0.02Negative\\n LPIPSNAR\\nFAR\\nFig. 4: Results of VPTR variants on KTH for increasing\\nprediction steps.\\nFor a better visualization, we plotted the metrics curve\\nof VPTR-NAR-BASE and VPTR-FAR-BASE with respect to\\nthe predicted future time steps in Fig. 4. For the ﬁrst few\\npredicted frames, VPTR-FAR achieves a better PSNR and\\nSSIM than VPTR-NAR, but the values drop quickly due to\\nthe accumulated errors introduced by recurrent inference. It\\nshows that VPTR-NAR has a smaller quality degradation in\\nterms of PSNR and SSIM. For the last 10 steps of the LPIPS\\ncurve, VPTR-NAR also has a smaller slope than VPTR-FAR.\\nThe accumulation of errors in VPTR-FAR is mainly due\\nto the discrepancy between training and testing behaviors.\\nSpeciﬁcally, the previously predicted frames are used during\\ninference instead of the ground-truth as during training, which\\nleads to a worse generalization ability of VPTR-FAR given\\nthe same number of Transformer layers as VPTR-NAR. In\\ncontrast, there is no discrepancy the between training and\\ntesting behaviors of VPTR-NAR. However, it is more difﬁcult\\nfor the VPTR-NAR to estimate the joint distribution directly,\\nso an additional contrastive feature loss is required.\\nAnother advantage of VPTR-NAR is the faster inference\\nspeed. For VPTR-NAR, predicting Nframes has a complexity\\nofO(N2), but the complexity for VPTR-FAR is O(∑N\\nn=1n2).\\nFor simplicity, in this assessment, we ignored the spatial\\ndimensions of features, computation cost of processing past\\nframes, and supposed that the future frames length of inference\\nis same as of the training. However, the model size of VPTR-\\nNAR is larger because of the learned future frame queries.\\nV. C ONCLUSION\\nIn this paper, we proposed an efﬁcient VidHRFormer block\\nfor spatiotemporal representation learning, and two different\\nVFFP models are developed based on it. We expect that the\\nproposed VidHRFormer block could be used as a backbone\\nfor many other video processing tasks. We compared the per-\\nformance of proposed VPTRs with SOTA models on various\\ndatasets, and we are competitive with more complex models.\\nFinally, we analyzed the inﬂuence of different modules for two\\nVPTR variants by a thorough ablation study, and we observed\\nthat VPTR-NAR achieves a better performance than VPTR-\\nFAR.\\n', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 23}),\n",
       " Document(page_content='REFERENCES\\n[1] W. Liu, W. Luo, D. Lian, and S. Gao, “Future Frame Prediction for\\nAnomaly Detection – A New Baseline,” in CVPR , 2018, pp. 6536–6545.\\n[2] Y . Lu, K. M. Kumar, S. s. Nabavi, and Y . Wang, “Future Frame\\nPrediction Using Convolutional VRNN for Anomaly Detection,” in 2019\\n16th IEEE International Conference on Advanced Video and Signal\\nBased Surveillance (AVSS) , Sep. 2019, pp. 1–8.\\n[3] J.-A. Bolte, A. Bar, D. Lipinski, and T. Fingscheidt, “Towards Corner\\nCase Detection for Autonomous Driving,” in 2019 IEEE Intelligent\\nVehicles Symposium (IV) , Jun. 2019, pp. 438–445, iSSN: 2642-7214.\\n[4] F. Leibfried, N. Kushman, and K. Hofmann, “A Deep Learning Ap-\\nproach for Joint Video Frame and Reward Prediction in Atari Games,”\\ninICML 2017 Workshop on Principled Approaches to Deep Learning ,\\nNov. 2016.\\n[5] Y . Bengio, A. Courville, and P. Vincent, “Representation Learning: A\\nReview and New Perspectives,” IEEE Transactions on Pattern Analysis\\nand Machine Intelligence , vol. 35, no. 8, pp. 1798–1828, Aug. 2013.\\n[6] X. Wang and A. Gupta, “Unsupervised Learning of Visual Representa-\\ntions Using Videos,” in ICCV , 2015, pp. 2794–2802.\\n[7] Z. Chang, X. Zhang, S. Wang, S. Ma, Y . Ye, X. Xiang, and W. Gao,\\n“MAU: A Motion-Aware Unit for Video Prediction and Beyond,” in\\nNeurIPS , May 2021.\\n[8] S. Lee, H. G. Kim, D. H. Choi, H.-I. Kim, and Y . M. Ro, “Video\\nPrediction Recalling Long-Term Motion Context via Memory Alignment\\nLearning,” in CVPR , 2021.\\n[9] T. Meinhardt, A. Kirillov, L. Leal-Taixe, and C. Feichtenhofer,\\n“TrackFormer: Multi-Object Tracking with Transformers,” in\\narXiv:2101.02702 [cs] , Jan. 2021, arXiv: 2101.02702.\\n[10] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:\\nTransformers for image recognition at scale,” in ICLR , 2021.\\n[11] P. Esser, R. Rombach, and B. Ommer, “Taming Transformers for High-\\nResolution Image Synthesis,” in arXiv:2012.09841 [cs] , Feb. 2021,\\narXiv: 2012.09841.\\n[12] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lu ˇci´c, and C. Schmid,\\n“ViViT: A video vision transformer,” in ICCV , 2021.\\n[13] Z. Liu, S. Luo, W. Li, J. Lu, Y . Wu, C. Li, and L. Yang,\\n“ConvTransformer: A Convolutional Transformer Network for Video\\nFrame Synthesis,” in arXiv:2011.10185 [cs] , Nov. 2020, arXiv:\\n2011.10185. [Online]. Available: http://arxiv.org/abs/2011.10185\\n[14] W. Yan, Y . Zhang, P. Abbeel, and A. Srinivas, “VideoGPT: Video\\nGeneration using VQ-V AE and Transformers,” in arXiv:2104.10157\\n[cs], Sep. 2021, arXiv: 2104.10157.\\n[15] C. Wu, J. Liang, L. Ji, F. Yang, Y . Fang, D. Jiang, and N. Duan,\\n“N\\\\”UWA: Visual Synthesis Pre-training for Neural visUal World\\ncreAtion,” arXiv:2111.12417 [cs] , Nov. 2021, arXiv: 2111.12417.\\n[Online]. Available: http://arxiv.org/abs/2111.12417\\n[16] M. Chaabane, A. Trabelsi, N. Blanchard, and R. Beveridge, “Looking\\nAhead: Anticipating Pedestrians Crossing with Future Frames Predic-\\ntion,” in WACV , 2020, pp. 2286–2295.\\n[17] B. Jin, Y . Hu, Q. Tang, J. Niu, Z. Shi, Y . Han, and X. Li, “Explor-\\ning Spatial-Temporal Multi-Frequency Analysis for High-Fidelity and\\nTemporal-Consistency Video Prediction,” in CVPR , 2020, pp. 4554–\\n4563.\\n[18] Y . Wang, J. Wu, M. Long, and J. B. Tenenbaum, “Probabilistic Video\\nPrediction From Noisy Data With a Posterior Conﬁdence,” in CVPR ,\\nJun. 2020.\\n[19] M. Mathieu, C. Couprie, and Y . LeCun, “Deep multi-scale video\\nprediction beyond mean square error,” in ICLR , 2016.\\n[20] B. Chen, W. Wang, and J. Wang, “Video Imagination from\\na Single Image with Transformation Generation,” in Proceedings\\nof the on Thematic Workshops of ACM Multimedia 2017 , ser.\\nThematic Workshops ’17. New York, NY , USA: Association for\\nComputing Machinery, Oct. 2017, pp. 358–366. [Online]. Available:\\nhttps://doi.org/10.1145/3126686.3126737\\n[21] C. V ondrick, H. Pirsiavash, and A. Torralba, “Generating videos with\\nscene dynamics,” in NIPS , 2016.\\n[22] Y . Wu, R. Gao, J. Park, and Q. Chen, “Future Video Synthesis With\\nObject Motion Prediction,” in CVPR , 2020.\\n[23] S. Oprea, P. Martinez-Gonzalez, A. Garcia-Garcia, J. A. Castro-Vargas,\\nS. Orts-Escolano, J. Garcia-Rodriguez, and A. Argyros, “A review ondeep learning techniques for video prediction,” IEEE Transactions on\\nPattern Analysis and Machine Intelligence , vol. 14, no. 8, 2020.\\n[24] M. Babaeizadeh, C. Finn, D. Erhan, R. Campbell, and S. Levine,\\n“Stochastic variational video prediction,” in ICLR , 2018.\\n[25] E. Denton and R. Fergus, “Stochastic Video Generation with a Learned\\nPrior,” in International Conference on Machine Learning . PMLR, Jul.\\n2018, pp. 1174–1183, iSSN: 2640-3498.\\n[26] L. Castrejon, N. Ballas, and A. Courville, “Improved Conditional\\nVRNNs for Video Prediction,” in ICCV , Oct. 2019, pp. 7607–7616,\\niSSN: 2380-7504.\\n[27] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving\\nlanguage understanding by generative pre-training,” 2018.\\n[28] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\\n“Language models are unsupervised multitask learners,” OpenAI blog ,\\nvol. 1, no. 8, p. 9, 2019.\\n[29] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and\\nL. Shao, “Pyramid Vision Transformer: A Versatile Backbone for Dense\\nPrediction without Convolutions,” in arXiv:2102.12122 [cs] , Feb. 2021,\\narXiv: 2102.12122. [Online]. Available: http://arxiv.org/abs/2102.12122\\n[30] H. Fan, B. Xiong, K. Mangalam, Y . Li, Z. Yan, J. Malik, and C. Feichten-\\nhofer, “Multiscale vision transformers,” in ICCV , 2021, pp. 6824–6835.\\n[31] P. Zhang, X. Dai, J. Yang, B. Xiao, L. Yuan, L. Zhang, and J. Gao,\\n“Multi-Scale Vision Longformer: A New Vision Transformer for High-\\nResolution Image Encoding,” in ICCV , 2021.\\n[32] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\\nB. Guo, “Swin transformer: Hierarchical vision transformer using shifted\\nwindows,” in ICCV , 2021.\\n[33] Y . Yuan, R. Fu, L. Huang, W. Lin, C. Zhang, X. Chen, and J. Wang,\\n“HRFormer: High-resolution transformer for dense prediction,” in\\nNeurIPS , 2021.\\n[34] Z. Huang, X. Wang, L. Huang, C. Huang, Y . Wei, and W. Liu, “CCNet:\\nCriss-Cross Attention for Semantic Segmentation,” 2019, pp. 603–612.\\n[35] H. Wang, Y . Zhu, B. Green, H. Adam, A. Yuille, and L.-C. Chen, “Axial-\\nDeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation,” in\\nComputer Vision – ECCV 2020 , ser. Lecture Notes in Computer Science,\\nA. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm, Eds. Cham: Springer\\nInternational Publishing, 2020, pp. 108–126.\\n[36] P. Isola, J. Y . Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation\\nwith conditional adversarial networks,” in CVPR , 2017.\\n[37] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\\nS. Zagoruyko, “End-to-End Object Detection with Transformers,” in\\nECCV . Springer International Publishing, 2020, pp. 213–229.\\n[38] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative posi-\\ntion representations,” in NAACL . New Orleans, Louisiana: Association\\nfor Computational Linguistics, Jun. 2018, pp. 464–468.\\n[39] Y . Wang, F. Tian, D. He, T. Qin, C. Zhai, and T.-Y . Liu, “Non-\\nautoregressive machine translation with auxiliary regularization,” in\\nProceedings of the AAAI conference on artiﬁcial intelligence , vol. 33,\\n2019, pp. 5377–5384, number: 01.\\n[40] A. Andonian, T. Park, B. Russell, P. Isola, J.-Y . Zhu, and R. Zhang,\\n“Contrastive feature loss for image prediction,” in Proceedings of the\\nIEEE/CVF international conference on computer vision , 2021, pp. 1934–\\n1943.\\n[41] C. Schuldt, I. Laptev, and B. Caputo, “Recognizing human actions: a\\nlocal SVM approach,” in ICPR , vol. 3, Aug. 2004, pp. 32–36 V ol.3,\\niSSN: 1051-4651.\\n[42] N. Srivastava, E. Mansimov, and R. Salakhudinov, “Unsupervised Learn-\\ning of Video Representations using LSTMs.” PMLR, Jun. 2015, pp.\\n843–852.\\n[43] F. Ebert, C. Finn, A. X. Lee, and S. Levine, “Self-supervised visual\\nplanning with temporal skip connections,” in 1st annual conference\\non robot learning, CoRL 2017, mountain view, california, USA,\\nnovember 13-15, 2017, proceedings , ser. Proceedings of machine\\nlearning research, vol. 78. PMLR, 2017, pp. 344–356. [Online].\\nAvailable: http://proceedings.mlr.press/v78/frederik-ebert17a.html\\n[44] R. Villegas, J. Yang, S. Hong, X. Lin, and H. Lee, “Decomposing motion\\nand content for natural video sequence prediction,” in ICLR , 2017.\\n[45] Y . Wang, L. Jiang, M.-H. Yang, L.-J. Li, M. Long, and L. Fei-Fei,\\n“Eidetic 3D LSTM: A Model for Video Prediction and Beyond,” in\\nICLR , Sep. 2018.\\n[46] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The\\nunreasonable effectiveness of deep features as a perceptual metric,” in\\nCVPR , 2018, pp. 586–595.\\n', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 24}),\n",
       " Document(page_content='[47] D. P. Kingma and J. L. Ba, “Adam: A method for stochastic optimiza-\\ntion,” in ICLR , 2015, pp. 1–15.\\n[48] I. Loshchilov and F. Hutter, “Decoupled Weight Decay Regularization,”\\nSep. 2018.\\n[49] Y . Wang, Z. Gao, M. Long, J. Wang, and P. S. Yu, “PredRNN++:\\nTowards A Resolution of the Deep-in-Time Dilemma in Spatiotemporal\\nPredictive Learning.” PMLR, Jul. 2018, pp. 5123–5132.\\n[50] J. Su, W. Byeon, J. Kossaiﬁ, F. Huang, J. Kautz, and A. Anandkumar,\\n“Convolutional Tensor-Train LSTM for Spatio-temporal Learning,” in\\nNeurIPS , 2020.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 25})]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have extracted texta s well images data from our pdf now time for chunking for this we are using 'RecursiveCharacterTextSplitter' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter #calling class of splitter \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20) #creating object of splitter class\n",
    "docs = text_splitter.split_documents(pages) #Chunks of our text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Future Frame Prediction Using Convolutional VRNN for Anomaly Detection\\nYiwei Lu, Mahesh Kumar Krishna Reddy, Seyed shahabeddin Nabavi and Yang Wang\\nUniversity of Manitoba, Winnipeg, MB, Canada\\n{luy2,kumarkm,nabaviss,ywang }@cs.umanitoba.ca\\nAbstract\\nAnomaly detection in videos aims at reporting anything\\nthat does not conform the normal behaviour or distribution.\\nHowever, due to the sparsity of abnormal video clips in real\\nlife, collecting annotated data for supervised learning is ex-\\nceptionally cumbersome. Inspired by the practicability of\\ngenerative models for semi-supervised learning, we propose\\na novel sequential generative model based on variational\\nautoencoder (VAE) for future frame prediction with convo-\\nlutional LSTM (ConvLSTM). To the best of our knowledge,\\nthis is the ﬁrst work that considers temporal information in\\nfuture frame prediction based anomaly detection framework\\nfrom the model perspective. Our experiments demonstrate', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 0}),\n",
       " Document(page_content='that our approach is superior to the state-of-the-art meth-\\nods on three benchmark datasets.\\n1. Introduction\\nAnomaly detection is an essential problem in video\\nsurveillance. Due to the massive amount of available video\\ndata from surveillance cameras, it is time-consuming and\\ninefﬁcient to have human observers watching surveillance\\nvideos and report any anomalies. Ideally, we want an au-\\ntomatic system that can report abnormal events. Anomaly\\ndetection is challenging since the deﬁnition of “anomaly”\\nis broad and ambiguous – anything that deviates expected\\nbehaviours can be considered as “anomaly”. It is infeasi-\\nble to collect labeled training data that cover all possible\\nanomalies. As a result, recent work in anomaly detection\\nhas focused on unsupervised approaches that do not require\\nhuman labels.\\nSome recent work (e.g. [10, 16, 17, 26]) in anomaly de-\\ntection uses the idea of frame reconstruction. They build\\nmodels that learn to reconstruct the normal (or regular)', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 0}),\n",
       " Document(page_content='frames observed during training. During testing, any irreg-\\nular (abnormal) event will lead to a large reconstruction er-\\nror. The higher reconstruction error indicates the possible\\nabnormal event in the frame. Previous work [1, 26, 15]\\nhas applied variants of generative models such as varia-\\ntional autoencoder (V AE) [14] or generative adversarial net-\\nFigure 1. An example of our proposed video anomaly detection\\nmethod. Our method uses a future frame prediction framework.\\nGiven several observed frames in a video, our model predicts the\\nfuture frame. If the future frame is an anomaly, the predicted\\nfuture frame is likely to be very different from the actual future\\nframe. This reconstruction error allows us to detect the anomaly\\nin a video.\\nwork (GAN) [9] to model the distribution of the natural be-\\nhaviours. To build a real-time anomaly detection system,\\nLiu et al. [15] propose a future frame prediction framework\\nfor anomaly detection. Given several observed frames, their', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 0}),\n",
       " Document(page_content='method learns a GAN-based model to predict the future\\nframe. An anomaly then corresponds to a large difference\\nbetween the predicted future frame and the actual future\\nframe. One limitation of [15] is that it directly concate-\\nnates the several observed frames as the input to the GAN\\nmodel. As a result, the model does not directly represent the\\ntemporal information in a video. Although [15] uses optical\\nﬂow features which capture some temporal information at\\nthe feature level, the optical ﬂow information is only used\\nas a constraint during training and is not used during testing.\\nIn this paper, we follow the future frame prediction\\nframework in [15] and propose a new approach that bet-\\nter capture the temporal information in a video for anomaly\\ndetection. We propose to combine sequential models (in\\nparticular, ConvLSTM) with generative models (in particu-\\n978-1-5386-9294-3/18/$31.00 2019 IEEEarXiv:1909.02168v2  [cs.CV]  18 Oct 2019\\nSuspicious Behaviour\\nNormal Events\\nDetected', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 0}),\n",
       " Document(page_content='Detected\\nNormal Events\\n1.0\\nProbability of Anomaly\\n0.8\\n0.6\\n0.4\\n0.2\\nSurveillance\\nVideo\\nPrediction', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 0}),\n",
       " Document(page_content='lar, V AE) to build a model that can be trained end-to-end.\\nAlthough sequential generative models have been previ-\\nously proposed for speech recognition and music generation\\n[23, 3], they have not been applied in anomaly detection. An\\nexample of our proposed video anomaly detection system\\ncan be seen in Fig 1. Given several consecutive frames, our\\nmodel learns to predict the next future frame. For normal\\nframes, our method is able to predict the next frame reason-\\nably well. When there is anomaly in the future frame, the\\nprediction is often distorted and blurry. By comparing the\\npredicted future frame with the actual future frame, our sys-\\ntem can detect suspicious behaviours or events (in this case,\\nthe man is throwing his bag up and down) are detected in a\\nvideo frame.\\nIn this paper, we make the following contributions. We\\npropose a sequential generative model for video anomaly\\ndetection using the future frame prediction framework. We', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 1}),\n",
       " Document(page_content='combine ConvLSTM with V AE to better capture the tempo-\\nral relationship among frames in a video. Our experimen-\\ntal results demonstrate that the proposed model outperforms\\nexisting state-of-the-art approaches, even without using op-\\ntical ﬂow features.\\n2. Related Work\\nIn this section, we review several lines of prior research\\nrelated to our work.\\nAnomaly Detection with Hand-crafted Features : Early\\nwork in video anomaly detection uses hand-crafted fea-\\ntures. [27, 30] use trajectory features to represent normal\\nbehaviours. However, these methods can not be applied to\\ncrowded scenes. To address this limitation, low-level fea-\\ntures such as histogram of oriented gradient and histogram\\nof oriented ﬂows are also applied [5, 6] for human detection.\\n[33, 16, 4] represent each scene by a dictionary of temporal\\nand spatial information. These approaches have low perfor-\\nmance due to the fact that the dictionary does not ensure\\nthe capacity of normal events and cannot classify anomaly', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 1}),\n",
       " Document(page_content='correspondingly. Statistical-based models have also been\\nproposed. For example, [13] proposes an approach based\\non a mixture of probabilistic PCA (MPPCA) with optical\\nﬂow pattern. Gaussian mixture model [19] has also been\\napplied for anomaly detection.\\nAnomaly Detection with Deep Learning : In order to ad-\\ndress the limitation of hand-crafted features in anomaly de-\\ntection, there has been recent work that explores the use of\\ndeep learning approaches. A lot of these methods learn a\\ndeep learning model to reconstruct a frame and use the re-\\nconstruction error for anomaly detection. Inspired by [20],\\nHasan et al. [10] apply convolutional autoencoder for re-\\nconstructing normal frames. Some follow-up works [25, 2]\\npropose to build a more robust version. Xu et al. [32] use\\nstacked de-noising autoencoders [28] and optical ﬂow to\\ncapture both appearance and motion information.Some work considers using a future frame prediction ap-\\nproach for anomaly detection. Medel et al. [22] apply Con-', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 1}),\n",
       " Document(page_content='vLSTM as a backbone network and build a future predic-\\ntion model for anomaly detection. Luo et al. [17] combine\\nautoencoder and ConvLSTM to reconstruct the output of\\nConvLSTM to the original image size. Because the inner\\nstructure of ConvLSTM is entirely deterministic, these pre-\\ndictive modeling methods cannot predict highly structured\\nmoving objects, which results in inaccurate predictions of\\nanomalies.\\nGenerative models, such as V AE [14] and GAN [9], have\\nbeen applied for the purpose of learning the distribution of\\nregular frames. Sabokrou et al. [26] propose a one class\\nclassiﬁer using conditional adversarial networks [12]. Xie\\net al. [31] use a GAN-based image inpainting method to\\ndetect and localize the abnormal objects. Liu et al. [15]\\npropose a GAN-based future frame prediction network with\\noptical ﬂow network[8]. An et al. [1] apply V AE to build an\\nanomaly detection system, but the method is not performed\\non real-world datasets.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 1}),\n",
       " Document(page_content='Sequential Generative Models : There has been some\\nwork on incorporating sequential information in generative\\nmodels. Chung et al. [3] argue that latent random variables\\ncan play crucial roles in the dynamics of RNN. By combin-\\ning V AE and RNN, they are able to model sequences with\\nsigniﬁcant improvement on RNN. However, this model has\\nonly been used on simple tasks such as speech generation\\nor handwriting generation. [23] propose a sequential gen-\\nerative model using adversarial training on RNN. They ar-\\ngue that with the supervision of a discriminator, their pro-\\nposed generative model can be trained to be very expres-\\nsive with high ﬂexibility on continuous sequences such as\\nmusic. However, the potential of this model on computer\\nvision tasks has not yet been explored.\\n3. Background\\n3.1. Variational Autoencoder\\nVariational autoencoder (V AE) [14] has been shown to\\nbe effective in reconstructing complex distributions for non-', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 1}),\n",
       " Document(page_content='sequential data. Given an input x, V AE applies an encoder\\n(also known as inference model) qθ(z|x)to generate the la-\\ntent variable zthat captures the variation in x. It uses a\\ndecoderpφ(x|z)to approximate the observation given the\\nlatent variable. The inference model represents the approx-\\nimate posterior using the mean µand variance σ2calculated\\nby a neural network qθ(z|x)∼N (µx, σ2\\nx), whereµxand\\nσ2\\nxare outputs of some neural networks that take xas the\\ninput. A prior p(z)is chosen to be a simple Gaussian dis-\\ntribution. With the constraints of distribution on latent vari-\\nables, the complete objective function can be described as', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 1}),\n",
       " Document(page_content='Figure 2. An overview of our proposed Conv-VRNN model at one time-step of a sequence. Our model requires 4 steps to process the input:\\n(a) calculating the prior distribution in V AE; (b) encoder for posterior distribution and latent variable; (c) recurrence module for sequence\\nmodelling; (d) decoder for prediction.\\nbelow:\\nL(x|θ,φ) =−KL(qθ(z|x)||p(z))+\\nEqθ(z|x)[logpφ(x|z)](1)\\nwhereKL(qθ(z|x)||p(z))is the Kullback-Leibler diver-\\ngence [11] between the prior and the posterior.\\n3.2. Variational Recurrent Neural Network\\nV AE is a generative model. It cannot directly be used\\nto model sequential data. For the problem of anomaly de-\\ntection, our data are inherently sequential since we need to\\nconsider the information in several consecutive frames in\\norder to predict the next frame. Variational Recurrent Neu-\\nral Network (VRNN) [3] is an extension of vanilla V AE. It\\ncombines V AE with a recurrent neural network in order to\\nmodel sequential data. Since this approach shares the same', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 2}),\n",
       " Document(page_content='inspiration with our Conv-VRNN approach, we will explain\\nthe technical details in the next section.\\n4. Approach\\nFollowing [15], we approach the anomaly detection\\nproblem using the future frame prediction framework. The\\ngoal is to build a model that takes several frames in a video\\nas the input and predict the future frame. The predicted fu-\\nture frame is then compared with the actual future frame.\\nIf their difference is signiﬁcant, we will consider it to be\\nan anomaly. The main difference from [15] is that our pro-\\nposed approach combines a recurrent network with a gen-\\nerative model. As a result, our approach can better capture\\ntemporal information in the video.\\nOur problem formulation is as follows. Given a se-\\nquence of frames x(1),...,x (T), we aim at predicting the\\nnext framex(T+ 1) . Note thatTis a constant which wedeﬁne as 4 in our case. We use x′(T+ 1) to denote the\\npredicted frame at time T+ 1. During training, we learn a', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 2}),\n",
       " Document(page_content='model that minimizes the difference between the predicted\\nand actual future frames, i.e. L=|(x(T+ 1)−x′(T+ 1)|.\\nDuring testing, if this difference is too large, we will con-\\nsiderx(T+ 1) to be an anomaly.\\nIn this section, we ﬁrst introduce our model Conv-\\nVRNN (Sec. 4.1) for future frame prediction. Our model\\ncombines V AE and a ConvLSTM module. We then describe\\nhow to use the proposed model to detect anomaly during\\ntesting (Sec. 4.2).\\n4.1. Conv-VRNN for Future Frame Prediction\\nTo extend V AE to model image sequences for anomaly\\ndetection, we use the idea of Variational Recurrent Neu-\\nral Network (VRNN) [3] and build a Conv-VRNN model\\nfor future frame prediction. An overview of our proposed\\nmodel is shown in Figure 2. Let x(t)∈RH×W×3be the\\ninput image at time t, whereH×Wis the spatial dimension\\nof the image. We deﬁne h(t)∈RH×W×3to be the hidden\\nstate of a ConvLSTM at time step t. Note that we choose\\nthe spatial dimension of h(t)to match the image size. Our', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 2}),\n",
       " Document(page_content='method consists of four components at each time step t:\\nPrior Distribution in VAE: This module takes the hid-\\nden stateh(t−1)from the previous time step as the in-\\nput. It then generates a distribution on the latent variable in\\nV AE. We ﬁrst extract a feature vector from h(t−1). Since\\nh(t−1)∈RH×W×3is a 3D tensor and can be treated as\\na image, we can use a standard convolutional neural net-\\nwork (CNN) to extract the feature from h(t−1). We de-\\nnote this feature as ϕh(h(t−1))∈RH′×W′×F, where\\nH′×W′andFcorrespond to the spatial dimension and\\nthe channel dimension of the CNN feature map. Here we\\nsetH′×W′×F= 16×16×32. We then apply two\\nz.(0)\\nH(0)\\nc(t)\\nPh(h(t --\\nPrior\\nh(t - 1)\\nx(t)\\nP. (r(0)\\nConvLSTM\\nh()\\n(1(t)2\\nh(t -\\n(a) Prior\\nKL-Divergence\\n(c) Recurrence\\nh(t - 1)\\nH2(0)\\nPr(h(T - 1)\\nz(t)\\nPosterior\\nx(T+ 1)\\nPrediction\\nx(t)\\n02 (0)2\\nZ(T)\\n(b) Encoder\\n(d) Decoder', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 2}),\n",
       " Document(page_content='different fully connected layers on ϕh(h(t−1))to produce\\ntwo vectors corresponding to the mean and the variance of a\\nGaussian distribution in V AE, denoted by µ1(t)andσ1(t).\\nIn our implementation, the dimension of µ1(t)andσ1(t)is\\nset to be 20, i.e. µ1(t),σ1(t)∈R20. We then use µ1(t)and\\nσl(t)to deﬁne a Gaussian distribution for the prior distribu-\\ntion on the latent variable in V AE as follows:\\nc(t)∼N(\\nµ1(t), diag(\\nσ1(t)2))\\n(2)\\nwherediag(·)creates a diagonal matrix from a vector and\\nc(t)represent the prior distribution on the latent variable.\\nEncoder: The module takes the hidden state h(t−1)of\\nprevious time step t−1and the frame x(t)at current time t\\nas the input. It then produces a vector of the latent variable\\nin V AE. We ﬁrst concatenate x(t)andh(t−1)along their\\nchannel dimensions, then apply a CNN to extract a feature\\nmap. Again, we apply two different fully connected layers\\non this feature map to produce µ2(t)andσ2(t). Similarly,', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 3}),\n",
       " Document(page_content='the dimension of µ2(t)andσ2(t)to be 20. We then deﬁne\\nthe posterior of the latent variable z(t)in V AE as:\\nqθ(z(t)|concat (x(t),h(t−1)))\\n∼N(\\nµ2(t), diag(\\nσ2(t)2)) (3)\\nwherez(t)∈R20. To measure the distribution loss between\\nEq. 2 and Eq. 3 at time step t, we can use the KL-divergence\\nmetricKL(qθ(z(t)|x(t),h(t−1))||c(t)).\\nRecurrence: To capture the temporal information among\\nframes in a video, we use a ConvLSTM to represent the\\nrecurrent relationship among frames. From the current in-\\nput imagex(t), we apply a CNN to extract a feature map\\nwhich we denote as ϕx(x(t))∈RH′×W′×F. To match\\nthe dimension of this feature, we also resize the latent vari-\\nablez(t)(recallz(t)∈R20) as follows. We ﬁrst use\\nfully connected layers to map z(t)to a high-dimensional\\nspace R1024, then reshape to a 3D tensor of dimension\\nH′×W′×F= 16×16×32. We usezr(t)∈RH′×W′×Fto\\ndenote this reshaped tensor. We concatenate the input fea-\\ntureϕx(x(t))with thezr(t)along the channel dimension', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 3}),\n",
       " Document(page_content='and use it as the input to ConvLSTM at time t:\\nh(t) =fConvLSTM (concat (ϕx(x(t)),zr(t)),h(t−1))\\n(4)\\nDecoder: This module takes the resized hidden state zr(t)\\nas its input and produces a predicted frame x′(t+ 1) for\\nthe next time-step. Note that the dimensions of zr(t)\\nmatch those of the extracted feature of previous hidden state\\nϕh(h(t−1)). We concatenate zr(t)andϕh(h(t−1))along\\nthe channel dimension. The result is used as the input of this\\ndecoder module. The decoder is implemented as a decon-\\nvolutional nerual network that generates the predicted frame\\nx′(t+ 1)∈RH×W×3.Model Learning: For learning parameters in Conv-VRNN,\\nwe combine the least absolute deviation ( L1loss) [24],\\nmulti-scale structural similarity measurement (msssim loss)\\n[29] and gradient difference (gdl loss) [21] to deﬁne a loss\\nthat measure the quality of the predicted frame. These three\\nloss functions can be deﬁned as follows:\\n(1) L1 loss between ground-truth and prediction is the sum-', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 3}),\n",
       " Document(page_content='mation of the absolute value between every pixel of the two\\nimages.\\n(2)We use multi-scale SSIM to represent the structural dif-\\nference. MSSSIM is a multi-scale version of SSIM, which\\nperforms better on video sequences.\\n(3) Gradient difference is widely used for measuring the\\nperformance of a prediction. Gradient difference loss con-\\nsiders the intensities difference between neighbour pixels.\\nOverall, given the predicted frame x′(T+ 1) and the\\nground-truth x(T+1), the complete loss function is deﬁned\\nas:\\nLprediction =L1(x(T+ 1),x′(T+ 1))\\n+Lmsssim (x(T+ 1),x′(T+ 1))\\n+Lgdl(x′(T+ 1),x′(T+ 1))(5)\\nWe deﬁne the complete objective function as:\\nL=T∑\\nt=1(−KL(qθ(z(t)|x(t),h(t−1))||c(t))) +Lprediction.\\n(6)\\n4.2. Anomaly Detection\\nGiven an input sequence of frames x(1),x(2),...,x (T)\\nduring testing, we use our model to predict the next frame\\nx′(T+ 1) in the future. This predicted future frame x′(T+\\n1)is compared with the ground-truth future frame x(T+ 1)', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 3}),\n",
       " Document(page_content='by calculating Lprediction (see Eq. 5). Same as [15], after\\ncalculating the overall spatial loss of each testing video, we\\nnormalize the losses to get a score S(t)in the range of [0,1]\\nfor each frame in the video by:\\nS(t) =Lprediction (t)−minLprediction\\nmaxLprediction−minLprediction. (7)\\nWe then use S(t)as the score indicating how likely a par-\\nticular frame is an anomaly.\\n5. Experiments\\nIn this section, we ﬁrst discuss our experimental setup\\nin Sec. 5.1. Then we present both quantitative and qualita-\\ntive results in Sec. 5.2. We also perform extensive ablation\\nstudies in Sec. 5.3 to analyze our proposed approach.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 3}),\n",
       " Document(page_content='normal abnormal\\nFigure 3. Example frames from the three datasets. 1st row: UCSD\\nPedestrian 1 (Ped1) dataset; 2nd row: UCSD Pedestrian 2 (Ped2)\\ndataset; 3nd row: CUHK Avenue dataset. We show both nor-\\nmal and abnormal frames from these datasets. The abnormal be-\\nhaviours are indicated by the red bounding box. Note that the red\\nbounding box is only for visualization purpose and is not used dur-\\ning training.\\n5.1. Experimental Setup\\nDatasets: We evaluate our method on three benchmark\\ndatasets. (1) UCSD Pedestrian 1 (Ped 1) dataset[19]: this\\ndataset contains 34 training videos and 36 testing videos.\\nIn training videos, only pedestrians exist in the frames. Test\\nvideos include 40 abnormal events, such as moving bicycles\\nand vehicles. (2) UCSD Pedestrian 2 (Ped 2) dataset[19].\\nThis dataset considers the same set of anomalies with the\\nUCSD Ped 1 dataset. It consists of 16 training videos and\\n12 testing videos with 12 irregular occasions. (3) CUHK', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 4}),\n",
       " Document(page_content='Avenue (Avenue) dataset [16]. This dataset consists of 16\\ntraining videos and 21 testing videos. It contains 47 abnor-\\nmal events like throwing things, wandering, and running.\\nFigure 3 shows some example frames from these datasets.\\nEvaluation Metric: Following prior work [15] [18] [19],\\nwe evaluate our methods using the area under the ROC\\ncurve (AUC). The ROC curve is obtained by varying the\\nthreshold for the anomaly score. A higher AUC value repre-\\nsents a more accurate anomaly detection system. To ensure\\nthe comparability between different methods, we calculate\\nAUC from the frame-level prediction, which has been used\\nby different existing methods.\\n5.2. Experimental Results\\nTable 1 shows the results of our proposed method com-\\npared with existing state-of-the-art approaches. To be con-Table 1. Comparison of different methods in terms of AUC on\\nUCSD Ped1, UCSD Ped2 and CUHK Avenue datasets.\\nPed1 Ped2 Avenue\\nMPCCA [13] 59.0% 69.3% N/A\\nDel et al.[7] N/A N/A 78.3%', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 4}),\n",
       " Document(page_content='Conv-AE [10] 75.0% 85.0% 80.0%\\nConvLSTM-AE [17] 75.5% 88.1% 77.0%\\nStacked RNN [18] N/A 92.2% 81.7%\\nLiu et al. [15] 83.1% 95.4% 84.9%\\nConv-VRNN (ours) 86.26% 96.06% 85.78%\\nTable 2. Comparision of Conv-V AEs versus Conv-VRNN in terms\\nof AUC on three datasets.\\nPed 1 Ped 2 Avenue\\nConv-V AE 82.42% 89.18% 81.82%\\nConv-VRNN 86.26% 96.06% 85.48%\\nsistent with [15], we have set T= 4. In other words, our\\nmodel takes 4 consecutive frames as the input and predicts\\nthe future frame at the next time step. It then compares\\nthe prediction with the actual frame at the next time step\\nto decide whether this frame is an anomaly. We can see\\nthat Conv-VRNN outperforms existing methods on all three\\ndatasets.\\nFigure 4 shows some qualitative examples of future\\nframe prediction. We can see that for a normal frame, the\\npredicted future frame tends to be close to the actual fu-\\nture prediction. For an abnormal frame, the predicted fu-\\nture frame tends to be blurry or distorted compared with', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 4}),\n",
       " Document(page_content='the actual future frame. Figure 5 shows example of de-\\ntected anomaly by visualizing the anomaly score on differ-\\nent frames in a video.\\n5.3. Ablation Study\\nWe perform additional ablation study to gain further in-\\nsights of our proposed methods.\\n5.3.1 Conv-VAE vs Conv-VRNN\\nIn order to analyze the effect of incorporating temporal in-\\nformation, we implement a variant of our model without\\nRNN. We call this variant Conv-V AE. Conv-V AE uses the\\nencoder module to encode a latent variable and uses the de-\\ncoder module for prediction. We have experimented with\\nConv-V AE that takes either one input frame or four frames\\nto predict the next frame. The results are shown in Table 2.\\nWe can see that Conv-VRNN outperforms Conv-V AE. This\\ndemonstrates the importance of capturing the temporal in-\\nformation using RNN for anomaly detection.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 4}),\n",
       " Document(page_content='GT (normal) prediction (normal) GT (abnormal) prediction (abnormal)\\nFigure 4. Examples of frame predictions on three datasets. The 1st row shows predicted frames that are normal. The 2nd row shows\\npredicted frames with anomalies. For an abnormal frame, the predicted frame tends to be blurry and distorted. The bounding boxes are for\\nvisualization purpose and are not part of the model prediction.\\nTable 3. Evaluation of different combinations of various loss terms\\nin the objective functions in our Conv-VRNN network on the Ped1\\ndataset. The results show that the combination of all loss terms\\ngives the best performance.\\nL1 \\x13 \\x13 \\x13\\nLmsssim \\x17 \\x13 \\x13\\nLgdl \\x17 \\x17 \\x13\\nAUC 80.29% 83.34% 86.26%\\n5.3.2 Analysis on Losses\\nAs we mentioned in Sec 4, we apply three different losses\\nfor prediction. The analysis of the impact of the losses\\ncan be visualized in Table 3. We choose three combi-\\nnations of objective functions for evaluation: constraint\\nonly on intensity ( L1), constraint on intensity and structure', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 5}),\n",
       " Document(page_content='(L1+Lmsssim ), constraint on intensity, structure and gra-\\ndient (L1+Lmsssim +Lgdl). The results demonstrate that\\nthe appearance information is better captured by the model\\nwith more constraints.\\n5.3.3 Sequential Model vs Optical Flow\\nOur Conv-VRNN uses a RNN module to capture the tem-\\nporal information in a video. An alternative way of cap-\\nturing temporal information is to use optical ﬂow features.\\nWe have implemented a Conv-V AE model with such con-\\nstraint. Following [15], we apply the pretrained Flownet [8]\\nto estimate the optical ﬂow, and use the returned loss of\\nthe Flownet as a motion constraint of the network only in\\ntraining time. Table 4, Figure 6 show that although adding\\noptical ﬂow in our implementation of Conv-V AE improvesTable 4. Comparison between our Conv-VRNN model with dif-\\nferent V AE-based models (with or without optical ﬂow features).\\nOur proposed Conv-VRNN outperforms Conv-V AE (with optical\\nﬂow) even if our model does not use optical ﬂow features.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 5}),\n",
       " Document(page_content='Ped1 Ped2 Avenue\\nConv-V AE(w/o optical ﬂow) 80.15% 88.13% 80.92%\\nConv-V AE(with optical ﬂow) 81.36% 89.52% 82.23%\\nConv-VRNN 86.26% 96.06% 85.78%\\nthe performance compared with Conv-V AE applied on only\\nraw frames, our proposed Conv-VRNN approach still per-\\nforms better even if we do not use optical ﬂow features.\\nThis demonstrates that it is more effective to design the gen-\\nerative model to directly capture the temporal information\\ninstead of relying on low-level optical ﬂow features.\\n6. Conclusion\\nIn this paper, we have proposed a sequential genera-\\ntive network for anomaly detection based on convolutional\\nVRNN using the future frame prediction framework. By\\ncombining a ConvLSTM module with V AE, our approach\\ncan effectively capture the temporal information crucial\\nfor future frame prediction. On three benchmark datasets,\\nour proposed approach outperforms existing state-of-the-art\\nmethods.\\nAcknowledgement : This work was supported by the', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 5}),\n",
       " Document(page_content='NSERC and UMGF funding programs. We thank NVIDIA\\nfor donating some of the GPUs used in this work.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 5}),\n",
       " Document(page_content=\"Ped1 Ped2\\nAvenue\\nFigure 5. Examples of anomaly detection on three datasets. We plot the anomaly score of our model and the ground-truth anomaly score.\\nAgain, the bounding boxes are for visualization purpose.\\nPed1 Ped2 Avenue\\nFigure 6. ROC curves of our Conv-VRNN method, Conv-V AE (w/o optical ﬂow) and Conv-V AE (with optical ﬂow) on three datasets.1.0\\nGround Truth\\n>>>>\\nConv-VRNN\\n0.8\\n0.6\\nScore\\n0.4\\n0.2\\n0.0\\n25\\n100\\n125\\n50\\n75\\n150\\n175\\n200\\nFrames\\nDetection'of Anomaly\\nNormal Scenes1.0\\nGround Truth\\nConv-VRNN\\n0.8\\n0.6\\nScore\\n0.4\\n0.2\\n0.0\\n75\\n100\\n125\\n150\\n25\\n50\\n175\\nFrames\\nNormal Scenes\\nDetection of Anomaly1.0\\n0.8\\nTruePositiveRate\\n0.6\\n0.4\\n0.2\\nConv-VAE(w/0optical flow)(AUC=0.81)\\nConv-VAE(with optical flow)(AUC =0.82)\\nConv-VRNN(AUC = 0.86)\\n0.0\\n0.2\\n0.0\\n0.4\\n0.6\\n0.8\\n1.0\\nFalse Positive Rate1.0\\n0.8-\\nTruePositiveRate\\n0.6\\n0.4\\n0.2\\nConv-VAE(w/0optical flow)(AUC=0.88)\\nConv-VAE(with optical flow)(AUC =0.89)\\nConv-VRNN(AUC = 0.96)\\n0.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nFalse Positive Rate1.0\\n0.8\\nTrue Positive Rate\\n0.6\\n0.4\\n0.2\", metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 6}),\n",
       " Document(page_content='0.6\\n0.4\\n0.2\\nConv-VAE(w/0 optical flow)(AUC = 0.82)\\nConv-VAE(with optical flow)(AUC = 0.83)\\nConv-VRNN(AUC = 0.86)\\n0.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nFalse Positive Rate', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 6}),\n",
       " Document(page_content='References\\n[1] J. An and S. Cho. Variational autoencoder based anomaly\\ndetection using reconstruction probability. Special Lecture\\non IE , 2015.\\n[2] R. Chalapathy, A. K. Menon, and S. Chawla. Robust, deep\\nand inductive anomaly detection. In Joint European Con-\\nference on Machine Learning and Knowledge Discovery in\\nDatabases , 2017.\\n[3] J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and\\nY . Bengio. A recurrent latent variable model for sequential\\ndata. In Advances in Neural Information Processing Systems ,\\n2015.\\n[4] Y . Cong, J. Yuan, and J. Liu. Sparse reconstruction cost for\\nabnormal event detection. In IEEE Conference on Computer\\nVision and Pattern Recognition , 2011.\\n[5] N. Dalal and B. Triggs. Histograms of oriented gradients for\\nhuman detection. In IEEE Conference on Computer Vision\\nand Pattern Recognition , 2005.\\n[6] N. Dalal, B. Triggs, and C. Schmid. Human detection using\\noriented histograms of ﬂow and appearance. In European\\nConference on Computer Vision , 2006.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 7}),\n",
       " Document(page_content='[7] A. Del Giorno, J. A. Bagnell, and M. Hebert. A discrimi-\\nnative framework for anomaly detection in large videos. In\\nEuropean Conference on Computer Vision , 2016.\\n[8] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,\\nV . Golkov, P. Van Der Smagt, D. Cremers, and T. Brox.\\nFlownet: Learning optical ﬂow with convolutional net-\\nworks. In IEEE International Conference on Computer Vi-\\nsion, 2015.\\n[9] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\\nD. Warde-Farley, S. Ozair, A. Courville, and Y . Bengio. Gen-\\nerative adversarial nets. In Advances in Neural Information\\nProcessing Systems , 2014.\\n[10] M. Hasan, J. Choi, J. Neumann, A. K. Roy-Chowdhury,\\nand L. S. Davis. Learning temporal regularity in video se-\\nquences. In IEEE Conference on Computer Vision and Pat-\\ntern Recognition , 2016.\\n[11] J. R. Hershey and P. A. Olsen. Approximating the kull-\\nback leibler divergence between gaussian mixture models.\\nInIEEE International Conference on Acoustics, Speech and', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 7}),\n",
       " Document(page_content='Signal Processing , 2007.\\n[12] P. Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros. Image-to-\\nimage translation with conditional adversarial networks. In\\nIEEE Conference on Computer Vision and Pattern Recogni-\\ntion, 2017.\\n[13] J. Kim and K. Grauman. Observe locally, infer globally: a\\nspace-time mrf for detecting abnormal activities with incre-\\nmental updates. In IEEE Conference on Computer Vision\\nand Pattern Recognition , 2009.\\n[14] D. P. Kingma and M. Welling. Auto-encoding variational\\nbayes. arXiv:1312.6114 , 2013.\\n[15] W. Liu, W. Luo, D. Lian, and S. Gao. Future frame prediction\\nfor anomaly detection–a new baseline. In IEEE Conference\\non Computer Vision and Pattern Recognition , 2018.\\n[16] C. Lu, J. Shi, and J. Jia. Abnormal event detection at 150 fps\\nin matlab. In IEEE International Conference on Computer\\nVision , 2013.[17] W. Luo, W. Liu, and S. Gao. Remembering history with con-\\nvolutional lstm for anomaly detection. In IEEE International\\nConference on Multimedia and Expo , 2017.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 7}),\n",
       " Document(page_content='[18] W. Luo, W. Liu, and S. Gao. A revisit of sparse coding based\\nanomaly detection in stacked rnn framework. In IEEE Inter-\\nnational Conference on Computer Vision , 2017.\\n[19] V . Mahadevan, W. Li, V . Bhalodia, and N. Vasconcelos.\\nAnomaly detection in crowded scenes. In IEEE Conference\\non Computer Vision and Pattern Recognition , 2010.\\n[20] J. Masci, U. Meier, D. Cires ¸an, and J. Schmidhuber. Stacked\\nconvolutional auto-encoders for hierarchical feature extrac-\\ntion. In International Conference on Artiﬁcial Neural Net-\\nworks , 2011.\\n[21] M. Mathieu, C. Couprie, and Y . LeCun. Deep multi-scale\\nvideo prediction beyond mean square error. In International\\nConference on Learning Representations , 2016.\\n[22] J. R. Medel and A. Savakis. Anomaly detection in video\\nusing predictive convolutional long short-term memory net-\\nworks. arXiv:1612.00390 , 2016.\\n[23] O. Mogren. C-rnn-gan: Continuous recurrent neural net-\\nworks with adversarial training. Constructive Machine', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 7}),\n",
       " Document(page_content='Learning Workshop at NIPS , 2016.\\n[24] D. Pollard. Asymptotics for least absolute deviation regres-\\nsion estimators. Econometric Theory , 1991.\\n[25] M. Sabokrou, M. Fathy, and M. Hoseini. Video anomaly\\ndetection and localization based on the sparsity and recon-\\nstruction error of auto-encoder. Electronics Letters , 2016.\\n[26] M. Sabokrou, M. Khalooei, M. Fathy, and E. Adeli. Adver-\\nsarially learned one-class classiﬁer for novelty detection. In\\nIEEE Conference on Computer Vision and Pattern Recogni-\\ntion, 2018.\\n[27] F. Tung, J. S. Zelek, and D. A. Clausi. Goal-based trajec-\\ntory analysis for unusual behaviour detection in intelligent\\nsurveillance. Image and Vision Computing , 2011.\\n[28] P. Vincent, H. Larochelle, Y . Bengio, and P.-A. Manzagol.\\nExtracting and composing robust features with denoising au-\\ntoencoders. In International Conference on Machine Learn-\\ning, 2008.\\n[29] Z. Wang, E. P. Simoncelli, and A. C. Bovik. Multiscale struc-', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 7}),\n",
       " Document(page_content='tural similarity for image quality assessment. In The Thrity-\\nSeventh Asilomar Conference on Signals, Systems & Com-\\nputers , 2003.\\n[30] S. Wu, B. E. Moore, and M. Shah. Chaotic invariants\\nof lagrangian particle trajectories for anomaly detection in\\ncrowded scenes. In IEEE Conference on Computer Vision\\nand Pattern Recognition , 2010.\\n[31] J. Xie, L. Xu, and E. Chen. Image denoising and inpainting\\nwith deep neural networks. In Advances in Neural Informa-\\ntion Processing Systems , 2012.\\n[32] D. Xu, Y . Yan, E. Ricci, and N. Sebe. Detecting anomalous\\nevents in videos by learning deep representations of appear-\\nance and motion. Computer Vision and Image Understand-\\ning, 2017.\\n[33] B. Zhao, L. Fei-Fei, and E. P. Xing. Online detection of un-\\nusual events in videos via dynamic sparse coding. In IEEE\\nConference on Computer Vision and Pattern Recognition ,\\n2011.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 7}),\n",
       " Document(page_content='Future Frame Prediction for Anomaly Detection – A New Baseline\\nWen Liu∗, Weixin Luo∗, Dongze Lian, Shenghua Gao†\\nShanghaiTech University\\n{liuwen, luowx, liandz, gaoshh }@shanghaitech.edu.cn\\nAbstract\\nAnomaly detection in videos refers to the identiﬁcation of\\nevents that do not conform to expected behavior. However,\\nalmost all existing methods tackle the problem by minimiz-\\ning the reconstruction errors of training data, which can-\\nnot guarantee a larger reconstruction error for an abnor-\\nmal event. In this paper, we propose to tackle the anomaly\\ndetection problem within a video prediction framework. To\\nthe best of our knowledge, this is the ﬁrst work that lever-\\nages the difference between a predicted future frame and\\nits ground truth to detect an abnormal event. To predict a\\nfuture frame with higher quality for normal events, other\\nthan the commonly used appearance (spatial) constraints\\non intensity and gradient, we also introduce a motion (tem-', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 8}),\n",
       " Document(page_content='poral) constraint in video prediction by enforcing the opti-\\ncal ﬂow between predicted frames and ground truth frames\\nto be consistent, and this is the ﬁrst work that introduces\\na temporal constraint into the video prediction task. Such\\nspatial and motion constraints facilitate the future frame\\nprediction for normal events, and consequently facilitate\\nto identify those abnormal events that do not conform the\\nexpectation. Extensive experiments on both a toy dataset\\nand some publicly available datasets validate the effec-\\ntiveness of our method in terms of robustness to the un-\\ncertainty in normal events and the sensitivity to abnormal\\nevents. All codes are released in https://github.\\ncom/StevenLiuWen/ano_pred_cvpr2018 .\\n1. Introduction\\nAnomaly detection in videos refers to the identiﬁcation\\nof events that do not conform to expected behavior [3]. It\\nis an important task because of its applications in video\\nsurveillance. However, it is extremely challenging because', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 8}),\n",
       " Document(page_content='abnormal events are unbounded in real applications, and it\\nis almost infeasible to gather all kinds of abnormal events\\nand tackle the problem with a classiﬁcation method.\\n∗The authors contribute equally and are listed in alphabetical order.\\n†Corresponding author.\\nInput \\n\\x7f ,\\x7f!,…,\\x7f\"Ground truth \\n\\x7f\"# Prediction \\n$\\x7f\"# normal abnormal \\nFigure 1. Some predicted frames and their ground truth in nor-\\nmal and abnormal events. Here the region is walking zone. When\\npedestrians are walking in the area, the frames can be well pre-\\ndicted. While for some abnormal events (a bicycle intrudes/ two\\nmen are ﬁghting), the predictions are blurred and with color dis-\\ntortion. Best viewed in color.\\nLots of efforts have been made for anomaly detec-\\ntion [20][13][23]. Of all these work, the idea of feature\\nreconstruction for normal training data is a commonly used\\nstrategy. Further, based on the features used, all existing\\nmethods can be roughly categorized into two categories: i)', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 8}),\n",
       " Document(page_content='hand-crafted features based methods [6][20]. They repre-\\nsent each video with some hand-crafted features including\\nappearance and motion ones. Then a dictionary is learnt to\\nreconstruct normal events with small reconstruction errors.\\nIt is expected that the features corresponding to abnormal\\nevents would have larger reconstruction errors. But since\\nthe dictionary is not trained with abnormal events and it is\\nusually overcomplete, we cannot guarantee the expectation.\\nii) deep learning based methods [13][5][26]. They usually\\nlearn a deep neural network with an Auto-Encoder way and\\nthey enforce it to reconstruct normal events with small re-\\nconstruction errors. But the capacity of deep neural network\\nis high, and larger reconstruction errors for abnormal events\\ndo not necessarily happen. Thus, we can see that almost all\\ntraining data reconstruction based methods cannot guaran-\\n1\\narXiv:1712.09867v3  [cs.CV]  13 Mar 2018', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 8}),\n",
       " Document(page_content='Flownet Optical \\nFlow Loss \\n\\x7f\\x81\\x8d\\x8f\\x90\\x81\\x8d\\x8f\\x90Flownet \\nIntensity Loss and \\nGradient Loss Generator \\n(U-Net) Discriminator \\n\\x81\\x90,\\x81 ,…,\\x81\\x8d\\nReal or Fake \\x81\\x8d\\nFigure 2. The pipeline of our video frame prediction network. Here we adopt U-Net as generator to predict next frame. To generate\\nhigh quality image, we adopt the constraints in terms of appearance (intensity loss and gradient loss) and motion (optical ﬂow loss).\\nHere Flownet is a pretrained network used to calculate optical ﬂow. We also leverage the adversarial training to discriminate whether the\\nprediction is real or fake.\\ntee the ﬁnding of abnormal events.\\nIt is interesting that even though anomaly is deﬁned as\\nthose events do not conform the expectation, most existing\\nwork in computer vision solve the problem within a frame-\\nwork of reconstructing training data [20][38][13]. We pre-\\nsume it is probable that the video frame prediction is far\\nfrom satisfactory at that time. Recently, as the emergence', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 9}),\n",
       " Document(page_content='of Generative Adversarial Network (GAN) [12], the perfor-\\nmance of video prediction has been greatly advanced [25].\\nIn this paper, rather than reconstructing training data for\\nanomaly detection, we propose to identify abnormal events\\nby comparing them with their expectation, and introduce\\na future video frame prediction based anomaly detection\\nmethod. Speciﬁcally, given a video clip, we predict the fu-\\nture frame based on its historical observation. We ﬁrst train\\na predictor that can well predict the future frame for nor-\\nmal training data. In the testing phase, if a frame agrees\\nwith its prediction, it potentially corresponds to a normal\\nevent. Otherwise, it potentially corresponds to an abnormal\\nevent. Thus a good predictor is a key to our task. We im-\\nplement our predictor with an U-Net [28] network architec-\\nture given its good performance at image-to-image trans-\\nlation [15]. First, we impose a constraint on the appear-\\nance by enforcing the intensity and gradient maps of the', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 9}),\n",
       " Document(page_content='predicted frame to be close to its ground truth; Then, mo-\\ntion is another important feature for video characterization,\\nand a good prediction should be consistent with real object\\nmotion. Thus we propose to introduce a motion constraint\\nby enforcing the optical ﬂow between predicted frames to\\nbe close to their ground truth. Further, we also add a Gen-\\nerative Adversarial Network (GAN) [12] module into our\\nframework in light of its success for video generation [25]\\nand image generation [9].\\nWe summarize our contributions as follows: i) We\\npropose a future frame prediction based framework foranomaly detection. Our solution agrees with the concept of\\nanomaly detection that normal events are predictable while\\nabnormal ones are unpredictable. Thus our solution is more\\nsuitable for anomaly detection. To the best of our knowl-\\nedge, it is the ﬁrst work that leverages video prediction for\\nanomaly detection; ii) For the video frame prediction frame-', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 9}),\n",
       " Document(page_content='work, other than enforcing predicted frames to be close to\\ntheir ground truth in spatial space, we also enforce the opti-\\ncal ﬂow between predicted frames to be close to their optical\\nﬂow ground truth. Such a temporal constraint is shown to\\nbe crucial for video frame prediction, and it is also the ﬁrst\\nwork that leverages a motion constraint for anomaly detec-\\ntion; iii) Experiments on toy dataset validate the robustness\\nto the uncertainty for normal events, which validates the ro-\\nbustness of our method. Further, extensive experiments on\\nreal datasets show that our method outperforms all existing\\nmethods.\\n2. Related Work\\n2.1. Hand-crafted Features Based Anomaly Detec-\\ntion\\nHand-crafted features based anomaly detection is mainly\\ncomprised of three modules: i) extracting features; In this\\nmodule, the features are either hand-crafted or learnt on\\ntraining set; ii) learning a model to characterize the distri-\\nbution of normal scenarios or encode regular patterns; iii)', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 9}),\n",
       " Document(page_content='identifying the isolated clusters or outliers as anomalies.\\nFor feature extraction module, early work usually utilizes\\nlow-level trajectory features, a sequence of image coordi-\\nnates, to represent the regular patterns [32][35]. However,\\nthese methods are not robust in complex or crowded scenes\\nwith multiple occlusions and shadows, because trajectory\\nfeatures are based on object tracking and it is very easy to\\nfail in these cases. Taking consideration of the shortcom-\\n2', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 9}),\n",
       " Document(page_content='ings of trajectory features, low-level spatial-temporal fea-\\ntures, such as histogram of oriented gradients (HOG) [27],\\nhistogram of oriented ﬂows (HOF) [7] are widely used.\\nBased on spatial-temporal features, Zhang et al. [37] ex-\\nploit a Markov random ﬁled (MRF) for modeling the nor-\\nmal patterns. Adam et al. [2] characterize the regularly lo-\\ncal histograms of optical ﬂow by an exponential distribu-\\ntion. Kim and Grauman [16] model the local optical ﬂow\\npattern with a mixture of probabilistic PCA (MPPCA). Ma-\\nhadevan et al. [23] ﬁt a Gaussian mixture model to mixture\\nof dynamic textures (MDT). Besides these statistic models,\\nsparse coding or dictionary learning is also a popular ap-\\nproach to encode the normal patterns [38][20][6]. The fun-\\ndamental underlying assumption of these methods is that\\nany regular pattern can be linearly represented as a linear\\ncombination of basis of a dictionary which encodes normal\\npatterns on training set. Therefore, a pattern is considered as', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 10}),\n",
       " Document(page_content='an anomaly if its reconstruction error is high and vice verse.\\nHowever, optimizing the sparse coefﬁcients is usually time-\\nconsuming in sparse reconstruction based methods. In order\\nto accelerate both in training and testing phase, Lu et al [20]\\npropose to discard the sparse constraint and learn multiple\\ndictionaries to encode normal scale-invariant patches.\\n2.2. Deep Learning Based Anomaly Detection.\\nDeep learning approaches have demonstrated their suc-\\ncesses in many computer vision tasks [18][11] as well as\\nanomaly detection [13]. In the work [36], Xu et al . de-\\nsign a multi-layer auto-encoder for feature learning, which\\ndemonstrates the effectiveness of deep learning features. In\\nanother work [13], a 3D convolutional auto-encoder (Conv-\\nAE) is proposed by Hasan to model regular frames. Fur-\\nther, motivated by the observation that Convolutional Neu-\\nral Networks (CNN) has strong capability to learn spatial\\nfeatures, while Recurrent Neural Network (RNN)and its', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 10}),\n",
       " Document(page_content='long short term memory (LSTM) variant have been widely\\nused for sequential data modeling. Thus, by taking both\\nadvantages of CNN and RNN, [5][21] leverage a Convo-\\nlutional LSTMs Auto-Encoder (ConvLSTM-AE) to model\\nnormal appearance and motion patterns at the same time,\\nwhich further boosts the performance of the Conv-AE based\\nsolution. In [22], Luo et al. propose a temporally coherent\\nsparse coding based method which can map to a stacked\\nRNN framework. Besides, Ryota et al. [14] combine de-\\ntection and recounting of abnormal events. However, all\\nthese anomaly detections are based on the reconstruction of\\nregular training data, even though all these methods assume\\nthat abnormal events would correspond to larger reconstruc-\\ntion errors, due to the good capacity and generalization of\\ndeep neural network, this assumption does not necessarily\\nhold. Therefore, reconstruction errors of normal and abnor-\\nmal events will be similar, resulting in less discrimination.2.3. Video Frame Prediction', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 10}),\n",
       " Document(page_content='Recently, prediction learning is attracting more and more\\nresearchers’ attention in light of its potential applications in\\nunsupervised feature learning for video representation [25].\\nIn [29], Shi et al. propose to modify original LSTM with\\nConvLSTM and use it for precipitation forecasting. In [25],\\na multi-scale network with adversarial training is proposed\\nto generate more natural future frames in videos. In [19],\\na predictive neural network is designed and each layer in\\nthe network also functions as making local predictions and\\nonly forwarding deviations. All aforementioned work fo-\\ncuses on how to directly predict future frames. Different\\nfrom these work, recently, people propose to predict trans-\\nformations needed for generating future frames [33] and [4],\\nwhich further boosts the performance of video prediction.\\n3. Future Frame Prediction Based Anomaly\\nDetection Method\\nSince anomaly detection is the identiﬁcation of events\\nthat do not conform the expectation, it is more natural', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 10}),\n",
       " Document(page_content='to predict future video frames based on previous video\\nframes, and compare the prediction with its ground truth\\nfor anomaly detection. Thus we propose to leverage video\\nprediction for anomaly detection. To generate a high qual-\\nity video frame, most existing work [15][25] only consid-\\ners appearance constraints by imposing intensity loss [25],\\ngradient loss [25], or adversarial training loss [15]. How-\\never, only appearance constraints cannot guarantee to char-\\nacterize the motion information well. Besides spatial in-\\nformation, temporal information is also an important fea-\\nture of videos. So we propose to add an optical ﬂow con-\\nstraint into the objective function to guarantee the motion\\nconsistency for normal events in training set, which further\\nboosts the performance for anomaly detection, as shown in\\nthe experiment section (section 4.5 and 4.6). It is worth\\nnoting abnormal events can be justiﬁed by either appear-\\nance (A giant monster appears in a shopping mall) or mo-', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 10}),\n",
       " Document(page_content='tion (A pickpocket walks away from an unlucky guy), and\\nour future frame prediction solution leverages both the ap-\\npearance and motion loss for normal events, therefore these\\nabnormal events can be easily identiﬁed by comparing the\\nprediction and ground truth. Thus the appearance and mo-\\ntion losses based video prediction are more consistent with\\nanomaly detection.\\nMathematically, given a video with consecutive tframes\\nI1,I2,...,I t, we sequentially stack all these frames and use\\nthem to predict a future frame It+1. We denote our predic-\\ntion as ˆIt+1. To make ˆIt+1close toIt+1, we minimize their\\ndistance regarding intensity as well as gradient. To pre-\\nserve the temporal coherence between neighboring frames,\\nwe enforce the optical ﬂow between It+1andItand that\\nbetween ˆIt+1andItto be close. Finally, the difference\\n3', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 10}),\n",
       " Document(page_content='Max pooling \\nConvolution \\nDeconvolution \\nConcatenate 256x256 3 64 64 128x128 64 128 128 \\n128 64x64 256 256 \\n256 32x32 512 512 512 \\n128x128 256 128 \\n256 256 128 128 64 64 3256x256 \\n256x256 \\n128x128 \\n128x128 \\n64x64 64x64 \\n32x32 32x32 64x64 \\n64x64 64x64 \\n128x128 \\n128x128 256x256 \\n256x256 \\n256x256 \\n256x256 Figure 3. The network architecture of our main prediction network\\n(U-Net). The resolutions of input and output are the same.\\nbetween a future frame’s prediction and itself determines\\nwhether it is normal or abnormal. The network architecture\\nof our framework is shown in Fig. 2. Next, we will intro-\\nduce all the components of our framework in details.\\n3.1. Future Frame Prediction\\nThe network commonly used for frame generation or\\nimage generation in existing work [25][13] usually con-\\ntains two modules: i) an encoder which extracts features by\\ngradually reducing the spatial resolution; and ii) a decoder\\nwhich gradually recovers the frame by increasing the spa-', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 11}),\n",
       " Document(page_content='tial resolution. However, such a solution confronts with the\\ngradient vanishing problem and information imbalance in\\neach layer. To avoid this, U-Net[28] is proposed by adding\\na shortcut between a high level layer and a low level layer\\nwith the same resolution. Such a manner suppresses gra-\\ndient vanishing and results in information symmetry. We\\nslightly modify U-Net for future frame prediction in our\\nimplementation. Speciﬁcally, for each two convolution lay-\\ners, we keep output resolution unchanged. Consequently, it\\ndoes not need the crop and resize operations anymore when\\nadding shortcuts. The details of this network are illustrated\\nin Figure 3. The kernel sizes of all convolution and decon-\\nvolution are set to 3×3and that of max pooling layers are\\nset to 2×2.\\n3.2. The Constraints on Intensity and Gradient\\nTo make the prediction close to its ground truth, follow-\\ning the work [25], intensity and gradient difference are used.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 11}),\n",
       " Document(page_content='The intensity penalty guarantees the similarity of all pixels\\nin RGB space, and the gradient penalty can sharpen the gen-\\nerated images. Speciﬁcally, we minimize the ℓ2distance be-\\ntween a predicted frame ˆIand its ground true Iin intensity\\nspace as follows:\\nLint(ˆI,I) =∥ˆI−I∥2\\n2 (1)Further, we deﬁne the gradient loss by following previous\\nwork [25] as follows:\\nLgd(ˆI,I) =∑\\ni,j\\ued79\\ued79|ˆIi,j−ˆIi−1,j|−|Ii,j−Ii−1,j|\\ued79\\ued79\\n1\\n+\\ued79\\ued79|ˆIi,j−ˆIi,j−1|−|Ii,j−Ii,j−1|\\ued79\\ued79\\n1(2)\\nwherei,jdenote the spatial index of a video frame.\\n3.3. The Constraint on Motion\\nPrevious work [25] only considers the difference be-\\ntween intensity and gradient for future frame generation,\\nand it can not guarantee to predict a frame with the correct\\nmotion. This is because even a small change occurs in terms\\nof the pixel intensity of all pixels in a predicted frame, even\\nthough it corresponds to a small prediction error in terms\\nof gradient and intensity, it may result in totally different', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 11}),\n",
       " Document(page_content='optical ﬂow, which is a good estimator of motion [30]. So\\nit is desirable to guarantee the correctness of motion pre-\\ndiction. Especially for anomaly detection, the coherence of\\nmotion is an important factor for the evaluation of normal\\nevents. Therefore, we introduce a temporal loss deﬁned as\\nthe difference between optical ﬂow of prediction frames and\\nground truth. However, the calculation of optical ﬂow is not\\neasy. Recently, a CNN based approach has been proposed\\nfor optical ﬂow estimation [8]. Thus we use the Flownet\\n[8] for optical ﬂow estimation. We denote fas the Flownet,\\nthen the loss in terms of optical ﬂow can be expressed as\\nfollows:\\nLop=∥f(ˆIt+1,It)−f(It+1,It)∥1 (3)\\nIn our implementation, fis pre-trained on a synthesized\\ndataset [8], and all the parameters in fare ﬁxed.\\n3.4. Adversarial Training\\nGenerative adversarial networks (GAN) have demon-\\nstrated its usefulness for image and video generation\\n[9][25]. By following [25], we also leverage a variant of', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 11}),\n",
       " Document(page_content='GAN (Least Square GAN [24]) module for generating a\\nmore realistic frame. Usually GAN contains a discrimi-\\nnative networkDand a generator network G.Glearns to\\ngenerate frames that are hard to be classiﬁed by D, while\\nDaims to discriminate the frames generated by G. Ideally,\\nwhenGis well trained,Dcannot predict better than chance.\\nIn practice, adversarial training is implemented with an al-\\nternative update manner. Moreover, we treat the U-Net\\nbased prediction network as G. As forD, we follow [15]\\nand utilize a patch discriminator which means each output\\nscalar ofDcorresponds a patch of an input image. Totally,\\nthe training schedule is illustrated as follows:\\nTrainingD. The goal of training Dis to classify It+1\\ninto class 1 andG(I1,I2,...,I t) =ˆIt+1into class 0, where 0\\nand 1 represent fake and genuine labels, respectively. When\\n4', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 11}),\n",
       " Document(page_content='trainingD, we ﬁx the weights of G, and a Mean Square\\nError (MSE) loss function is imposed:\\nLD\\nadv(ˆI,I) =∑\\ni,j1\\n2LMSE(D(I)i,j,1)\\n+∑\\ni,j1\\n2LMSE(D(ˆI)i,j,0)(4)\\nwherei,jdenotes the spatial patches indexes and LMSE is\\na MSE function, which is deﬁned as follows:\\nLMSE(ˆY,Y) = ( ˆY−Y)2(5)\\nwhereYtakes values in{0,1}andˆY∈[0,1]\\nTrainingG. The goal of training Gis to generate frames\\nwhereDclassify them into class 1. When training G, the\\nweights ofDare ﬁxed. Again, a MSE function is imposed\\nas follows:\\nLG\\nadv(ˆI) =∑\\ni,j1\\n2LMSE(D(ˆI)i,j,1) (6)\\n3.5. Objective Function\\nWe combine all these constraints regarding appearance,\\nmotion, and adversarial training, into our objective function,\\nand arrive at the following objective function:\\nLG=λintLint(ˆIt+1,It+1)\\n+λgdLgd(ˆIt+1,It+1)\\n+λopLop\\n+λadvLG\\nadv(ˆIt+1)(7)\\nWhen we trainD, we use the following loss function:\\nLD=LD\\nadv(ˆIt+1,It+1) (8)\\nTo train the network, the intensity of pixels in all frames\\nare normalized to [-1, 1] and the size of each frame is re-', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 12}),\n",
       " Document(page_content='sized to 256×256. We sett= 4 and use a random\\nclip of 5 sequential frames which is the same with [25].\\nAdam [17] based Stochastic Gradient Descent method is\\nused for parameter optimization. The mini-batch size is 4.\\nFor gray scale datasets, the learning rate of generator and\\ndiscriminator are set to 0.0001 and 0.00001, respectively.\\nWhile for color scale datasets, the learning rate of gener-\\nator and discriminator start from 0.0002 and 0.00002, re-\\nspectively. For different datasets, the coefﬁcient factors of\\nλint,λgd,λopandλadvare slightly different. An easy way\\nis to setλint,λgd,λopandλadvas 1.0, 1.0, 2.0 and 0.05,\\nrespectively.3.6. Anomaly Detection on Testing Data\\nWe assume that normal events can be well predicted.\\nTherefore, we can use the difference between predicted\\nframe ˆIand its ground truth Ifor anomaly prediction. MSE\\nis one popular way to measure the quality of predicted im-\\nages by computing a Euclidean distance between the predic-', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 12}),\n",
       " Document(page_content='tion and its ground truth of all pixels in RGB color space.\\nHowever, Mathieu [25] shows that Peak Signal to Noise Ra-\\ntio (PSNR) is a better way for image quality assessment,\\nshown as following:\\nPSNR (I,ˆI) = 10 log10[max ˆI]2\\n1\\nN∑N\\ni=0(Ii−ˆIi)2\\nHigh PSNR of the t-th frame indicates that it is more likely\\nto be normal. After calculating each frame’s PSNR of each\\ntesting video, following the work [25], we normalize PSNR\\nof all frames in each testing video to the range [0, 1] and\\ncalculate the regular score for each frame by using the fol-\\nlowing equation:\\nS(t) =PSNR (It,ˆIt)−mintPSNR (It,ˆIt)\\nmax tPSNR (It,ˆIt)−mintPSNR (It,ˆIt)\\nTherefore, we can predict whether a frame is normal or ab-\\nnormal based its score S(t). One can set a threshold to dis-\\ntinguish regular or irregular frames.\\n4. Experiments\\nIn this section, we evaluate our proposed method as\\nwell as the functionalities of different components on three\\npublicly available anomaly detection datasets, including', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 12}),\n",
       " Document(page_content='the CUHK Avenue dataset [20], the UCSD Pedestrian\\ndataset [23] and the ShanghaiTech dataset [22]. We further\\nuse a toy dataset to validate the robustness of our method,\\ni.e., even if there exists some uncertainties in normal events,\\nour method can still correctly classify normal and abnormal\\nevents.\\n4.1. Datasets\\nHere we brieﬂy introduce the datasets used in our exper-\\niments. Some samples are shown in Fig. 4.\\n•CUHK Avenue dataset contains 16 training videos and\\n21 testing ones with a total of 47 abnormal events, in-\\ncluding throwing objects, loitering and running. The\\nsize of people may change because of the camera po-\\nsition and angle.\\n•The UCSD dataset contains two parts: The UCSD\\nPedestrian 1 (Ped1) dataset and the UCSD Pedestrian 2\\n(Ped2) dataset. The UCSD Pedestrian 1 (Ped1) dataset\\nincludes 34 training videos and 36 testing ones with\\n5', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 12}),\n",
       " Document(page_content='Table 1. AUC of different methods on the Avenue, Ped1, Ped2 and ShanghaiTech datasets.\\nCUHK Avenue UCSD Ped1 UCSD Ped2 ShanghaiTech\\nMPPCA [16] N/A 59.0% 69.3% N/A\\nMPPC+SFA [23] N/A 66.8% 61.3% N/A\\nMDT [23] N/A 81.8% 82.9% N/A\\nConv-AE [13] 80.0% 75.0% 85.0% 60.9%\\nDelet al. [10] 78.3% N/A N/A N/A\\nConvLSTM-AE [21] 77.0% 75.5% 88.1% N/A\\nUnmasking [31] 80.6% 68.4% 82.2% N/A\\nHinami et al. [14] N/A N/A 92.2% N/A\\nStacked RNN [22] 81.7% N/A 92.2% 68.0%\\nOur proposed method 84.9% 83.1% 95.4% 72.8%\\nNormal Abnormal UCSD Ped1 UCSD Ped2 CUHK Avenue \\n ShanghaiTech \\nFigure 4. Some samples including normal and abnormal frames in\\nthe UCSD, CUHK Avenue and ShanghaiTech datasets are illus-\\ntrated. Red boxes denote anomalies in abnormal frames.\\n40 irregular events. All of these abnormal cases are\\nabout vehicles such as bicycles and cars. The UCSD\\nPedestrian 2 (Ped2) dataset contains 16 training videos\\nand 12 testing videos with 12 abnormal events. The\\ndeﬁnition of anomaly for Ped2 is the same with Ped1.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 13}),\n",
       " Document(page_content='Usually different methods are evaluated on these two\\nparts separately.\\n•The ShanghaiTech dataset is a very challenging\\nanomaly detection dataset. It contains 330 training\\nvideos and 107 testing ones with 130 abnormal events.\\nTotally, it consists of 13 scenes and various anomaly\\ntypes. Following the setting used in [22], we train the\\nmodel on all scenes.\\n4.2. Evaluation Metric\\nIn the literature of anomaly detection [20][23], a popu-\\nlar evaluation metric is to calculate the Receiver Operation\\nCharacteristic (ROC) by gradually changing the threshold\\nof regular scores. Then the Area Under Curve (AUC) is cu-\\nmulated to a scalar for performance evaluation. A higher\\nvalue indicates better anomaly detection performance. In\\nthis paper, following the work [22], we leverage frame-level\\nAUC for performance evaluation.4.3. Comparison with Existing Methods\\nIn this section, we compare our method with different\\nhand-craft features based method [16][23][34][10] and lat-', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 13}),\n",
       " Document(page_content='est deep learning based methods [13][31][14][22]. The\\nAUC of different methods is listed in Table 1. We can see\\nthat our method outperforms all existing methods (around\\n(3–5)% on all datasets), which demonstrates the effective-\\nness of our method.\\n4.4. The Design of Prediction Network\\nIn our anomaly detection framework, the future frame\\nprediction network is an important module. To evaluate\\nhow different prediction networks affect the performance\\nof anomaly detection, we compare our U-Net prediction\\nnetwork with Beyond Mean Square Error (Beyond-MSE)\\n[25] which achieves state-of-the-art performance for video\\ngeneration. Beyond-MSE leverages a multi-scale prediction\\nnetwork to gradually generate video frames with larger spa-\\ntial resolution. Because of its multi-scale strategy, it is much\\nslower than U-Net. To be consistent with Beyond-MSE, we\\nadapt our network architecture by removing the motion con-\\nstraint and only use the intensity loss, the gradient loss and', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 13}),\n",
       " Document(page_content='adversarial training in our U-Net based solution.\\nQuantitative comparison for anomaly detection. We\\nﬁrst compute the gap between average score of normal\\nframes and that of abnormal frames, denoted as ∆s. We\\ncompare the result of U-Net with that of Beyond-MSE on\\nthe Ped1 and Ped2 datasets, respectively. Larger ∆smeans\\nthe network can be more capable to distinguish normal and\\nabnormal patterns. Then, we also compare the U-Net based\\nsolution and Beyond-MSE with the AUC metric on the Ped1\\nand Ped2 datasets, respectively. We demonstrate the results\\nin Table 2. We can see that our method both achieves a\\nlarger ∆sand higher AUC than Beyond-MSE, which show\\nthat our network is more suitable for anomaly detection than\\nBeyond-MSE. Therefore, we adapt U-Net architecture as\\nour prediction network. As we aforementioned, the results\\nlisted here do not contain motion constraint, which would\\n6', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 13}),\n",
       " Document(page_content='further boost the AUC.\\nTable 2. The gap ( ∆s) and AUC of different prediction networks\\nin the Ped1 and Ped2 datasets.\\nPed1 Ped2\\n∆s AUC ∆s AUC\\nBeyond-MSE 0.200 75.8% 0.396 88.5%\\nU-Net 0.243 81.8% 0.435 93.5%\\n\\x7f !\" P P P P\\n\\x7f#$ — P P P\\n\\x7f%$& — — P P\\n\\x7f\\'( — — — P\\n)*+ 82.0% 82.6% 83.7% 84.9% 0.745 \\n0.718 \\n0.757 \\n0.844 0.487 \\n0.456 \\n0.487 \\n0.569 0.258 \\n0.262 \\n0.270\\n0.275 \\n00.5 SCORE normal abnormal gap \\nFigure 5. The evaluation of different components in our future\\nframe prediction network in the Avenue dataset. Each column in\\nthe histogram corresponds to a method with different loss func-\\ntions. We calculate the average scores of normal and abnormal\\nevents in the testing set. The gap is calculated by subtracting the\\nabnormal score from the normal one.\\n0.243 0.384 \\n0.256 0.259 0.469 \\n0.275 \\n0.2 0.25 0.3 0.35 0.4 0.45 0.5 \\nPed1 Ped2 CUHK Avenue Score Gap \\nConv-AE Our method \\nFigure 6. We ﬁrstly compute the average score for normal frames', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 14}),\n",
       " Document(page_content='and that for abnormal frames in the testing set of the Ped1, Ped2\\nand Avenue datasets. Then, we calculate the difference of these\\ntwo scores( ∆s) to measure the ability of our method and Conv-AE\\nto discriminate normal and abnormal frames. A larger gap( ∆s)\\ncorresponds to small false alarm rate and higher detection rate.\\nThe results show that our method consistently outperforms Conv-\\nAE in term of the score gap between normal and abnormal events.Table 3. AUC for anomaly detection of networks with/wo the mo-\\ntion constraint in Ped1 and Ped2.\\nPed1 Ped2\\nwithout motion constraint 81.8% 93.5%\\nwith motion constraint 83.1% 95.4%\\n4.5. Impact of Constraint on Motion.\\nTo evaluate the importance of motion constraint for\\nvideo frame generation as well as anomaly detection, we\\nconduct the experiment by removing the constraint from the\\nobjective in the training. Then we compare such a baseline\\nwith our method.\\nEvaluation of motion constraint with optical ﬂow', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 14}),\n",
       " Document(page_content='maps. We show the optical ﬂow maps generated\\nwith/without motion constraint in Fig. 7, we can see that\\nthe optical ﬂow generated with motion constraint is more\\nconsistent with ground truth, which shows that such motion\\nconstraint term helps our prediction network to capture mo-\\ntion information more precisely. We also compare the MSE\\nbetween optical ﬂow maps generated with/without motion\\nconstraint and the ground truth, which is 7.51 and 8.26, re-\\nspectively. This further shows the effectiveness of motion\\nconstraint.\\nQuantitatively evaluation of motion with anomaly de-\\ntection. The result in Table 3 shows that the model trained\\nwith motion constraint consistently achieves higher AUC\\nthan that without the constraint on Ped1 and Ped2 dataset.\\nThis also proves that it is necessary to explicitly impose\\nthe motion consistency constraint into the objective for\\nanomaly detection.\\n4.6. Impact of Different Losses for Anomaly Detec-\\ntion.\\nWe also analyze the impact of different loss functions', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 14}),\n",
       " Document(page_content='for anomaly detection by ablating different terms gradu-\\nally. We combine different losses to conduct experiments\\non the Avenue dataset. To evaluate how different losses af-\\nfect the performance of anomaly detection, we also utilize\\nthe score gap( ∆s) mentioned above. The larger gap repre-\\nsents the more discriminations between normal and abnor-\\nmal frames. The results in Figure 5 show more constraints\\nusually achieve a higher gap as well as AUC value, and our\\nmethod achieves the highest value under all settings.\\n4.7. Comparison of Prediction Network and Auto-\\nEncoder Networks for Anomaly Detection\\nWe also compare the video prediction network based and\\nAuto-Encoder network based anomaly detection. Here for\\nAuto-Encoder network based anomaly detection, we use\\nthe Conv-AE [13] which is the latest work and achieves\\nstate-of-the-art performance for anomaly detection. Be-\\ncause of the capacity of deep neural network, Auto-Encoder\\n7', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 14}),\n",
       " Document(page_content='Optical flow \\nwith motion constraint \\nPredicted image \\nwith motion constraint Optical flow \\nground truth Optical flow\\nwithout motion constraint \\nPredicted image \\nwithout motion constraintFigure 7. The visualization of optical ﬂow and the predicted images on the Ped1 dataset. The red boxes represent the difference of optical\\nﬂow predicted by the model with/without motion constraint. We can see that the optical ﬂow predicted by the model with motion constraint\\nis closer to ground truth. Best viewed in color.\\n020 40 60 \\n19\\n17 25 33 41 49 57 65 73 81 89 97 \\n105 113 121 129 137 145 153 161 169 177 PSNR #Frame crossroad Vehicle Intrude \\ncrossroad \\n\\x7f !\\x7f\"\"# \\x7f$% \\n\\x7f&& \\n\\x7f\"$$ \\x7f&% \\n\\x7f\"\\'! \\n\\x7f\"($ prediction ground truth Uncertainty in A Normal Event & Vehicle Intruding \\n020 40 60 \\n1\\n15 29 43 57 71 85 99 \\n113 127 141 155 169 183 197 211 225 239 253 267 281 \\n295 \\n309 323 337 351 PSNR \\n#Frame Jump fight crossroad normal abnormal normal abnormal \\nprediction ground truth Fighting', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 15}),\n",
       " Document(page_content='Figure 8. The visualization of predicted testing frames in our toy pedestrian dataset. There are two abnormal cases including vehicle\\nintruding(left column) and humans ﬁghting(right column). The orange circles correspond to normal events with uncertainty in prediction\\nwhile the red ones correspond to abnormal events. It is noticeable that the predicted truck is blurred, because no vehicles appear in the\\ntraining set. Further, in the ﬁghting case, two persons cannot be predicted well because ﬁghting motion never appear in the training phase.\\nbased methods may well reconstruct normal and abnormal\\nframes in the testing phase. To evaluate the performance of\\nprediction network and Auto-Encoder one, we also utilize\\nthe aforementioned gap( ∆s) between normal and abnormal\\nscores. The result in Fig. 6 shows that our solution always\\nachieves higher gaps than Conv-AE, which validates the ef-\\nfectiveness of video prediction for anomaly detection.\\n4.8. Evaluation with A Toy Dataset', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 15}),\n",
       " Document(page_content='We also design a toy pedestrian dataset for performance\\nevaluation. In the training set, only a pedestrian walks on\\nthe road and he/she can choose different directions when\\nhe/she comes to a crossroad. In the testing set, there are\\nsome abnormal cases such as vehicles intruding, humans\\nﬁghting, etc.. We have uploaded our toy dataset in the sup-\\nplementary material. Totally, the training data contains 210frames and testing data contains 1242 frames.\\nIt is interesting that the motion direction is sometimes\\nalso uncertain for normal events, for example, a pedestrian\\nstands at the crossroad. Even though we cannot predict the\\nmotion well, we only cannot predict the next frame at a mo-\\nment which leads a slightly instant drop in terms of PSNR.\\nAfter observing the pedestrian for a while when the pedes-\\ntrian has made his or her choice, it becomes predictable and\\nPSNR would go up, shown in Fig. 8. Therefore the un-\\ncertainty of normal events does not affect our solution too', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 15}),\n",
       " Document(page_content='much. However, for the real abnormal events, for exam-\\nple, a truck breaks into the scene and hits the pedestrian, it\\nwould leads to a continuous lower PSNR, which facilitates\\nthe anomaly prediction. Totally, the AUC is 98.9%.\\n8', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 15}),\n",
       " Document(page_content='4.9. Running Time\\nOur framework is implemented with NVIDIA GeForce\\nTITAN GPUs and Tensorﬂow [1]. The average running\\ntime is about 25 fps, which contains both the video frame\\ngeneration and anomaly prediction. We also report the run-\\nning time of other methods such as 20 fps in [31], 150 fps\\n[20] and 0.5 fps in [38].\\n5. Conclusion\\nSince normal events are predictable while abnormal\\nevents do not conform to the expectation, therefore we pro-\\npose a future frame prediction network for anomaly detec-\\ntion. Speciﬁcally, we use a U-Net as our basic prediction\\nnetwork. To generate a more realistic future frame, other\\nthan adversarial training and constraints in appearance, we\\nalso impose a loss in temporal space to ensure the optical\\nﬂow of predicted frames to be consistent with ground truth.\\nIn this way, we can guarantee to generate the normal events\\nin terms of both appearance and motion, and the events\\nwith larger difference between prediction and ground truth', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 16}),\n",
       " Document(page_content='would be classiﬁed as anomalies. Extensive experiments on\\nthree datasets show our method outperforms existing meth-\\nods by a large margin, which proves the effectiveness of our\\nmethod for anomaly detection.\\nReferences\\n[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,\\nC. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al.\\nTensorﬂow: Large-scale machine learning on heterogeneous\\ndistributed systems. arXiv preprint arXiv:1603.04467 , 2016.\\n[2] A. Adam, E. Rivlin, I. Shimshoni, and D. Reinitz. Ro-\\nbust real-time unusual event detection using multiple ﬁxed-\\nlocation monitors. TPAMI , 30(3):555–560, 2008.\\n[3] V . Chandola, A. Banerjee, and V . Kumar. Anomaly detec-\\ntion: A survey. ACM computing surveys (CSUR) , 41(3):15,\\n2009.\\n[4] B. Chen, W. Wang, J. Wang, X. Chen, and W. Li. Video\\nimagination from a single image with transformation gener-\\nation. arXiv preprint arXiv:1706.04124 , 2017.\\n[5] Y . S. Chong and Y . H. Tay. Abnormal event detection in', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 16}),\n",
       " Document(page_content='videos using spatiotemporal autoencoder. arXiv preprint\\narXiv:1701.01546 , 2017.\\n[6] Y . Cong, J. Yuan, and J. Liu. Sparse reconstruction cost\\nfor abnormal event detection. In CVPR , pages 3449–3456.\\nIEEE, 2011.\\n[7] N. Dalal, B. Triggs, and C. Schmid. Human detection using\\noriented histograms of ﬂow and appearance. In ECCV , pages\\n428–441. Springer, 2006.\\n[8] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,\\nV . Golkov, P. van der Smagt, D. Cremers, and T. Brox.\\nFlownet: Learning optical ﬂow with convolutional networks.\\nInICCV , pages 2758–2766, 2015.\\n[9] J. Gauthier. Conditional generative adversarial nets for\\nconvolutional face generation. Class Project for StanfordCS231N: Convolutional Neural Networks for Visual Recog-\\nnition, Winter semester , 2014(5):2, 2014.\\n[10] A. D. Giorno, J. A. Bagnell, and M. Hebert. A discriminative\\nframework for anomaly detection in large videos. In ECCV ,\\npages 334–349. Springer, 2016.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 16}),\n",
       " Document(page_content='[11] R. Girshick. Fast r-cnn. In ICCV , pages 1440–1448, 2015.\\n[12] I. J. Goodfellow. Generative adversarial networks. CoRR ,\\n2014.\\n[13] M. Hasan, J. Choi, J. Neumann, A. K. Roy-Chowdhury,\\nand L. S. Davis. Learning temporal regularity in video se-\\nquences. In CVPR , 2016.\\n[14] R. Hinami, T. Mei, and S. Satoh. Joint detection and recount-\\ning of abnormal events by learning deep generic knowledge.\\nInICCV , Oct 2017.\\n[15] P. Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros. Image-to-image\\ntranslation with conditional adversarial networks. In CVPR ,\\nJuly 2017.\\n[16] J. Kim and K. Grauman. Observe locally, infer globally: a\\nspace-time mrf for detecting abnormal activities with incre-\\nmental updates. In CVPR , pages 2921–2928. IEEE, 2009.\\n[17] D. Kingma and J. Ba. Adam: A method for stochastic opti-\\nmization. arXiv preprint arXiv:1412.6980 , 2014.\\n[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\\nclassiﬁcation with deep convolutional neural networks. In\\nNIPS , pages 1097–1105, 2012.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 16}),\n",
       " Document(page_content='[19] W. Lotter, G. Kreiman, and D. Cox. Deep predictive cod-\\ning networks for video prediction and unsupervised learning.\\narXiv preprint arXiv:1605.08104 , 2016.\\n[20] C. Lu, J. Shi, and J. Jia. Abnormal event detection at 150 fps\\nin matlab. In ICCV , pages 2720–2727, 2013.\\n[21] W. Luo, W. Liu, and S. Gao. Remembering history with\\nconvolutional lstm for anomaly detection. In Multimedia\\nand Expo (ICME), 2017 IEEE International Conference on ,\\npages 439–444. IEEE, 2017.\\n[22] W. Luo, W. Liu, and S. Gao. A revisit of sparse coding based\\nanomaly detection in stacked rnn framework. In ICCV , Oct\\n2017.\\n[23] V . Mahadevan, W. Li, V . Bhalodia, and N. Vasconcelos.\\nAnomaly detection in crowded scenes. In CVPR , volume\\n249, page 250, 2010.\\n[24] X. Mao, Q. Li, H. Xie, R. Y . Lau, Z. Wang, and S. P. Smol-\\nley. Least squares generative adversarial networks. arXiv\\npreprint ArXiv:1611.04076 , 2016.\\n[25] M. Mathieu, C. Couprie, and Y . LeCun. Deep multi-scale', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 16}),\n",
       " Document(page_content='video prediction beyond mean square error. arXiv preprint\\narXiv:1511.05440 , 2015.\\n[26] J. R. Medel and A. Savakis. Anomaly detection in video\\nusing predictive convolutional long short-term memory net-\\nworks. arXiv preprint arXiv:1612.00390 , 2016.\\n[27] N. Navneet and B. Triggs. Histograms of oriented gradients\\nfor human detection. In CVPR 2005. IEEE Computer Society\\nConference on , volume 1, pages 886–893. IEEE, 2005.\\n[28] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu-\\ntional networks for biomedical image segmentation. In In-\\nternational Conference on Medical Image Computing and\\nComputer-Assisted Intervention , pages 234–241. Springer,\\n2015.\\n9', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 16}),\n",
       " Document(page_content='[29] X. Shi, Z. Chen, H. W, D. Yeung, D. Wong, and W. Woo.\\nConvolutional lstm network: A machine learning approach\\nfor precipitation nowcasting. In NIPS , pages 802–810, 2015.\\n[30] K. Simonyan and A. Zisserman. Two-stream convolutional\\nnetworks for action recognition in videos. In NIPS , pages\\n568–576, 2014.\\n[31] R. Tudor Ionescu, S. Smeureanu, B. Alexe, and M. Popescu.\\nUnmasking the abnormal events in video. In ICCV , Oct\\n2017.\\n[32] F. Tung, J. S. Zelek, and D. A. Clausi. Goal-based trajec-\\ntory analysis for unusual behaviour detection in intelligent\\nsurveillance. Image and Vision Computing , 29(4):230–240,\\n2011.\\n[33] J. van Amersfoort, A. Kannan, M. Ranzato, A. Szlam,\\nD. Tran, and S. Chintala. Transformation-based models of\\nvideo sequences. arXiv preprint arXiv:1701.08435 , 2017.\\n[34] T. Wang and H. Snoussi. Histograms of optical ﬂow orienta-\\ntion for abnormal events detection. In Performance Evalua-\\ntion of Tracking and Surveillance (PETS), 2013 IEEE Inter-', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 17}),\n",
       " Document(page_content='national Workshop on , pages 45–52. IEEE, 2013.\\n[35] S. Wu, B. E. Moore, and M. Shah. Chaotic invariants\\nof lagrangian particle trajectories for anomaly detection in\\ncrowded scenes. In CVPR , pages 2054–2060. IEEE, 2010.\\n[36] D. Xu, E. Ricci, Y . Yan, J. Song, and N. Sebe. Learning deep\\nrepresentations of appearance and motion for anomalous\\nevent detection. arXiv preprint arXiv:1510.01553 , 2015.\\n[37] D. Zhang, D. Gatica-Perez, S. Bengio, and I. McCowan.\\nSemi-supervised adapted hmms for unusual event detection.\\nInCVPR , volume 1, pages 611–618. IEEE, 2005.\\n[38] B. Zhao, F. Li, and E. P. Xing. Online detection of unusual\\nevents in videos via dynamic sparse coding. In CVPR , pages\\n3313–3320. IEEE, 2011.\\n10', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 17}),\n",
       " Document(page_content='VPTR: Efﬁcient Transformers for Video Prediction\\nXi Ye\\nLITIV Laboratory, Polytechnique Montr ´eal\\nMontr ´eal, Canada\\nEmail: xi.ye@polymtl.caGuillaume-Alexandre Bilodeau\\nLITIV Laboratory, Polytechnique Montr ´eal\\nMontr ´eal, Canada\\nEmail: gabilodeau@polymtl.ca\\nAbstract —In this paper, we propose a new Transformer block\\nfor video future frames prediction based on an efﬁcient local\\nspatial-temporal separation attention mechanism. Based on this\\nnew Transformer block, a fully autoregressive video future\\nframes prediction Transformer is proposed. In addition, a non-\\nautoregressive video prediction Transformer is also proposed\\nto increase the inference speed and reduce the accumulated\\ninference errors of its autoregressive counterpart. In order to\\navoid the prediction of very similar future frames, a contrastive\\nfeature loss is applied to maximize the mutual information\\nbetween predicted and ground-truth future frame features. This', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 18}),\n",
       " Document(page_content='work is the ﬁrst that makes a formal comparison of the two types\\nof attention-based video future frames prediction models over\\ndifferent scenarios. The proposed models reach a performance\\ncompetitive with more complex state-of-the-art models. The\\nsource code is available at https://github.com/XiYe20/VPTR .\\nI. I NTRODUCTION\\nVideo future frames prediction (VFFP) is applied to many\\nresearch areas, for instance, intelligent agents [1], [2], au-\\ntonomous vehicles [3], model-based reinforcement learning\\n[4]. More recently, it has drawn a lot of attention since it\\nis naturally a good self-supervised learning task [5], [6].\\nIn this paper, we focus on the most common video pre-\\ndiction task, i.e. predicting Nfuture frames given Lpast\\nframes, with LandNgreater than 1. For training a deep\\nlearning VFFP model, we can formalize the task to be\\narg maxθp(ˆxL+N,...,ˆxL+1|xL,...,x 1;θ), where ˆxtandxt\\ndenote the predicted future frames and input past frames\\nrespectively, θdenotes the model parameters.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 18}),\n",
       " Document(page_content='Even though many deep learning-based VFFP models have\\nbeen proposed, some challenges still remain to be solved. Al-\\nmost all the state-of-the-art (SOTA) VFFP models are based on\\nConvLSTMs, i.e. convolutional short-term memory networks,\\nwhich are efﬁcient and powerful. Nevertheless, they suffer\\nfrom some inherent problems of recurrent neural networks\\n(RNNs), such as slow training and inference speed, error\\naccumulation during inference, gradient vanishing, and pre-\\ndicted frames quality degradation. Researchers keep improving\\nthe performance by developing more and more sophisticated\\nConvLSTM-based models. For instance, by integrating custom\\nmotion-aware units into ConvLSTM [7], or building complex\\nmemory modules to store the motion context [8].\\nInspired by the great success of Transformers in NLP, more\\nand more researchers are starting to adapt Transformers for\\nvarious computer vision tasks [9], [10], [11], [12], including', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 18}),\n",
       " Document(page_content='few recent works for VFFP [13], [14], [15]. However, itis computational expensive to apply Transformer to high\\ndimensional visual features. We still need further research\\nabout more efﬁcient visual Transformers, especially for videos.\\nTherefore, we propose a novel efﬁcient Transformer block with\\nsmaller complexity, and we developed a new video prediction\\nTransformer (VPTR) based on it.\\nAmong the Transformers-based VFFP models [13], [14],\\n[15] that we mentioned earlier, some of them are autoregres-\\nsive models while some others are non-autoregressive models,\\nand they are based on different attention mechanisms, e.g.\\na custom convolution multi-head attention (MHA) [13] and\\nstandard dot-product MHA [14], [15]. There is no formal\\ncomparison of the two typical approaches (autoregressive vs\\nnon-autoregressive) to use Transformer-based VFFP models so\\nfar. Thus, we developed an fully autoregressive VPTR (VPTR-\\nFAR) and a non-autoregressive VPTR (VPTR-NAR). The two', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 18}),\n",
       " Document(page_content='VPTR variants share the same attention mechanism and same\\nnumber of Transformer block layers, which guarantees a fair\\ncomparison between the two approaches.\\nOur main contributions are summarized as:\\n1) We proposed a new efﬁcient Transformer block for\\nspatio-temporal feature learning by combining spatial lo-\\ncal attention and temporal attention in two steps. The new\\nTransformer block successfully reduces the complexity of\\na standard Transformer block with respect to same input\\nspatio-temporal feature size, speciﬁcally, from O((THW )2)\\ntoO(H2W2\\nP2+T2).\\n2) Two VPTR models, VPTR-NAR and VPTR-FAR, were\\ndeveloped. We show that the proposed simple attention-based\\nVPTRs are capable of reaching and outperforming more\\ncomplex SOTA ConvLSTM-based VFFP models.\\n3) A formal comparison of two VPTR variants was con-\\nducted. The results show that VPTR-NAR has a faster infer-\\nence speed and smaller accumulation of errors during infer-\\nence, but it is more difﬁcult to train. We solved the training', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 18}),\n",
       " Document(page_content='problem of VPTR-NAR by employing a contrastive feature\\nloss which maximizes the mutual information of predicted and\\nground-truth future frame features.\\n4) We found that given the same number of Transformer\\nblock layers, VPTR-FAR has a worse generalization perfor-\\nmance due to the accumulated inference errors, which are\\nintroduced by the discrepancy between train and test behav-\\niors. We also found that recurrent inference over pixel space\\nintroduces less accumulation errors than recurrent inference\\nover latent space in the case of VPTR-FAR.arXiv:2203.15836v1  [cs.CV]  29 Mar 2022', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 18}),\n",
       " Document(page_content='II. R ELATED WORK\\nAlmost all the SOTA deep learning-based VFFP models are\\nConvLSTM-based autoencoders, where the encoder extracts\\nthe representations of past frames, and then the decoder\\ngenerates future frame pixels based on those representations\\n[16], [17], [18], [7], [8]. In general, the SOTA models rely on\\ncomplex ConvLSTM models that integrates attention mech-\\nanism or memory augmented modules. For example, LMC-\\nMemory model [8] stores the long-term motion context by a\\nnovel memory alignment learning, and the motion information\\nis recalled during test to facilitate the long-term prediction.\\nZhang et al. [7] proposed a attention-based motion-aware unit\\nto increase the temporal receptive ﬁeld of RNNs.\\nThe ConvLSTM-based models are ﬂexible and efﬁcient, but\\nrecurrent prediction is slow. Therefore, standard CNNs or 3D-\\nCNNs are also used as the backbones of VFFP to generate\\nmultiple future frames simultaneously [19], [20], [21], [22].', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 19}),\n",
       " Document(page_content='Besides, the future prediction is by nature multimodal [23],\\ni.e. stochastic. Some VFFP models aim to solve this problem\\nbased on V AEs, such as SV2P [24], SVG-LP [25], improved\\nconditional VRNNs [26]. Stochasticity learning is challenging\\nand thus most VFFP models ignore it. A detail survey of VFFP\\nmodels can be found in [23].\\nRecently, Transformers were applied for VFFP. The Con-\\nvTransformer [13] model follows the architecture of DETR\\n[9]. DETR follows a classical neural machine translation\\n(NMT) Transformer architecture. It also inspired the develop-\\nment of our VPTR-NAR. Despite the similarities, our VPTR-\\nNAR is different from ConvTransformer with respect to the\\nfundamental attention mechanism. Speciﬁcally, ConvTrans-\\nformer proposed a custom hybrid multi-head attention module\\nbased on convolution, but our VPTR-NAR uses the standard\\nmulti-head dot-product attention. Another more recent model\\n(VideoGPT) [14] takes a 3D-CNN as backbone to encode', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 19}),\n",
       " Document(page_content='video clips into spatial-temporal features, which are then\\nﬂattened to be a sequence to train a standard Transformer\\nwith the GPT manner [27], [28]. VideoGPT shares a similar\\narchitecture and train/test behaviours as our VPTR-FAR. But\\nVideoGPT performs the attention along the spatial and tem-\\nporal dimensions jointly while our VPTR-FAR performs the\\nattention along the spatial and temporal dimensions separately.\\nMore importantly, VideoGPT downsamples the time dimen-\\nsion of input videos by 3D-CNN and thus helps the temporal\\ninformation modeling. In contrast, our VPTR models solely\\ndepend on attention for a full temporal information modeling,\\nwithout downsampling. Another recent work N ¨UWA [15]\\nshares a similar idea to VideoGPT.\\nEfﬁcient visual Transformers. To reduce the computation\\ncost for visual Transformers, some models reduce the ﬂattened\\nsequence length by different methods. ViT and the succes-\\nsive works [10], [29], [12] divided input features into local', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 19}),\n",
       " Document(page_content='patches, either 2D or 3D, and then tokenize the local patch by\\nconcatenation or Pooling [30]. Some other models introduce\\nsparse attention to reduce the complexity, e.g. restricting the\\nattention over a local region [31], [32], [33], or decomposingthe global attention into a series of axial-attention [34], [35],\\n[12]. HRFormer [33] is an example of local region attention-\\nbased Transformers, which is designed for image classiﬁcation\\nand dense prediction.\\nSpeciﬁcally, a HRFormer block is composed of a local-\\nwindow multi-head self attention layer and a depth-wise con-\\nvolution feed-forward network. The input feature maps Z∈\\nRH×W×Care ﬁrstly evenly divided into Pnon-overlapping\\nlocal patches, each patch is Zp∈RH\\nP×W\\nP×C. Then a multi-\\nhead self attention is performed for each patch. Finally, the\\ndepth-wise convolution is used to exchange information among\\ndifferent local patches.\\nIII. T HE PROPOSED VPTR MODELS\\nA. Overall framework of VPTR', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 19}),\n",
       " Document(page_content='Our overall VPTR framework is illustrated in Fig. 1. A\\nCNN encoder shared by all the past frames extracts the visual\\nfeatures of each frame. Then a VPTR is taken to predict\\nthe visual features of each future frame based on the past\\nframe features. The detail architectures of two different VPTR\\nvariants are described in the following subsections. In order to\\nmake the model architecture simple and easier to train, there\\nis no skip connections between encoder and decoder.\\nB. Encoder and decoder\\nWe adapted the ResNet-based autoencoder from the Pix2Pix\\nmodel [36]. The output feature channels of the encoder and\\ninput feature channels of the decoder are modiﬁed to be of\\nsizedmodel to match with the VPTR input and output size.\\nThe loss function to train the encoder and decoder is deﬁned\\nas follows,\\nLrec=L2(X,ˆX) +Lgdl(X,ˆX)\\n+λ1arg min\\nGmax\\nDLGAN(G,D ),(1)\\nwhereL2denotes the MSE loss (Eq. 2) and Lgdldenotes\\nimage gradient difference loss [19] (Eq. 3), Xand ˆXdenote', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 19}),\n",
       " Document(page_content='the original frames and reconstructed frames respectively, xi\\ndenotes a single frame, λ1andαare hyperparameters. LGAN\\ndenotes the GAN loss (Eq. 4), where Ddenotes a discrimina-\\ntor, which is not shown in Fig. 1, and the combination of the\\nencoder and decoder is considered to be a generator G. We\\ntrainLGAN with the PatchGAN [36] manner.\\nL2(X,ˆX) =n∑\\ni=1∥xi−ˆxi∥2\\n2 (2)\\nLgdl(X,ˆX) =n∑\\ni=1∑\\ni,j⏐⏐|xi,j−xi−1,j|−|ˆxi,j−ˆxi−1,j|⏐⏐α\\n+⏐⏐|xi,j−1−xi,j|−|ˆxi,j−1−ˆxi,j|⏐⏐α(3)\\nLGAN(G,D ) =EX[logD (X)]+EˆX[log(1−D(G(X))](4)', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 19}),\n",
       " Document(page_content='EncoderDecoderVPTRPast framesFuture framesFig. 1: Overall framework of VPTR. Green squares and blue\\nsquares denote the past frame features and future frames\\nfeatures respectively.\\nC. VidHRFormer Block\\nWe proposed a new Transformer block based on the\\nHRFormer block [33] for video processing, which is\\nnamed VidHRFormer block. The detail architecture of a\\nVidHRFormer block is shown in the gray area of Fig. 2(a).\\nEssentially, we integrate a temporal multi-head attention layer,\\ntogether with some other necessary feed-forward and normal-\\nization layers, into the HRFormer block.\\nLocal spatial multi-head self-attention (MHSA). Given\\na spatiotemporal feature map Z∈RN×T×H×W×dmodel ,\\nwe ﬁrstly reshape and evenly divide it into Plocal\\npatches{Z1,Z2,...,ZP}along theHandWdimensions,\\nwhereZp∈R(NT)×K2×dmodel , and each local patch\\nis of size K×K, withP=HW\\nK2patches in total.\\nMHSA (Zp) =Concat [head (Zp)1,...,head (Zp)h], where\\nhead (Zp)i∈RK2×dmodel\\nh is formulated as\\nhead (Zp)i=softmax [((ZQ\\npWQ\\ni)(ZK', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 20}),\n",
       " Document(page_content='pWQ\\ni)(ZK\\npWK\\ni)√\\ndmodel/h]ZpWV\\ni,(5)\\nwhereWQ\\ni,WK\\ni,WV\\niare linear projection matrices for the\\nquery, key and value of each head irespectively, ZQ\\npand\\nZK\\npdenote the key and query for attention. We may use\\na ﬁxed absolute 2D positional encoding [37], or a relative\\npositional encoding (RPE) [38] of the local patch to get ZQ\\np\\nandZK\\np. We compared the two different positional encodings\\nin the experiments. The complexity of local spatial MHSA is\\nO(H2W2\\nP2).\\nConvolutional feed-forward neural network (Conv FFN).\\nAfter the local spatial MHSA, {Z1,Z2,...,ZP}are assembled\\nback to beZ∈R(NT)×H×W×dmodel . The Conv FFN layer is\\ncomposed of a 3×3depth-wise convolution and two point-\\nwise MLPs. Note that all the normalization layers in Conv\\nFFN are layer normalization, instead of batch normalization\\nused in the original HRFormer block.\\nTemporal MHSA. The local spatial MHSA and Conv-\\nFFN are shared by every frame feature. A temporal MHSA\\nis placed on top of them to model the temporal dependency', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 20}),\n",
       " Document(page_content='between frames. We reshape the input feature map Z∈\\nR(NT)×H×W×dmodel to beZ∈R(NHW )×T×dmodel . Temporal\\nMHSA is a standard multi-head self-attention similar to the\\nlocal spatial MHSA, except that there is no local patch division\\nand it takes a ﬁxed absolute 1D positional encoding of time.\\nThe complexity of temporal MHSA is O(T2). The temporal\\nMHSA is followed by a MLP feed-forward neural networkas in a standard Transformer, and the output feature map is\\nreshaped back to be Z∈RN×T×H×W×dmodel for the next\\nlayer of VidHRFormer block.\\nIn summary, the proposed VidHRFormer block reduces the\\ncompute complexity from O((THW )2)to beO(H2W2\\nP2+T2)\\nby combining spatial local window attention and temporal\\nattention in two steps. Based on the VidHRFormer, we develop\\ntwo different VPTR models.\\nD. VPTR-FAR\\nThe fully autoregressive VPTR model is simply a stack of\\nmultiple VidHRFormer blocks. The architecture is shown in\\nFig. 2(a). Theoretically, given a well-trained CNN encoder', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 20}),\n",
       " Document(page_content='and decoder, the VPTR-FAR parameterizes the following\\ndistribution:\\np(x1,...,xL,...,xL+N) =L+N∏\\nt=1p(xt|xt−1,...x 1) (6)\\nIn other words, VPTR-FAR predicts the next frame condi-\\ntioned on all previous frames, which is also the most common\\nparadigm for most SOTA VFFP models. An attention mask\\nis applied to the temporal MHSA module to impose the\\nconditional dependency between the next frame and previous\\nframes.\\nDuring training, we feed the ground-truth frames\\n{x1,...,xL+N−1}into the encoder, which generates the\\nfeature sequence{z1,...,zL+N−1}. VPTR-FAR then predicts\\nthe future feature sequence {ˆz2,...,ˆzL+N}, which is then\\ndecoded by the decoder to generate frames {ˆx2,...,ˆxL+N}.\\nThe training loss of VPTR-FAR is:\\nLFAR =L+N∑\\nt=2L2(xt,ˆxt) +L+N∑\\nt=2Lgdl(xt,ˆxt) (7)\\nDuring test, we ﬁrstly get the ground-truth past frames\\nfeatures{z1,...,zL}. Then there are two different ways of\\nrecurrently predicting the future frames. The ﬁrst method', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 20}),\n",
       " Document(page_content='is recurrently generating all the future frame features only\\nby the VPTR-FAR module, i.e. ˆzt=T(z1,...,zt−1),t∈\\n[L+ 1,...L +N], whereTdenotes the VPTR-FAR module.\\nThen we get ˆxt=Dec(ˆzt),t∈[L+ 1,...L +N], where\\nDec denotes the CNN frame decoder. The second prediction\\nmethod introduces two additional steps. Particularly, ˆzt=\\nEnc(Dec(T(z1,...,zt−1))),t∈[L+ 1,...L +N], whereEnc\\ndenotes the CNN frame encoder. In short, we decode each\\nfuture feature to be frame ˆxt, and then encode the frame back\\ninto a latent feature before the prediction of next future frame\\nfeature. The second way signiﬁcantly reduces the accumulated\\nerror during inference, and the reasons are analyzed in the\\nexperiments section.\\nE. VPTR-NAR\\nInspired by the achitecture of DETR [37], a non-\\nautoregressive variant is proposed to increase the inference\\nspeed and reduce inference accumulation error of autore-\\ngressive models. VPTR-NAR is illustrated in Fig. 2(b). It', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 20}),\n",
       " Document(page_content='Layer NormLocal spatial MHSAConv FFNTemporal MHSALayer NormLayer NormLayer NormMLP FFNN×1D PE2D PE……\\n𝑧\"𝑧#𝑧$%&\\'#𝑧$%&\\'\"̂𝑧#̂𝑧(̂𝑧$%&\\'\"̂𝑧$%&(a)\\nLayer NormLocal spatial MHSAConv FFNTemporal MHSALayer NormLayer NormLayer NormMLP FFNN×1D PE2D PE……\\n𝑧\"𝑧#𝑧$%\"𝑧$𝑒\"𝑒#𝑒$%\"𝑒$\\nLayer NormLocal spatial MHSAConv FFNTemporal MHSALayer NormLayer NormLayer NormMLP FFN1D PE2D PETemporal MHALayer Norm𝑄𝐾𝑉Layer NormConv FFN…̂𝑧$)\"̂𝑧$)#̂𝑧$)*%\"̂𝑧$)*1D PE\\nN×…𝑞$)\"𝑞$)#𝑞$)*%\"𝑞$)*++𝑉𝐾𝑄+TDTE (b)\\nFig. 2: (a) VPTR-FAR. The gray area indicates the proposed basic VidHRFormer block. A temporal attention mask is applied\\nto the Temporal MHSA module for VPTR-FAR. (b)VPTR-NAR. The left part is the Transformer encoder and right part is the\\nnon-autoregressive Transformer decoder.\\nconsists of a Transformer encoder and decoder, where the\\nencoderTEencodes all past frame features zt,t∈[1,L]to\\nbeet,t∈[1,L], which are normally named as ”memories” in\\nNLP. The architecture of TE, left part of Fig. 2(b), is the same', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 21}),\n",
       " Document(page_content='as the VPTR-FAR, except that there is no temporal attention\\nmask for the temporal MHSA module.\\nThe decoderTDof VPTR-NAR, right part of Fig. 2(b),\\nincludes two more layers compared with TE. A temporal\\nmulti-head attention (MHA) layer and another output Conv\\nFFN layer. The Temporal MHA layer is also called the\\nencoder-decoder attention layer, which takes the memories as\\nvalue and key, while the query is derived from the future frame\\nquery sequence{qL+1,...,qL+N}, whereqt∈RH×W×C,t∈\\n[L+ 1,L+N].{qL+1,...,qL+N}is randomly initialized\\nand updated during training. Note that there is no temporal\\nattention mask for Temporal MHSA layer, since we do not\\nneed to impose conditional dependency between each future\\nframe query. Theoretically, VPTR-NAR directly models the\\nfollowing conditional distribution:\\np(xL+N,...,xL+1|xL,...,x 1) (8)\\nContrastive feature loss for VPTR-NAR. We failed to\\ntrain VPTR-NAR with a loss only composed by MSE and\\nGDL, i.e.,L=∑L+N\\nt=L+1L2(xt,ˆxt) +Lgdl(xt,ˆxt), since it', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 21}),\n",
       " Document(page_content='is easy to fall into some local minimums. Speciﬁcally, all\\nthe predicted future frames are somewhat similar to each\\nother. A similar phenomenon is also observed in the non-\\nautoregressive NMT models, where the Transformer decoder\\nfrequently generate repeated tokens [39]. To solve this prob-\\nlem, we impose another contrastive feature loss Lc[40] to\\nmaximize the mutual information between predicted futureframe feature ˆztand the future frame feature zt(ground-truth)\\ngenerated by the CNN encoder, where t∈[L+ 1,L+N].Lc\\nis formulated as follows,\\nLc(zt,ˆzt) =1\\n2Sl∑\\ns=1lc(ˆvs,vs,sg(¯vs)) +lc(vs,ˆvs,sg(ˆ¯vs)),(9)\\nwherevs∈Rdmodel denotes a feature vector at spatial location\\nsofzt,¯vs∈R(Sl−1)×dmodel denotes the collection of feature\\nvectors at all other spatial locations of zt.Sl=H×Wis\\nthe total number of spatial locations in a feature map. ˆvsand\\nˆ¯vsofˆztare deﬁned in the same way. sgis the stop gradient\\noperation and lcis the info-NCE based contrastive loss deﬁned\\nby\\nlc(v,v+,v−) =', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 21}),\n",
       " Document(page_content='by\\nlc(v,v+,v−) =\\n−logexp(s(v,v+))\\nexp(s(v,v+)) +∑M\\nm=1exp(s(v,v−)).(10)\\nGiven a feature vector v∈Rdmodel ,v+∈Rdmodel is\\nthe spatially-corresponding ground-truth feature vector, and\\nv−∈RM×dmodel denotes the Mother spatially different\\nground-truth feature vectors. s(v1,v2)measures the feature\\ndot-product similarity. Finally, the training loss function for\\nVPTR-NAR is deﬁned as\\nLNAR =L+N∑\\nt=L+1L2(xt,ˆxt) +Lgdl(xt,ˆxt) +λ2Lc(zt,ˆzt).\\n(11)\\nDuring test, VPTR-NAR predicts Nfuture frames simulta-\\nneously, instead of recurrently.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 21}),\n",
       " Document(page_content='F . Training strategy\\nThe whole VFFP model training process is divided into two\\nstages. For stage one, we ignore the VPTR module and only\\ntrain the encoder and decoder as a normal autoencoder with\\nthe loss function in Eq. 1, which aims to reconstruct all the\\nframes of the whole training set perfectly. During stage two,\\nwe only update parameters of the VPTR module while the\\nwell-trained encoder and decoder are ﬁxed. VPTR-FAR and\\nVPTR-NAR are trained with the loss function in Eq. 7 and Eq.\\n11 respectively. It is well-known that Transformers are hard\\nto train, therefore we proposed this two-stage training strategy\\nto ease the training. Besides, the two-stage training strategy is\\nﬂexible and allows us to test different VPTR variants without\\nrepetitive training of the encoder and decoder. Experimental\\nresults show that a ﬁnal joint ﬁnetuning of autoencoder and\\nVPTR is not helpful.\\nIV. E XPERIMENTS\\nA. Datasets and Metrics\\nWe evaluate the proposed VPTR models over three datasets,', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 22}),\n",
       " Document(page_content='KTH [41], MovingMNIST [42] and BAIR [43]. For KTH and\\nMoving MNIST, VPTR models are trained to predict 10 future\\nframes given 10 past frames. For BAIR dataset, VPTR models\\nare trained to predict 10 future frames given 2 past frames.\\nAll datasets are trained with a resolution of 64×64. We\\nprocess the KTH dataset as previous works [44], [17]. Random\\nhorizontal and vertical ﬂip of each video clip are utilized\\nas data augmentation. We use the MovingMNIST created by\\nE3D-LSTM [45], which takes the same data augmentation\\nmethod as KTH. There is no data augmentation for BAIR.\\nMetrics. Learned Perceptual Image Patch Similarity\\n(LPIPS)[46] and Structural Similarity Index Measure (SSIM)\\nare used to evaluate all the three datasets. Peak Signal-to-\\nNoise Ratio (PSNR) is used to evaluate the KTH and BAIR\\ndataset, and Mean Square Error (MSE) is used to evaluate the\\nMovingMNIST dataset. All the LPIPS values are presented in\\n10−3scale.\\nB. Implementation\\nTraining stage one. In Eq. 1,λ1= 0.01for KTH and', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 22}),\n",
       " Document(page_content='MovingMNIST, λ1= 0 for the BAIR dataset. The optimizer\\nis Adam [47], with a learning rate of 2e−4.Training stage\\ntwo. For the visual features of each frame, H= 8,W=\\n8,dmodel = 528 .K= 4 for the local spatial MHSA. The\\nTransformer of VPTR-FAR includes 12 layers. For VPTR-\\nNAR, the number of layers of TEis 4, and the number of\\nlayers ofTDis 8. We take AdamW [48] with a learning rate\\nof1e−4for the optimization of all Transformers. Gradient\\nclipping is taken to stabilize the training. For the loss function\\nof VPTR-NAR (Eq. 11), λ2= 0.1.\\nC. Results\\nResults on KTH. The best results of the two VPTR variants\\nare recorded in Table I. Following the evaluation protocol of\\nprevious works, we extend the prediction length to be 20\\nframes during test. Compared with the SOTA models, theproposed VPTR models reach competitive performances in\\nterms of PSNR and SSIM. Notably, both two VPTR variants\\noutperform the SOTAs in terms of LPIPS by a large margin.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 22}),\n",
       " Document(page_content='Some prediction examples are shown in Fig. 3. It shows\\nthat the predicted arm motion by VPTRs is more aligned\\nwith the ground-truth, which indicates that the VPTRs more\\nsuccessfully capture the cyclic hand waving movements that\\nonly depends on the past frames, in contrast to LMC-Memory\\nthat recalls some inaccurate motion from the memory bank.\\nTABLE I: Results on KTH and MovingMNIST. ↑: higher is\\nbetter,↓: lower is better. Boldface : best results.\\nMethodsKTH MovingMNIST\\n10→20 10 →10\\nPSNR↑SSIM↑LPIPS↓MSE↓SSIM↑LPIPS↓\\nMCNET [44] 25.95 0.804 - - - -\\nPredRNN++ [49] 28.47 0.865 228.9 46.5 0.898 59.5\\nE3D-LSTM [45] 29.31 0.879 - 41.3 0.910 -\\nSTMFANet [17] 29.85 0.893 118.1 - - -\\nConv-TT-LSTM [50] 28.36 0.907 133.4 53.0 0.915 40.5\\nLMC-Memory [8] 28.61 0.894 133.3 41.5 0.924 46.9\\nVPTR-NAR 26.96 0.879 86.1 63.6 0.882 107.5\\nVPTR-FAR 26.13 0.859 79.6 107.2 0.844 157.8\\nVPTR-NARVPTR-FARLMC-Memory\\nFig. 3: Qualitative results on KTH dataset. The ﬁrst row is', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 22}),\n",
       " Document(page_content='ground-truth. For the past frames, t∈[1,10]. For the future\\nframes,t∈[11,30].\\nResults on MovingMNIST. The right part of Table I shows\\nthe results on the MovingMNIST dataset. We observe that the\\nSSIM of the VPTR variants is close to the SOTAs, but there\\nare large gaps in terms of MSE and LPIPS, especially for\\nVPTR-FAR. Qualitative examination shows that VPTRs make\\npoor predictions for the overlapping characters.\\nResults on BAIR. Compared with KTH and MovingM-\\nNIST, BAIR is more challenging, because the robot arm\\nmotion is random and only two past frames are given for the\\nprediction. From Table II, we ﬁnd that VPTR-NAR outper-\\nforms STMFANet [17] in terms of SSIM and LPIPS. Our\\ngood performance can be attribute to the large model capacity\\nof VPTRs and the large size of BAIR dataset. We note however\\nthat the predicted robot arm becomes blurry after the ﬁrst few\\nframes, due the deterministic nature of VPTRs. Our VPTRs\\ncould be extended to be stochastic models easily, and we', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 22}),\n",
       " Document(page_content='expect that the stochastic version of VPTRs would achieve\\nan even better performance on the BAIR dataset.\\nD. Ablation Study\\nRPE. The VPTRs with ﬁxed absolute positional encodings\\nare taken as the base models, i.e. VPTR-NAR-BASE and', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 22}),\n",
       " Document(page_content='TABLE II: Results on BAIR. ↑: higher is better, ↓: lower is\\nbetter. Boldface : best results.\\nMethods PSNR ↑SSIM↑LPIPS↓\\nSV2P [24] 20.36 0.817 91.4\\nSVG-LP [25] 17.72 0.815 60.3\\nImproved VRNN [26] - 0.822 55.0\\nSTMFANet [17] 21.02 0.844 93.6\\nVPTR-NAR 19.40 0.852 53.9\\nVPTR-FAR 18.63 0.824 69.3\\nTABLE III: Ablation study on KTH and MovingMNIST. ↑:\\nhigher is better,↓: lower is better. Boldface : best results.\\nMethodsKTH MovingMNIST\\n10→20 10 →10\\nPSNR↑SSIM↑LPIPS↓MSE↓SSIM↑LPIPS↓\\nVPTR-NAR-BASE 26.92 0.881 94.6 64.2 0.880 114.2\\nVPTR-NAR-RPE 26.96 0.879 86.1 63.6 0.882 107.5\\nVPTR-NAR-FEDA 26.25 0.872 101.1 68.0 0.872 128.7\\nVPTR-FAR-BASE 25.71 0.816 79.5 108.3 0.843 157.3\\nVPTR-FAR-RPE 26.13 0.859 79.6 107.2 0.844 157.8\\nVPTR-FAR-RIL 21.61 0.678 192.7 138.2 0.821 445.7\\nVPTR-FAR-BASE in Table III. To investigate the inﬂuence\\nof relative positional encodings, we get VPTR-NAR-RPE and\\nVPTR-FAR-RPE by substituting the 2D absolute positional\\nencoding of all local spatial MHSA module with a learned', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 23}),\n",
       " Document(page_content='2D RPE. We argue that RPE is beneﬁcial because both VPTR-\\nFAR-RPE and VPTR-NAR-RPE outperform the base models\\nwith regard to most metrics on the two datasets.\\nSpatial-temporal separation attention. The separation of\\nspatial and temporal attention reduces the complexity, but it\\nalso means that a feature at one location only attends to partial\\nlocations of the whole spatiotemporal space. To investigate the\\ninﬂuence of the separated attention, we replace the encoder-\\ndecoder attention layers of VPTR-NAR with a full spatiotem-\\nporal attention, which has a complexity of O(H2W2T2\\nP2). The\\nincreased computation cost is affordable as we only replace\\nthe encoder-decoder attention layers. Comparing the VPTR-\\nNAR-FEDA with the base model, where FEDA denotes “full\\nencoder-decoder attention”, we ﬁnd that FEDA is not beneﬁ-\\ncial. It indicates that the alternate stacking of multiple spatial\\nand temporal attention layers is capable of propagating global\\ninformation from past frames to future frames.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 23}),\n",
       " Document(page_content='Autoregressive inference methods. As we have described\\nin Section III-D, we can perform recurrently inference over\\nlatent space (RIL) or recurrently inference over pixel space\\n(RIP) for VPTR-FAR. VPTR-FAR-BASE is evaluated by\\nRIP. Even though RIL is little faster than RIP, VPTR-FAR-\\nBASE outperforms VPTR-FAR-RIL by a large margin. Severe\\naccumulation of errors is observed for VPTR-FAR-RIL.\\nWe believe the reason is that VPTR-FAR receive only\\nsupervision from the pixel space during training. There is no\\ndirect constraints on the distance between the feature space\\npredicted by the Transformer and the feature space generated\\nby the CNN encoder. Furthermore, the latent space dimension\\nof the autoencoder is greater than the pixel space dimension,\\nwhich is a common case for VFFP, as we expect a good\\nreconstruction visual quality. Therefore, recurrent inferenceonly depending on the Transformer predictor would make the\\npredicted features deviate from the ground-truth (learned by', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 23}),\n",
       " Document(page_content='autoencoder during stage one) features quickly. But decoding\\nthe feature ﬁrstly and then encoding it back into latent space\\nrestrict the deviation to some degree.\\nE. Comparison of VPTR variants\\n2 6 10 14 18\\nFuture time step22.525.027.530.032.535.037.5PSNR NAR\\nFAR\\n2 6 10 14 18\\nFuture time step0.800.850.900.95SSIM NAR\\nFAR\\n2 6 10 14 18\\nFuture time step−0.12−0.10−0.08−0.06−0.04−0.02Negative\\n LPIPSNAR\\nFAR\\nFig. 4: Results of VPTR variants on KTH for increasing\\nprediction steps.\\nFor a better visualization, we plotted the metrics curve\\nof VPTR-NAR-BASE and VPTR-FAR-BASE with respect to\\nthe predicted future time steps in Fig. 4. For the ﬁrst few\\npredicted frames, VPTR-FAR achieves a better PSNR and\\nSSIM than VPTR-NAR, but the values drop quickly due to\\nthe accumulated errors introduced by recurrent inference. It\\nshows that VPTR-NAR has a smaller quality degradation in\\nterms of PSNR and SSIM. For the last 10 steps of the LPIPS\\ncurve, VPTR-NAR also has a smaller slope than VPTR-FAR.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 23}),\n",
       " Document(page_content='The accumulation of errors in VPTR-FAR is mainly due\\nto the discrepancy between training and testing behaviors.\\nSpeciﬁcally, the previously predicted frames are used during\\ninference instead of the ground-truth as during training, which\\nleads to a worse generalization ability of VPTR-FAR given\\nthe same number of Transformer layers as VPTR-NAR. In\\ncontrast, there is no discrepancy the between training and\\ntesting behaviors of VPTR-NAR. However, it is more difﬁcult\\nfor the VPTR-NAR to estimate the joint distribution directly,\\nso an additional contrastive feature loss is required.\\nAnother advantage of VPTR-NAR is the faster inference\\nspeed. For VPTR-NAR, predicting Nframes has a complexity\\nofO(N2), but the complexity for VPTR-FAR is O(∑N\\nn=1n2).\\nFor simplicity, in this assessment, we ignored the spatial\\ndimensions of features, computation cost of processing past\\nframes, and supposed that the future frames length of inference\\nis same as of the training. However, the model size of VPTR-', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 23}),\n",
       " Document(page_content='NAR is larger because of the learned future frame queries.\\nV. C ONCLUSION\\nIn this paper, we proposed an efﬁcient VidHRFormer block\\nfor spatiotemporal representation learning, and two different\\nVFFP models are developed based on it. We expect that the\\nproposed VidHRFormer block could be used as a backbone\\nfor many other video processing tasks. We compared the per-\\nformance of proposed VPTRs with SOTA models on various\\ndatasets, and we are competitive with more complex models.\\nFinally, we analyzed the inﬂuence of different modules for two\\nVPTR variants by a thorough ablation study, and we observed\\nthat VPTR-NAR achieves a better performance than VPTR-\\nFAR.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 23}),\n",
       " Document(page_content='REFERENCES\\n[1] W. Liu, W. Luo, D. Lian, and S. Gao, “Future Frame Prediction for\\nAnomaly Detection – A New Baseline,” in CVPR , 2018, pp. 6536–6545.\\n[2] Y . Lu, K. M. Kumar, S. s. Nabavi, and Y . Wang, “Future Frame\\nPrediction Using Convolutional VRNN for Anomaly Detection,” in 2019\\n16th IEEE International Conference on Advanced Video and Signal\\nBased Surveillance (AVSS) , Sep. 2019, pp. 1–8.\\n[3] J.-A. Bolte, A. Bar, D. Lipinski, and T. Fingscheidt, “Towards Corner\\nCase Detection for Autonomous Driving,” in 2019 IEEE Intelligent\\nVehicles Symposium (IV) , Jun. 2019, pp. 438–445, iSSN: 2642-7214.\\n[4] F. Leibfried, N. Kushman, and K. Hofmann, “A Deep Learning Ap-\\nproach for Joint Video Frame and Reward Prediction in Atari Games,”\\ninICML 2017 Workshop on Principled Approaches to Deep Learning ,\\nNov. 2016.\\n[5] Y . Bengio, A. Courville, and P. Vincent, “Representation Learning: A\\nReview and New Perspectives,” IEEE Transactions on Pattern Analysis', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 24}),\n",
       " Document(page_content='and Machine Intelligence , vol. 35, no. 8, pp. 1798–1828, Aug. 2013.\\n[6] X. Wang and A. Gupta, “Unsupervised Learning of Visual Representa-\\ntions Using Videos,” in ICCV , 2015, pp. 2794–2802.\\n[7] Z. Chang, X. Zhang, S. Wang, S. Ma, Y . Ye, X. Xiang, and W. Gao,\\n“MAU: A Motion-Aware Unit for Video Prediction and Beyond,” in\\nNeurIPS , May 2021.\\n[8] S. Lee, H. G. Kim, D. H. Choi, H.-I. Kim, and Y . M. Ro, “Video\\nPrediction Recalling Long-Term Motion Context via Memory Alignment\\nLearning,” in CVPR , 2021.\\n[9] T. Meinhardt, A. Kirillov, L. Leal-Taixe, and C. Feichtenhofer,\\n“TrackFormer: Multi-Object Tracking with Transformers,” in\\narXiv:2101.02702 [cs] , Jan. 2021, arXiv: 2101.02702.\\n[10] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:\\nTransformers for image recognition at scale,” in ICLR , 2021.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 24}),\n",
       " Document(page_content='[11] P. Esser, R. Rombach, and B. Ommer, “Taming Transformers for High-\\nResolution Image Synthesis,” in arXiv:2012.09841 [cs] , Feb. 2021,\\narXiv: 2012.09841.\\n[12] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lu ˇci´c, and C. Schmid,\\n“ViViT: A video vision transformer,” in ICCV , 2021.\\n[13] Z. Liu, S. Luo, W. Li, J. Lu, Y . Wu, C. Li, and L. Yang,\\n“ConvTransformer: A Convolutional Transformer Network for Video\\nFrame Synthesis,” in arXiv:2011.10185 [cs] , Nov. 2020, arXiv:\\n2011.10185. [Online]. Available: http://arxiv.org/abs/2011.10185\\n[14] W. Yan, Y . Zhang, P. Abbeel, and A. Srinivas, “VideoGPT: Video\\nGeneration using VQ-V AE and Transformers,” in arXiv:2104.10157\\n[cs], Sep. 2021, arXiv: 2104.10157.\\n[15] C. Wu, J. Liang, L. Ji, F. Yang, Y . Fang, D. Jiang, and N. Duan,\\n“N\\\\”UWA: Visual Synthesis Pre-training for Neural visUal World\\ncreAtion,” arXiv:2111.12417 [cs] , Nov. 2021, arXiv: 2111.12417.\\n[Online]. Available: http://arxiv.org/abs/2111.12417', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 24}),\n",
       " Document(page_content='[16] M. Chaabane, A. Trabelsi, N. Blanchard, and R. Beveridge, “Looking\\nAhead: Anticipating Pedestrians Crossing with Future Frames Predic-\\ntion,” in WACV , 2020, pp. 2286–2295.\\n[17] B. Jin, Y . Hu, Q. Tang, J. Niu, Z. Shi, Y . Han, and X. Li, “Explor-\\ning Spatial-Temporal Multi-Frequency Analysis for High-Fidelity and\\nTemporal-Consistency Video Prediction,” in CVPR , 2020, pp. 4554–\\n4563.\\n[18] Y . Wang, J. Wu, M. Long, and J. B. Tenenbaum, “Probabilistic Video\\nPrediction From Noisy Data With a Posterior Conﬁdence,” in CVPR ,\\nJun. 2020.\\n[19] M. Mathieu, C. Couprie, and Y . LeCun, “Deep multi-scale video\\nprediction beyond mean square error,” in ICLR , 2016.\\n[20] B. Chen, W. Wang, and J. Wang, “Video Imagination from\\na Single Image with Transformation Generation,” in Proceedings\\nof the on Thematic Workshops of ACM Multimedia 2017 , ser.\\nThematic Workshops ’17. New York, NY , USA: Association for\\nComputing Machinery, Oct. 2017, pp. 358–366. [Online]. Available:', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 24}),\n",
       " Document(page_content='https://doi.org/10.1145/3126686.3126737\\n[21] C. V ondrick, H. Pirsiavash, and A. Torralba, “Generating videos with\\nscene dynamics,” in NIPS , 2016.\\n[22] Y . Wu, R. Gao, J. Park, and Q. Chen, “Future Video Synthesis With\\nObject Motion Prediction,” in CVPR , 2020.\\n[23] S. Oprea, P. Martinez-Gonzalez, A. Garcia-Garcia, J. A. Castro-Vargas,\\nS. Orts-Escolano, J. Garcia-Rodriguez, and A. Argyros, “A review ondeep learning techniques for video prediction,” IEEE Transactions on\\nPattern Analysis and Machine Intelligence , vol. 14, no. 8, 2020.\\n[24] M. Babaeizadeh, C. Finn, D. Erhan, R. Campbell, and S. Levine,\\n“Stochastic variational video prediction,” in ICLR , 2018.\\n[25] E. Denton and R. Fergus, “Stochastic Video Generation with a Learned\\nPrior,” in International Conference on Machine Learning . PMLR, Jul.\\n2018, pp. 1174–1183, iSSN: 2640-3498.\\n[26] L. Castrejon, N. Ballas, and A. Courville, “Improved Conditional\\nVRNNs for Video Prediction,” in ICCV , Oct. 2019, pp. 7607–7616,', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 24}),\n",
       " Document(page_content='iSSN: 2380-7504.\\n[27] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving\\nlanguage understanding by generative pre-training,” 2018.\\n[28] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\\n“Language models are unsupervised multitask learners,” OpenAI blog ,\\nvol. 1, no. 8, p. 9, 2019.\\n[29] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and\\nL. Shao, “Pyramid Vision Transformer: A Versatile Backbone for Dense\\nPrediction without Convolutions,” in arXiv:2102.12122 [cs] , Feb. 2021,\\narXiv: 2102.12122. [Online]. Available: http://arxiv.org/abs/2102.12122\\n[30] H. Fan, B. Xiong, K. Mangalam, Y . Li, Z. Yan, J. Malik, and C. Feichten-\\nhofer, “Multiscale vision transformers,” in ICCV , 2021, pp. 6824–6835.\\n[31] P. Zhang, X. Dai, J. Yang, B. Xiao, L. Yuan, L. Zhang, and J. Gao,\\n“Multi-Scale Vision Longformer: A New Vision Transformer for High-\\nResolution Image Encoding,” in ICCV , 2021.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 24}),\n",
       " Document(page_content='[32] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\\nB. Guo, “Swin transformer: Hierarchical vision transformer using shifted\\nwindows,” in ICCV , 2021.\\n[33] Y . Yuan, R. Fu, L. Huang, W. Lin, C. Zhang, X. Chen, and J. Wang,\\n“HRFormer: High-resolution transformer for dense prediction,” in\\nNeurIPS , 2021.\\n[34] Z. Huang, X. Wang, L. Huang, C. Huang, Y . Wei, and W. Liu, “CCNet:\\nCriss-Cross Attention for Semantic Segmentation,” 2019, pp. 603–612.\\n[35] H. Wang, Y . Zhu, B. Green, H. Adam, A. Yuille, and L.-C. Chen, “Axial-\\nDeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation,” in\\nComputer Vision – ECCV 2020 , ser. Lecture Notes in Computer Science,\\nA. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm, Eds. Cham: Springer\\nInternational Publishing, 2020, pp. 108–126.\\n[36] P. Isola, J. Y . Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation\\nwith conditional adversarial networks,” in CVPR , 2017.\\n[37] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 24}),\n",
       " Document(page_content='S. Zagoruyko, “End-to-End Object Detection with Transformers,” in\\nECCV . Springer International Publishing, 2020, pp. 213–229.\\n[38] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative posi-\\ntion representations,” in NAACL . New Orleans, Louisiana: Association\\nfor Computational Linguistics, Jun. 2018, pp. 464–468.\\n[39] Y . Wang, F. Tian, D. He, T. Qin, C. Zhai, and T.-Y . Liu, “Non-\\nautoregressive machine translation with auxiliary regularization,” in\\nProceedings of the AAAI conference on artiﬁcial intelligence , vol. 33,\\n2019, pp. 5377–5384, number: 01.\\n[40] A. Andonian, T. Park, B. Russell, P. Isola, J.-Y . Zhu, and R. Zhang,\\n“Contrastive feature loss for image prediction,” in Proceedings of the\\nIEEE/CVF international conference on computer vision , 2021, pp. 1934–\\n1943.\\n[41] C. Schuldt, I. Laptev, and B. Caputo, “Recognizing human actions: a\\nlocal SVM approach,” in ICPR , vol. 3, Aug. 2004, pp. 32–36 V ol.3,\\niSSN: 1051-4651.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 24}),\n",
       " Document(page_content='iSSN: 1051-4651.\\n[42] N. Srivastava, E. Mansimov, and R. Salakhudinov, “Unsupervised Learn-\\ning of Video Representations using LSTMs.” PMLR, Jun. 2015, pp.\\n843–852.\\n[43] F. Ebert, C. Finn, A. X. Lee, and S. Levine, “Self-supervised visual\\nplanning with temporal skip connections,” in 1st annual conference\\non robot learning, CoRL 2017, mountain view, california, USA,\\nnovember 13-15, 2017, proceedings , ser. Proceedings of machine\\nlearning research, vol. 78. PMLR, 2017, pp. 344–356. [Online].\\nAvailable: http://proceedings.mlr.press/v78/frederik-ebert17a.html\\n[44] R. Villegas, J. Yang, S. Hong, X. Lin, and H. Lee, “Decomposing motion\\nand content for natural video sequence prediction,” in ICLR , 2017.\\n[45] Y . Wang, L. Jiang, M.-H. Yang, L.-J. Li, M. Long, and L. Fei-Fei,\\n“Eidetic 3D LSTM: A Model for Video Prediction and Beyond,” in\\nICLR , Sep. 2018.\\n[46] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 24}),\n",
       " Document(page_content='unreasonable effectiveness of deep features as a perceptual metric,” in\\nCVPR , 2018, pp. 586–595.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 24}),\n",
       " Document(page_content='[47] D. P. Kingma and J. L. Ba, “Adam: A method for stochastic optimiza-\\ntion,” in ICLR , 2015, pp. 1–15.\\n[48] I. Loshchilov and F. Hutter, “Decoupled Weight Decay Regularization,”\\nSep. 2018.\\n[49] Y . Wang, Z. Gao, M. Long, J. Wang, and P. S. Yu, “PredRNN++:\\nTowards A Resolution of the Deep-in-Time Dilemma in Spatiotemporal\\nPredictive Learning.” PMLR, Jul. 2018, pp. 5123–5132.\\n[50] J. Su, W. Byeon, J. Kossaiﬁ, F. Huang, J. Kautz, and A. Anandkumar,\\n“Convolutional Tensor-Train LSTM for Spatio-temporal Learning,” in\\nNeurIPS , 2020.', metadata={'source': 'Merged_Papers_RAG.pdf', 'page': 25})]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have chunked text time to convert it into embedding(Vectors cause texts are not recognized by ml model),And we have to store that embedded chunks into vector databse (WEAVIATE is cloud vector database that we are using)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = Weaviate.from_documents( #The code calls the function.\n",
    "    docs, #It provides the docs list containing the documents you want to store.\n",
    "    embeddings, #It provides the embeddings list containing the corresponding embeddings for each document.\n",
    "    client=client,#The client argument might be left blank if you haven't created a separate Weaviate client object beforehand.\n",
    "    by_text=False #Setting by_text=False indicates that you want to prioritize retrieval based on the vector embeddings rather than the textual content of the documents.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this code snippet creates a Weaviate vector database populated with document embeddings, allowing for efficient retrieval based on semantic similarity using vector search techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN (Least Square GAN [24]) module for generating a\n",
      "more realistic frame. Usually GAN contains a discrimi-\n",
      "native networkDand a generator network G.Glearns to\n",
      "generate frames that are hard to be classiﬁed by D, while\n",
      "Daims to discriminate the frames generated by G. Ideally,\n",
      "whenGis well trained,Dcannot predict better than chance.\n",
      "In practice, adversarial training is implemented with an al-\n",
      "ternative update manner. Moreover, we treat the U-Net\n",
      "based prediction network as G. As forD, we follow [15]\n",
      "and utilize a patch discriminator which means each output\n",
      "scalar ofDcorresponds a patch of an input image. Totally,\n",
      "the training schedule is illustrated as follows:\n",
      "TrainingD. The goal of training Dis to classify It+1\n",
      "into class 1 andG(I1,I2,...,I t) =ˆIt+1into class 0, where 0\n",
      "and 1 represent fake and genuine labels, respectively. When\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#just experementing similarity search between vector database and querie\n",
    "print(vector_db.similarity_search(\"what is GAN?\", k=2)[0].page_content) #code for performinng similarity search where k represents the top K similar vectors in decoded formats(Documents format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "few recent works for VFFP [13], [14], [15]. However, itis computational expensive to apply Transformer to high\n",
      "dimensional visual features. We still need further research\n",
      "about more efﬁcient visual Transformers, especially for videos.\n",
      "Therefore, we propose a novel efﬁcient Transformer block with\n",
      "smaller complexity, and we developed a new video prediction\n",
      "Transformer (VPTR) based on it.\n",
      "Among the Transformers-based VFFP models [13], [14],\n",
      "[15] that we mentioned earlier, some of them are autoregres-\n",
      "sive models while some others are non-autoregressive models,\n",
      "and they are based on different attention mechanisms, e.g.\n",
      "a custom convolution multi-head attention (MHA) [13] and\n",
      "standard dot-product MHA [14], [15]. There is no formal\n",
      "comparison of the two typical approaches (autoregressive vs\n",
      "non-autoregressive) to use Transformer-based VFFP models so\n",
      "far. Thus, we developed an fully autoregressive VPTR (VPTR-\n",
      "FAR) and a non-autoregressive VPTR (VPTR-NAR). The two\n"
     ]
    }
   ],
   "source": [
    "print(vector_db.similarity_search(\"what is Transformer?\", k=3)[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='GAN (Least Square GAN [24]) module for generating a\\nmore realistic frame. Usually GAN contains a discrimi-\\nnative networkDand a generator network G.Glearns to\\ngenerate frames that are hard to be classiﬁed by D, while\\nDaims to discriminate the frames generated by G. Ideally,\\nwhenGis well trained,Dcannot predict better than chance.\\nIn practice, adversarial training is implemented with an al-\\nternative update manner. Moreover, we treat the U-Net\\nbased prediction network as G. As forD, we follow [15]\\nand utilize a patch discriminator which means each output\\nscalar ofDcorresponds a patch of an input image. Totally,\\nthe training schedule is illustrated as follows:\\nTrainingD. The goal of training Dis to classify It+1\\ninto class 1 andG(I1,I2,...,I t) =ˆIt+1into class 0, where 0\\nand 1 represent fake and genuine labels, respectively. When\\n4', metadata={'page': 11, 'source': 'Merged_Papers_RAG.pdf'}), Document(page_content='the original frames and reconstructed frames respectively, xi\\ndenotes a single frame, λ1andαare hyperparameters. LGAN\\ndenotes the GAN loss (Eq. 4), where Ddenotes a discrimina-\\ntor, which is not shown in Fig. 1, and the combination of the\\nencoder and decoder is considered to be a generator G. We\\ntrainLGAN with the PatchGAN [36] manner.\\nL2(X,ˆX) =n∑\\ni=1∥xi−ˆxi∥2\\n2 (2)\\nLgdl(X,ˆX) =n∑\\ni=1∑\\ni,j⏐⏐|xi,j−xi−1,j|−|ˆxi,j−ˆxi−1,j|⏐⏐α\\n+⏐⏐|xi,j−1−xi,j|−|ˆxi,j−1−ˆxi,j|⏐⏐α(3)\\nLGAN(G,D ) =EX[logD (X)]+EˆX[log(1−D(G(X))](4)', metadata={'page': 19, 'source': 'Merged_Papers_RAG.pdf'}), Document(page_content='vLSTM as a backbone network and build a future predic-\\ntion model for anomaly detection. Luo et al. [17] combine\\nautoencoder and ConvLSTM to reconstruct the output of\\nConvLSTM to the original image size. Because the inner\\nstructure of ConvLSTM is entirely deterministic, these pre-\\ndictive modeling methods cannot predict highly structured\\nmoving objects, which results in inaccurate predictions of\\nanomalies.\\nGenerative models, such as V AE [14] and GAN [9], have\\nbeen applied for the purpose of learning the distribution of\\nregular frames. Sabokrou et al. [26] propose a one class\\nclassiﬁer using conditional adversarial networks [12]. Xie\\net al. [31] use a GAN-based image inpainting method to\\ndetect and localize the abnormal objects. Liu et al. [15]\\npropose a GAN-based future frame prediction network with\\noptical ﬂow network[8]. An et al. [1] apply V AE to build an\\nanomaly detection system, but the method is not performed\\non real-world datasets.', metadata={'page': 1, 'source': 'Merged_Papers_RAG.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\n",
    "    vector_db.similarity_search(\n",
    "        \"what is GAN?\", k=3)\n",
    "    )\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='[11] P. Esser, R. Rombach, and B. Ommer, “Taming Transformers for High-\\nResolution Image Synthesis,” in arXiv:2012.09841 [cs] , Feb. 2021,\\narXiv: 2012.09841.\\n[12] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lu ˇci´c, and C. Schmid,\\n“ViViT: A video vision transformer,” in ICCV , 2021.\\n[13] Z. Liu, S. Luo, W. Li, J. Lu, Y . Wu, C. Li, and L. Yang,\\n“ConvTransformer: A Convolutional Transformer Network for Video\\nFrame Synthesis,” in arXiv:2011.10185 [cs] , Nov. 2020, arXiv:\\n2011.10185. [Online]. Available: http://arxiv.org/abs/2011.10185\\n[14] W. Yan, Y . Zhang, P. Abbeel, and A. Srinivas, “VideoGPT: Video\\nGeneration using VQ-V AE and Transformers,” in arXiv:2104.10157\\n[cs], Sep. 2021, arXiv: 2104.10157.\\n[15] C. Wu, J. Liang, L. Ji, F. Yang, Y . Fang, D. Jiang, and N. Duan,\\n“N\\\\”UWA: Visual Synthesis Pre-training for Neural visUal World\\ncreAtion,” arXiv:2111.12417 [cs] , Nov. 2021, arXiv: 2111.12417.\\n[Online]. Available: http://arxiv.org/abs/2111.12417', metadata={'page': 24, 'source': 'Merged_Papers_RAG.pdf'}), Document(page_content='few recent works for VFFP [13], [14], [15]. However, itis computational expensive to apply Transformer to high\\ndimensional visual features. We still need further research\\nabout more efﬁcient visual Transformers, especially for videos.\\nTherefore, we propose a novel efﬁcient Transformer block with\\nsmaller complexity, and we developed a new video prediction\\nTransformer (VPTR) based on it.\\nAmong the Transformers-based VFFP models [13], [14],\\n[15] that we mentioned earlier, some of them are autoregres-\\nsive models while some others are non-autoregressive models,\\nand they are based on different attention mechanisms, e.g.\\na custom convolution multi-head attention (MHA) [13] and\\nstandard dot-product MHA [14], [15]. There is no formal\\ncomparison of the two typical approaches (autoregressive vs\\nnon-autoregressive) to use Transformer-based VFFP models so\\nfar. Thus, we developed an fully autoregressive VPTR (VPTR-\\nFAR) and a non-autoregressive VPTR (VPTR-NAR). The two', metadata={'page': 18, 'source': 'Merged_Papers_RAG.pdf'}), Document(page_content='patches, either 2D or 3D, and then tokenize the local patch by\\nconcatenation or Pooling [30]. Some other models introduce\\nsparse attention to reduce the complexity, e.g. restricting the\\nattention over a local region [31], [32], [33], or decomposingthe global attention into a series of axial-attention [34], [35],\\n[12]. HRFormer [33] is an example of local region attention-\\nbased Transformers, which is designed for image classiﬁcation\\nand dense prediction.\\nSpeciﬁcally, a HRFormer block is composed of a local-\\nwindow multi-head self attention layer and a depth-wise con-\\nvolution feed-forward network. The input feature maps Z∈\\nRH×W×Care ﬁrstly evenly divided into Pnon-overlapping\\nlocal patches, each patch is Zp∈RH\\nP×W\\nP×C. Then a multi-\\nhead self attention is performed for each patch. Finally, the\\ndepth-wise convolution is used to exchange information among\\ndifferent local patches.\\nIII. T HE PROPOSED VPTR MODELS\\nA. Overall framework of VPTR', metadata={'page': 19, 'source': 'Merged_Papers_RAG.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    vector_db.similarity_search(\n",
    "        \"what is Transformer?\", k=3)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='sequential data. Given an input x, V AE applies an encoder\\n(also known as inference model) qθ(z|x)to generate the la-\\ntent variable zthat captures the variation in x. It uses a\\ndecoderpφ(x|z)to approximate the observation given the\\nlatent variable. The inference model represents the approx-\\nimate posterior using the mean µand variance σ2calculated\\nby a neural network qθ(z|x)∼N (µx, σ2\\nx), whereµxand\\nσ2\\nxare outputs of some neural networks that take xas the\\ninput. A prior p(z)is chosen to be a simple Gaussian dis-\\ntribution. With the constraints of distribution on latent vari-\\nables, the complete objective function can be described as', metadata={'page': 1, 'source': 'Merged_Papers_RAG.pdf'}), Document(page_content='the dimension of µ2(t)andσ2(t)to be 20. We then deﬁne\\nthe posterior of the latent variable z(t)in V AE as:\\nqθ(z(t)|concat (x(t),h(t−1)))\\n∼N(\\nµ2(t), diag(\\nσ2(t)2)) (3)\\nwherez(t)∈R20. To measure the distribution loss between\\nEq. 2 and Eq. 3 at time step t, we can use the KL-divergence\\nmetricKL(qθ(z(t)|x(t),h(t−1))||c(t)).\\nRecurrence: To capture the temporal information among\\nframes in a video, we use a ConvLSTM to represent the\\nrecurrent relationship among frames. From the current in-\\nput imagex(t), we apply a CNN to extract a feature map\\nwhich we denote as ϕx(x(t))∈RH′×W′×F. To match\\nthe dimension of this feature, we also resize the latent vari-\\nablez(t)(recallz(t)∈R20) as follows. We ﬁrst use\\nfully connected layers to map z(t)to a high-dimensional\\nspace R1024, then reshape to a 3D tensor of dimension\\nH′×W′×F= 16×16×32. We usezr(t)∈RH′×W′×Fto\\ndenote this reshaped tensor. We concatenate the input fea-\\ntureϕx(x(t))with thezr(t)along the channel dimension', metadata={'page': 3, 'source': 'Merged_Papers_RAG.pdf'}), Document(page_content='and decoder, the VPTR-FAR parameterizes the following\\ndistribution:\\np(x1,...,xL,...,xL+N) =L+N∏\\nt=1p(xt|xt−1,...x 1) (6)\\nIn other words, VPTR-FAR predicts the next frame condi-\\ntioned on all previous frames, which is also the most common\\nparadigm for most SOTA VFFP models. An attention mask\\nis applied to the temporal MHSA module to impose the\\nconditional dependency between the next frame and previous\\nframes.\\nDuring training, we feed the ground-truth frames\\n{x1,...,xL+N−1}into the encoder, which generates the\\nfeature sequence{z1,...,zL+N−1}. VPTR-FAR then predicts\\nthe future feature sequence {ˆz2,...,ˆzL+N}, which is then\\ndecoded by the decoder to generate frames {ˆx2,...,ˆxL+N}.\\nThe training loss of VPTR-FAR is:\\nLFAR =L+N∑\\nt=2L2(xt,ˆxt) +L+N∑\\nt=2Lgdl(xt,ˆxt) (7)\\nDuring test, we ﬁrstly get the ground-truth past frames\\nfeatures{z1,...,zL}. Then there are two different ways of\\nrecurrently predicting the future frames. The ﬁrst method', metadata={'page': 20, 'source': 'Merged_Papers_RAG.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    vector_db.similarity_search(\n",
    "        \"what is VARs?\", k=3)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to integrate our data with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate #The line from langchain.prompts import ChatPromptTemplate imports the ChatPromptTemplate class from the prompts module within the langchain library. This class is specifically designed for creating templates used in chat-oriented question-answering tasks.\n",
    "\n",
    "template=\"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use thirty sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Chat Prompt Template:\n",
    "\n",
    "The above code defines a multi-line string variable named template. This string contains the actual template that will be used to generate prompts for the question-answering model. Let's break down the template's content:\n",
    "\n",
    "Introduction:\"You are an assistant for question-answering tasks.\" - This part sets the context for the model, informing it that it's acting as a question-answering assistant.\n",
    "\n",
    "Context Placeholder:\"{context}\" - This placeholder represents the retrieved context relevant to the question being asked. It will be replaced with the actual retrieved context when the prompt is generated.\n",
    "\n",
    "Question Placeholder:\"{question}\" - This placeholder represents the question that needs to be answered. It will be replaced with the actual question when the prompt is generated.\n",
    "\n",
    "Instruction:\"If you don't know the answer, just say that you don't know.\" - This instructs the model to indicate its limitations if it cannot answer the question based on the provided context.\n",
    "\n",
    "Answer Placeholder:\"Answer:\\n\" - This line signifies the start of the answer section in the generated prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements for Using the ChatPromptTemplate:\n",
    "\n",
    "Language Model Compatibility: Ensure your language model is compatible with receiving prompts in the format generated by ChatPromptTemplate. This typically involves models trained for question-answering tasks that can take instructions and context into account.\n",
    "Context and Question Replacement: You'll need a mechanism to replace the placeholders ({context} and {question}) with the actual retrieved context and the question you want to ask. This might involve integrating ChatPromptTemplate with your specific workflow for question answering.\n",
    "Answer Processing (Optional): Depending on your model's output format, you might need to process the generated response to extract the answer portion (text following \"Answer:\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_template(template) #creating prompt for LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks.\\nUse the following pieces of retrieved context to answer the question.\\nIf you don't know the answer, just say that you don't know.\\nUse thirty sentences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n\"))])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading LLM in our local device using \"HuggingFace\" library 'API' key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingfacehub_api_token='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HuggingFaceHub(\n",
    "    huggingfacehub_api_token=huggingfacehub_api_token,\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    model_kwargs={\"temperature\":1, \"max_length\":400}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough #This line imports the RunnablePassthrough class from the runnable module within the schema subpackage of LangChain.is a basic component used in LangChain workflows. It simply passes the input data through the chain unchanged\n",
    "from langchain.schema.output_parser import StrOutputParser #This line imports the StrOutputParser class from the output_parser module within the schema subpackage of LangChain.The StrOutputParser class is a component used for parsing the output of a LangChain workflow. In this case, it specifically expects the output to be a string. It essentially converts the output (which could be in various formats depending on the chain) into a simple string representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser=StrOutputParser() #creating object of StrOutputparser class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vector_db.as_retriever() #creating object to perform retriving operation on our vector database  in-order to get most similar context to query in vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever,  \"question\": RunnablePassthrough()} #This is a LangChain component that simply passes the input data through the chain unchanged. In this case, it ensures that the original user's question is preserved and sent to later stages.\n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")#The pipe symbol (|) acts as a chaining operator in LangChain. It connects the previous stage (the dictionary) to the next stage, which is defined by prompt in this case,and so on...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You are an assistant for question-answering tasks.\n",
      "Use the following pieces of retrieved context to answer the question.\n",
      "If you don't know the answer, just say that you don't know.\n",
      "Use thirty sentences maximum and keep the answer concise.\n",
      "Question: what is GAN ?\n",
      "Context: [Document(page_content='GAN (Least Square GAN [24]) module for generating a\\nmore realistic frame. Usually GAN contains a discrimi-\\nnative networkDand a generator network G.Glearns to\\ngenerate frames that are hard to be classiﬁed by D, while\\nDaims to discriminate the frames generated by G. Ideally,\\nwhenGis well trained,Dcannot predict better than chance.\\nIn practice, adversarial training is implemented with an al-\\nternative update manner. Moreover, we treat the U-Net\\nbased prediction network as G. As forD, we follow [15]\\nand utilize a patch discriminator which means each output\\nscalar ofDcorresponds a patch of an input image. Totally,\\nthe training schedule is illustrated as follows:\\nTrainingD. The goal of training Dis to classify It+1\\ninto class 1 andG(I1,I2,...,I t) =ˆIt+1into class 0, where 0\\nand 1 represent fake and genuine labels, respectively. When\\n4', metadata={'page': 11, 'source': 'Merged_Papers_RAG.pdf'}), Document(page_content='the original frames and reconstructed frames respectively, xi\\ndenotes a single frame, λ1andαare hyperparameters. LGAN\\ndenotes the GAN loss (Eq. 4), where Ddenotes a discrimina-\\ntor, which is not shown in Fig. 1, and the combination of the\\nencoder and decoder is considered to be a generator G. We\\ntrainLGAN with the PatchGAN [36] manner.\\nL2(X,ˆX) =n∑\\ni=1∥xi−ˆxi∥2\\n2 (2)\\nLgdl(X,ˆX) =n∑\\ni=1∑\\ni,j⏐⏐|xi,j−xi−1,j|−|ˆxi,j−ˆxi−1,j|⏐⏐α\\n+⏐⏐|xi,j−1−xi,j|−|ˆxi,j−1−ˆxi,j|⏐⏐α(3)\\nLGAN(G,D ) =EX[logD (X)]+EˆX[log(1−D(G(X))](4)', metadata={'page': 19, 'source': 'Merged_Papers_RAG.pdf'}), Document(page_content='vLSTM as a backbone network and build a future predic-\\ntion model for anomaly detection. Luo et al. [17] combine\\nautoencoder and ConvLSTM to reconstruct the output of\\nConvLSTM to the original image size. Because the inner\\nstructure of ConvLSTM is entirely deterministic, these pre-\\ndictive modeling methods cannot predict highly structured\\nmoving objects, which results in inaccurate predictions of\\nanomalies.\\nGenerative models, such as V AE [14] and GAN [9], have\\nbeen applied for the purpose of learning the distribution of\\nregular frames. Sabokrou et al. [26] propose a one class\\nclassiﬁer using conditional adversarial networks [12]. Xie\\net al. [31] use a GAN-based image inpainting method to\\ndetect and localize the abnormal objects. Liu et al. [15]\\npropose a GAN-based future frame prediction network with\\noptical ﬂow network[8]. An et al. [1] apply V AE to build an\\nanomaly detection system, but the method is not performed\\non real-world datasets.', metadata={'page': 1, 'source': 'Merged_Papers_RAG.pdf'}), Document(page_content='method learns a GAN-based model to predict the future\\nframe. An anomaly then corresponds to a large difference\\nbetween the predicted future frame and the actual future\\nframe. One limitation of [15] is that it directly concate-\\nnates the several observed frames as the input to the GAN\\nmodel. As a result, the model does not directly represent the\\ntemporal information in a video. Although [15] uses optical\\nﬂow features which capture some temporal information at\\nthe feature level, the optical ﬂow information is only used\\nas a constraint during training and is not used during testing.\\nIn this paper, we follow the future frame prediction\\nframework in [15] and propose a new approach that bet-\\nter capture the temporal information in a video for anomaly\\ndetection. We propose to combine sequential models (in\\nparticular, ConvLSTM) with generative models (in particu-\\n978-1-5386-9294-3/18/$31.00 2019 IEEEarXiv:1909.02168v2  [cs.CV]  18 Oct 2019\\nSuspicious Behaviour\\nNormal Events\\nDetected', metadata={'page': 0, 'source': 'Merged_Papers_RAG.pdf'})]\n",
      "Answer:\n",
      "GAN stands for Generative Adversarial Network. It is a type of neural network that consists of two main components: a generator and a discriminator. The generator learns to generate new data that is similar to the training data, while the discriminator learns to distinguish between real and fake data. The two networks are trained together in an adversarial manner, with the goal of improving the generator's ability to generate realistic data.\n",
      "\n",
      "In the context provided, GAN\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke(\"what is GAN ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":\n",
      "VAE stands for Variational Autoencoder. It is a type of autoencoder that uses a probabilistic model to learn a compressed representation of input data. In the context of the provided documents, VAE is used as a baseline model for training and evaluating the performance of other models. It is trained during the first stage of the training process, where the VPTR module is ignored and the encoder and decoder are trained as a normal autoencoder. The loss function used for training\n"
     ]
    }
   ],
   "source": [
    "print((rag_chain.invoke(\"what is VAE ?\")).split('Answer')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":\n",
      "\n",
      "A Transformer is a type of neural network architecture that is commonly used for natural language processing tasks. It was first introduced in a 2017 paper by Vaswani et al. and has since become a popular choice for a wide range of NLP tasks, including language translation, text classification, and language generation.\n",
      "\n",
      "In the context of computer vision, Transformers have been used for tasks such as image classification, object detection, and image synthesis. However, they\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print((rag_chain.invoke(\"what is Transformer ?\")).split('Answer')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":\n",
      "\n",
      "VAE and GAN are both generative models used in image processing. However, they differ in their approach to generating new images. VAE learns to encode and decode images, while GAN learns to generate new images by training a generator network to fool a discriminator network. The training process for VAE involves reconstructing the input image, while the training process for GAN involves generating new images that are difficult to classify by the discriminator. The two models can be\n"
     ]
    }
   ],
   "source": [
    "print((rag_chain.invoke(\"what is difference between vae and gan ?\").split('Answer')[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
